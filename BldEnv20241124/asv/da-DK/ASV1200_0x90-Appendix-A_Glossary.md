# Appendiks A: Ordforråd

> Denne omfattende ordliste giver definitioner på centrale AI-, ML- og sikkerhedsbegreber, der anvendes gennem hele AISVS for at sikre klarhed og fælles forståelse.
> ​
* Adversarialt eksempel: En input bevidst udformet til at få en AI-model til at lave en fejl, ofte ved at tilføje subtile forstyrrelser, der er ufattelige for mennesker.
  ​
* Robusthed over for angreb – Robusthed over for angreb i AI refererer til en models evne til at opretholde dens præstation og modstå at blive narret eller manipuleret af bevidst udformede, skadelige input designet til at forårsage fejl.
  ​
* Agent – AI-agenter er softwaresystemer, der bruger AI til at forfølge mål og udføre opgaver på vegne af brugere. De udviser ræsonnering, planlægning og hukommelse og har et niveau af autonomi til at træffe beslutninger, lære og tilpasse sig.
  ​
* Agentisk AI: AI-systemer, der kan operere med en vis grad af autonomi for at nå mål, ofte ved at træffe beslutninger og handle uden direkte menneskelig indblanding.
  ​
* Attributbaseret adgangskontrol (ABAC): Et adgangskontrolparadigme, hvor autorisationsbeslutninger baseres på attributter for brugeren, ressourcen, handlingen og miljøet, som evalueres på forespørgselstidspunktet.
  ​
* Bagdørsangreb: En type datainfektionsangreb, hvor modellen trænes til at reagere på en bestemt måde på visse udløsere, mens den opfører sig normalt ellers.
  ​
* Bias: Systematiske fejl i AI-modeloutput, der kan føre til urimelige eller diskriminerende resultater for visse grupper eller i specifikke sammenhænge.
  ​
* Biasudnyttelse: En angrebsteknik, der udnytter kendte bias i AI-modeller til at manipulere output eller resultater.
  ​
* Cedar: Amazons politik-sprog og motor til detaljerede tilladelser, der bruges til implementering af ABAC for AI-systemer.
  ​
* Tankegang: En teknik til forbedring af ræsonnering i sprogmodeller ved at generere mellemliggende ræsonneringstrin, før det endelige svar produceres.
  ​
* Strømafbrydere: Mekanismer, der automatisk stopper AI-systemets operationer, når specifikke risikogrænser overskrides.
  ​
* Data Lækage: Utilsigtet afsløring af følsomme oplysninger gennem AI-models output eller adfærd.
  ​
* Datatoksinering: Den bevidste korruption af træningsdata for at kompromittere modelintegriteten, ofte for at installere bagdøre eller forringe ydeevnen.
  ​
* Differential Privacy – Differential privacy er en matematisk stringent ramme for at udgive statistisk information om datasæt, samtidig med at beskyttelsen af individuelle datapersoners privatliv sikres. Det gør det muligt for en dataindehaver at dele aggregerede mønstre i gruppen, samtidig med at information, der lækkes om specifikke individer, begrænses.
  ​
* Indlejringer: Tætte vektorrepræsentationer af data (tekst, billeder osv.), der fanger semantisk betydning i et højdimensionelt rum.
  ​
* Forklarbarhed – Forklarbarhed i AI er et AI-systems evne til at give menneskeligt forståelige begrundelser for dets beslutninger og forudsigelser, hvilket giver indsigt i dets interne funktioner.
  ​
* Forklarlig AI (XAI): AI-systemer designet til at give menneskeligt forståelige forklaringer på deres beslutninger og adfærd gennem forskellige teknikker og rammeværk.
  ​
* Federeret læring: En maskinlæringsmetode, hvor modeller trænes på tværs af flere decentraliserede enheder, der indeholder lokale datasæt, uden at udveksle selve dataene.
  ​
* Værn: Begrænsninger implementeret for at forhindre AI-systemer i at producere skadelige, forudindtagede eller på anden måde uønskede output.
  ​
* Hallucination – En AI-hallucination refererer til et fænomen, hvor en AI-model genererer forkerte eller vildledende oplysninger, som ikke er baseret på dens træningsdata eller faktiske virkelighed.
  ​
* Human-in-the-Loop (HITL): Systemer designet til at kræve menneskelig overvågning, verifikation eller indgriben ved vigtige beslutningspunkter.
  ​
* Infrastructure som kode (IaC): Administration og udrulning af infrastruktur gennem kode i stedet for manuelle processer, hvilket muliggør sikkerhedsscanning og konsekvente udrulninger.
  ​
* Jailbreak: Teknikker brugt til at omgå sikkerhedsvagter i AI-systemer, især i store sprogmodeller, for at producere forbudt indhold.
  ​
* Mindste privilegium: Sikkerhedsprincippet om kun at tildele de mindst nødvendige adgangsrettigheder til brugere og processer.
  ​
* LIME (Lokale Interpretable Model-agnostiske Forklaringer): En teknik til at forklare forudsigelser fra enhver maskinlæringsklassifikator ved at approximere den lokalt med en fortolkelig model.
  ​
* Membership Inference Attack: Et angreb, der har til formål at bestemme, om et specifikt datapunkt blev brugt til at træne en maskinlæringsmodel.
  ​
* MITRE ATLAS: Modstandsdygtigt trusselslandskab for kunstige intelligenssystemer; en vidensdatabase over modstandstaktikker og -teknikker mod AI-systemer.
  ​
* Modelkort – Et modelkort er et dokument, der giver standardiseret information om en AI-models ydeevne, begrænsninger, tilsigtede anvendelser og etiske overvejelser for at fremme gennemsigtighed og ansvarlig AI-udvikling.
  ​
* Modeludtrækning: Et angreb, hvor en modstander gentagne gange forespørger en målsmodel for at skabe en funktionelt tilsvarende kopi uden tilladelse.
  ​
* Modelinversion: Et angreb, der forsøger at rekonstruere træningsdata ved at analysere modeludgange.
  ​
* Modelstyring – Modelstyring for AI er processen med at overvåge alle faser af en AI-models levetid, herunder dens design, udvikling, implementering, overvågning, vedligeholdelse og eventuelle udfasning, for at sikre, at den forbliver effektiv og i overensstemmelse med målene.
  ​
* Modelforgiftning: Indføring af sårbarheder eller bagindgange direkte i en model under træningsprocessen.
  ​
* Modeltyveri: Udtrækning af en kopi eller en tilnærmelse af en proprietær model gennem gentagne forespørgsler.
  ​
* Multi-agent System: Et system bestående af flere interagerende AI-agenter, hver med potentielt forskellige kapaciteter og mål.
  ​
* OPA (Open Policy Agent): En open source-politikmotor, der muliggør ensartet politikhåndhævelse på tværs af hele stacken.
  ​
* Privatlivsbevarende maskinlæring (PPML): Teknikker og metoder til at træne og implementere ML-modeller samtidig med, at privatlivets fred for træningsdata beskyttes.
  ​
* Prompt Injection: Et angreb, hvor ondsindede instruktioner indlejres i input for at tilsidesætte en models tilsigtede adfærd.
  ​
* RAG (Retrieval-Augmented Generation): En teknik, der forbedrer store sprogmodeller ved at hente relevant information fra eksterne videnskilder, før der genereres et svar.
  ​
* Red-Teaming: Praksis med aktivt at teste AI-systemer ved at simulere fjendtlige angreb for at identificere sårbarheder.
  ​
* SBOM (Software Bill of Materials): En formel optegnelse, der indeholder detaljer og forsyningskædeforhold for forskellige komponenter, der bruges til at bygge software eller AI-modeller.
  ​
* SHAP (SHapley Additive exPlanations): En spilteoretisk tilgang til at forklare outputtet af enhver maskinlæringsmodel ved at beregne bidraget fra hver funktion til forudsigelsen.
  ​
* Supply Chain Attack: Kompromittering af et system ved at angribe mindre sikre elementer i dets forsyningskæde, såsom tredjepartsbiblioteker, datasæt eller forudtrænede modeller.
  ​
* Transfer Learning: En teknik, hvor en model udviklet til én opgave genbruges som udgangspunkt for en model til en anden opgave.
  ​
* Vektordatabase: En specialiseret database designet til at gemme højdimensionelle vektorer (indlejringer) og udføre effektive søger efter ligheder.
  ​
* Sårbarhedsscanning: Automatiserede værktøjer, der identificerer kendte sikkerhedssårbarheder i softwarekomponenter, herunder AI-rammer og afhængigheder.
  ​
* Vandmærkning: Teknikker til at indlejre uundværlige markører i AI-genereret indhold for at spore dets oprindelse eller opdage AI-generering.
  ​
* Zero-Day-sårbarhed: En tidligere ukendt sårbarhed, som angribere kan udnytte, før udviklere skaber og implementerer en rettelse.

