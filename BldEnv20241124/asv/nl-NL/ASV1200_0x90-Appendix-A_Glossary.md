# Bijlage A: Woordenlijst

> Deze uitgebreide woordenlijst biedt definities van belangrijke AI-, ML- en beveiligingstermen die door het hele AISVS worden gebruikt om duidelijkheid en gemeenschappelijk begrip te waarborgen.
> ​
* Adversariale voorbeeld: een invoer die opzettelijk is gemaakt om een AI-model een fout te laten maken, vaak door subtiele verstoringen toe te voegen die voor mensen onwaarneembaar zijn.
  ​
* Robuustheid tegen adversariaal aanvallen – Adversariale robuustheid in AI verwijst naar het vermogen van een model om zijn prestaties te behouden en weerstand te bieden tegen misleiding of manipulatie door opzettelijk vervaardigde, kwaadaardige invoer die fouten moet veroorzaken.
  ​
* Agent – AI-agenten zijn software systemen die AI gebruiken om doelen na te streven en taken namens gebruikers te voltooien. Ze tonen redenering, planning en geheugen en hebben een mate van autonomie om beslissingen te nemen, te leren en zich aan te passen.
  ​
* Agentische AI: AI-systemen die met een zekere mate van autonomie kunnen opereren om doelen te bereiken, waarbij ze vaak beslissingen nemen en acties uitvoeren zonder directe menselijke tussenkomst.
  ​
* Attribute-Based Access Control (ABAC): Een toegangscontroleparadigma waarbij autorisatiebeslissingen worden genomen op basis van attributen van de gebruiker, bron, actie en omgeving, geëvalueerd op het moment van de aanvraag.
  ​
* Backdoor-aanval: Een type data-vergiftigingsaanval waarbij het model wordt getraind om op een specifieke manier te reageren op bepaalde triggers, terwijl het zich anders normaal gedraagt.
  ​
* Bias: Systematische fouten in AI-modeluitvoer die kunnen leiden tot onrechtvaardige of discriminerende uitkomsten voor bepaalde groepen of in specifieke contexten.
  ​
* Bias Exploitatie: Een aanvalstechniek die gebruikmaakt van bekende vooroordelen in AI-modellen om uitvoer of uitkomsten te manipuleren.
  ​
* Cedar: Amazons beleids-taal en machine voor fijnmazige permissies gebruikt bij de implementatie van ABAC voor AI-systemen.
  ​
* Chain of Thought: Een techniek voor het verbeteren van redeneren in taalmodellen door tussenliggende redeneringsstappen te genereren voordat een definitief antwoord wordt gegeven.
  ​
* Circuitonderbrekers: Mechanismen die automatisch de werking van AI-systemen stoppen wanneer specifieke risicodrempels worden overschreden.
  ​
* Data-lek: Onbedoelde blootstelling van gevoelige informatie via de uitvoer of het gedrag van een AI-model.
  ​
* Datavergiftiging: Het opzettelijk corrumperen van trainingsgegevens om de integriteit van het model te compromitteren, vaak om achterdeurtjes te installeren of de prestaties te verslechteren.
  ​
* Differentiële privacy – Differentiële privacy is een wiskundig rigoureus kader voor het vrijgeven van statistische informatie over datasets, terwijl de privacy van individuele gegevenssubjecten wordt beschermd. Het stelt een gegevenshouder in staat om aggregaatpatronen van de groep te delen, terwijl informatie die over specifieke individuen wordt gelekt, wordt beperkt.
  ​
* Inbeddingen: Dichte vectorrepresentaties van gegevens (tekst, afbeeldingen, enz.) die semantische betekenis vastleggen in een hoge-dimensionale ruimte.
  ​
* Uitlegbaarheid – Uitlegbaarheid in AI is het vermogen van een AI-systeem om voor mensen begrijpelijke redenen te geven voor zijn beslissingen en voorspellingen, waardoor inzicht wordt geboden in de interne werking ervan.
  ​
* Uitlegbare AI (XAI): AI-systemen die zijn ontworpen om menselijk begrijpelijke verklaringen te geven voor hun beslissingen en gedragingen via verschillende technieken en raamwerken.
  ​
* Federated Learning: Een machine learning-benadering waarbij modellen worden getraind op meerdere gedecentraliseerde apparaten die lokale datasets bevatten, zonder de data zelf uit te wisselen.
  ​
* Beveiligingsmaatregelen: Beperkingen die zijn geïmplementeerd om te voorkomen dat AI-systemen schadelijke, bevooroordeelde of anderszins ongewenste output genereren.
  ​
* Hallucinatie – Een AI-hallucinatie verwijst naar een verschijnsel waarbij een AI-model onjuiste of misleidende informatie genereert die niet is gebaseerd op zijn trainingsgegevens of feitelijke realiteit.
  ​
* Mens-in-de-lus (HITL): Systemen die zijn ontworpen om menselijke toezicht, verificatie of interventie bij cruciale besluitmomenten te vereisen.
  ​
* Infrastructure as Code (IaC): Het beheren en beschikbaar stellen van infrastructuur via code in plaats van handmatige processen, waardoor beveiligingsscans en consistente uitrol mogelijk zijn.
  ​
* Jailbreak: Technieken die worden gebruikt om veiligheidsvoorzieningen in AI-systemen, met name in grote taalmodellen, te omzeilen om verboden inhoud te produceren.
  ​
* Least Privilege: Het beveiligingsprincipe waarbij alleen de minimaal noodzakelijke toegangsrechten worden toegekend aan gebruikers en processen.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): Een techniek om de voorspellingen van elke machine learning-classificator uit te leggen door deze lokaal te benaderen met een interpreteerbaar model.
  ​
* Lidmaatschapsinference-aanval: een aanval die tot doel heeft te bepalen of een specifiek datapunt is gebruikt om een machine learning-model te trainen.
  ​
* MITRE ATLAS: Adversarial Threat Landscape voor Kunstmatige Intelligentiesystemen; een kennisbank van adversariële tactieken en technieken tegen AI-systemen.
  ​
* Modelkaart – Een modelkaart is een document dat gestandaardiseerde informatie biedt over de prestaties, beperkingen, beoogde toepassingen en ethische overwegingen van een AI-model om transparantie en verantwoordelijke AI-ontwikkeling te bevorderen.
  ​
* Model Extractie: Een aanval waarbij een tegenstander herhaaldelijk een doelfunctiemodel bevraagt om zonder toestemming een functioneel vergelijkbare kopie te maken.
  ​
* Modelinversie: Een aanval die probeert trainingsgegevens te reconstrueren door modeluitvoer te analyseren.
  ​
* Beheer van de levenscyclus van modellen – Het beheer van de levenscyclus van AI-modellen is het proces van toezicht houden op alle fasen van het bestaan van een AI-model, inclusief het ontwerp, de ontwikkeling, implementatie, monitoring, onderhoud en uiteindelijke buitengebruikstelling, om ervoor te zorgen dat het effectief blijft en aansluit bij de doelstellingen.
  ​
* Modelvergiftiging: Het direct inbrengen van kwetsbaarheden of achterdeurtjes in een model tijdens het trainingsproces.
  ​
* Modeldiefstal/-roof: Het extraheren van een kopie of benadering van een eigendomsmodel door herhaalde queries.
  ​
* Multi-agentensysteem: Een systeem dat bestaat uit meerdere interactieve AI-agenten, elk met mogelijk verschillende mogelijkheden en doelen.
  ​
* OPA (Open Policy Agent): Een open-source beleidsmotor die uniforme handhaving van beleidsregels over de gehele stack mogelijk maakt.
  ​
* Privacy-behoudende Machine Learning (PPML): Technieken en methoden om ML-modellen te trainen en te implementeren terwijl de privacy van de trainingsgegevens wordt beschermd.
  ​
* Promptinjectie: Een aanval waarbij kwaadaardige instructies in invoer worden ingebed om het beoogde gedrag van een model te overschrijven.
  ​
* RAG (Retrieval-Augmented Generation): Een techniek die grote taalmodellen verbetert door relevante informatie op te halen uit externe kennisbronnen voordat een antwoord wordt gegenereerd.
  ​
* Red-Teaming: De praktijk van het actief testen van AI-systemen door het simuleren van vijandige aanvallen om kwetsbaarheden te identificeren.
  ​
* SBOM (Software Bill of Materials): Een formeel document dat de details en toeleveringsketenrelaties bevat van diverse componenten die worden gebruikt bij het bouwen van software of AI-modellen.
  ​
* SHAP (SHapley Additive exPlanations): Een speltheoretische benadering om de output van elk machine learning-model uit te leggen door de bijdrage van elke eigenschap aan de voorspelling te berekenen.
  ​
* Supply Chain-aanval: Een systeem compromitteren door minder beveiligde elementen in de supply chain aan te vallen, zoals bibliotheken van derden, datasets of vooraf getrainde modellen.
  ​
* Transfer Learning: Een techniek waarbij een model, ontwikkeld voor één taak, wordt hergebruikt als uitgangspunt voor een model voor een tweede taak.
  ​
* Vectordatabase: Een gespecialiseerde database ontworpen om hoogdimensionale vectoren (embedding) op te slaan en efficiënte gelijkeniszoekopdrachten uit te voeren.
  ​
* Kwetsbaarheidsscanning: Geautomatiseerde tools die bekende beveiligingskwetsbaarheden in softwarecomponenten identificeren, waaronder AI-frameworks en afhankelijkheden.
  ​
* Watermarking: Technieken om onzichtbare markeringen in AI-gegenereerde inhoud in te voegen om de herkomst te volgen of AI-generatie te detecteren.
  ​
* Zero-day kwetsbaarheid: Een eerder onbekende kwetsbaarheid die aanvallers kunnen misbruiken voordat ontwikkelaars een patch creëren en uitrollen.

