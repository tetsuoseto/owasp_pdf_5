# Дадатак А: Гласарый

> Гэты комплексны гласарый змяшчае вызначэнні ключавых тэрмінаў у сферы штучнага інтэлекту, машыннага навучання і бяспекі, якія выкарыстоўваюцца на працягу ўсяго дакумента AISVS для забеспячэння яснасці і агульнага разумення.
> ​
* Адверсарны прыклад: вход, спецыяльна створаны для выклікання памылкі ў мадэлі штучнага інтэлекту, часта шляхам дадання тонкіх перашкод, якія непрымянімыя для чалавека.
  ​
* Супрацьстаянне адвэрсарыяльным уздзеянням – Супрацьстаянне адвэрсарыяльным уздзеянням у штучным інтэлекце азначае здольнасць мадэлі падтрымліваць сваю эфектыўнасць і супрацьстаяць падману або маніпуляцыям з боку намерена створаных шкодных уводных даных, прызначаных для выкліку памылак.
  ​
* Агент – AI-агенты — гэта праграмныя сістэмы, якія выкарыстоўваюць штучны інтэлект для дасягнення мэт і выканання задач ад імя карыстальнікаў. Яны праяўляюць мысленне, планаванне і памяць, а таксама маюць пэўны ўзровень аўтаномнасці для прыняцця рашэнняў, навучання і адаптацыі.
  ​
* Агенцкія сістэмы штучнага інтэлекту: сістэмы ШІ, якія могуць дзейнічаць з пэўным ступенем аўтаномнасці для дасягнення мэтаў, часта прымаючы рашэнні і выконваючы дзеянні без прамой чалавечай умяшальніцтва.
  ​
* Кіраванне доступам на аснове атрыбутаў (ABAC): парадыгма кіравання доступам, дзе рашэнні аб аўтарызацыі прымаюцца на падставе атрыбутаў карыстальніка, рэсурсу, дзеяння і асяроддзя, што ацэньваюцца ў момант запыту.
  ​
* Атакa праз задні дзверы: тып атакі з атручваннем даных, калі мадэль навучаецца рэагаваць пэўным чынам на пэўныя трыгеры, пры гэтым у іншых сітуацыях паводзіць сябе нармальна.
  ​
* Зрушэнне: сістэматычныя памылкі ў выхадных дадзеных мадэляў ШІ, якія могуць прывесці да несправядлівых або дыскрымінацыйных вынікаў для пэўных груп або ў асобных кантэкстах.
  ​
* Эксплуатацыя прадузятасцей: тэхніка атакі, якая выкарыстоўвае вядомыя прадузятасці ў мадэлях штучнага інтэлекту для маніпуляцыі вынікамі або вывадамі.
  ​
* Cedar: мова палітык і рухавік Amazon для дэталёвых дазволаў, якія выкарыстоўваюцца для рэалізацыі ABAC у AI-сістэмах.
  ​
* Ланцуг мыслення: тэхніка паляпшэння разважанняў у мадэлях мовы шляхам стварэння прамежкавых крокаў разважання перад выніковым адказам.
  ​
* Перамовы: Механізмы, якія аўтаматычна спыняюць працу сістэм штучнага інтэлекту, калі перавышаюцца пэўныя парогі рызыкі.
  ​
* Уцечка дадзеных: ненавмиснае раскрыццё канфідэнцыйнай інфармацыі праз вынікі або паводзіны мадэлі штучнага інтэлекту.
  ​
* Атака ін’екцыі шкодніцкіх дадзеных: Намеранае пашкоджанне навучальных дадзеных для падрыву цэласнасці мадэлі, часта з мэтай устаноўкі задніх дзвярэй або пагаршэння прадукцыйнасці.
  ​
* Дыферэнцыяльная прыватнасць – дыферэнцыяльная прыватнасць з'яўляецца матэматычна строгай структурай для публікацыі статыстычнай інфармацыі пра наборы даных пры гэтым абараняючы прыватнасць асобных дадзеных. Яна дазваляе ўладальніку даных падзяляць агрэгаваныя ўзоры групы, пры абмежаванні інфармацыі, якая прарастае пра канкрэтных асоб.
  ​
* Убудаванні прадстаўляюць сабой шчыльныя вектарныя прадстаўленні даных (тэкст, малюнкі і г.д.), якія захоўваюць семантычны сэнс у шматмернай прасторы.
  ​
* Тлумачальнасць – Тлумачальнасць у штучным інтэлекце – гэта здольнасць сістэмы ШІ даваць чалавеку зразумелыя прычыны сваіх рашэнняў і прагнозаў, прапаноўваючы ўяўленне пра яе ўнутраную працу.
  ​
* Тлумачальная штучны інтэлект (Explainable AI, XAI): сістэмы ШІ, распрацаваныя для прадастаўлення чалавеку зразумелых тлумачэнняў сваіх рашэнняў і паводзін з дапамогай розных метадаў і рамак.
  ​
* Федэратыўнае навучанне: падыход машыннага навучання, пры якім мадэлі навучаюцца на некалькіх дэцыентралізаваных прыладах, якія ўтрымліваюць лакальныя ўзоры даных, без абмену самімі данымі.
  ​
* Ахоўныя механізмы: Абмежаванні, укаранёныя для прадухілення стварэння штучным інтэлектам шкодных, прадубленычных або іншым чынам небажаных вынікаў.
  ​
* Галюцынацыя – Галюцынацыя ШІ азначае з'яву, калі мадэль штучнага інтэлекту генеруе няправільную або ўводзічую ў зман інфармацыю, якая не заснавана на яе навучальных даных або фактычнай рэчаіснасці.
  ​
* Чалавек у цыкле (HITL): сістэмы, распрацаваныя для патрабавання чалавечага кантролю, пацверджання або ўмяшання ў ключавых этапах прыняцця рашэнняў.
  ​
* Інфраструктура як код (IaC): кіраванне і прадастаўленне інфраструктуры праз код замест ручных працэсаў, што дазваляе выконваць сканіраванне бяспекі і забяспечваць паслядоўнае разгортванне.
  ​
* Jailbreak: Тэхнікі, якія выкарыстоўваюцца для абыходу ахоўных бар'ераў у сістэмах штучнага інтэлекту, асабліва ў вялікіх моўных мадэлях, з мэтай генерацыі забароненага кантэнту.
  ​
* Прынцып найменшых прывілеяў: бяспечны прынцып прадастаўлення толькі мінімальна неабходных правоў доступу для карыстальнікаў і працэсаў.
  ​
* LIME (локальныя інтэрпрэтавальныя мадэль-незалежныя тлумачэнні): метад тлумачэння прагнозаў любога класіфікатара машыннага навучання праз лакальную апроксімацыю інтэрпрэтавальнай мадэллю.
  ​
* Атака вызначэння ўдзелу ў навучанні: атака, якая мае на мэце вызначыць, ці быў пэўны пункт дадзеных выкарыстаны для навучання мадэлі машыннага навучання.
  ​
* MITRE ATLAS: Ляндзі пагроз супраць штучна інтэлектуальных сістэм; база ведаў пра тактыкі і метады супрацьдзеяння для сістэм ШІ.
  ​
* Мадэльная карта – гэта дакумент, які прадастаўляе стандартную інфармацыю аб эфектыўнасці мадэлі ШІ, яе абмежаваннях, прызначэннях выкарыстання і этычных аспектах для садзейнічання празрыстасці і адказнага развіцця ШІ.
  ​
* Экстракцыя мадэлі: атака, пры якой супрацьнік неаднаразова робіць запыты да мэтавай мадэлі для стварэння функцыянальна падобнай копіі без дазволу.
  ​
* Інверсія мадэлі: атака, якая спрабуе аднавіць навучальныя даныя шляхам аналізу вынікаў мадэлі.
  ​
* Кіраванне цыклам жыцця мадэлі – гэта працэс кантролю ўсіх этапаў існавання мадэлі штучнага інтэлекту, уключаючы яе праектаванне, распрацоўку, разгортванне, маніторынг, абслугоўванне і канчатковае вывад з эксплуатацыі, каб забяспечыць яе эфектыўнасць і адпаведнасць пастаўленым мэтам.
  ​
* Атака Заражання Мадэлі: Увядзенне ўразлівасцяў або сакрэтных дзвярэй непасрэдна ў мадэль падчас працэсу навучання.
  ​
* Крадзеж/Выкраданне мадэлі: Атрыманне копіі або набліжэння ўласнай мадэлі праз шматразовыя запыты.
  ​
* Сістэма з некалькімі агенцамі: сістэма, якая складаецца з некалькіх узаемадзейных AI-агентаў, кожны з якіх можа мець розныя магчымасці і мэты.
  ​
* OPA (Open Policy Agent): Адкрыты рухавік палітык, які дазваляе ўніфікаваць прымяненне палітык ва ўсім стэку.
  ​
* Абараняльнае машыннае навучанне (PPML): Тэхнікі і метады навучання і разгортвання мадэляў машыннага навучання з захаваннем прыватнасці навучальных даных.
  ​
* Ін'екцыя падказак: атака, калі шкоднасныя інструкцыі ўбудаваныя ў ўваходныя дадзеныя для перазапісу задуманай паводзінаў мадэлі.
  ​
* RAG (Retrieval-Augmented Generation): Тэхніка, якая ўзмацняе вялікія моўныя мадэлі шляхам атрымання адпаведнай інфармацыі з знешніх крыніц ведаў перад генераваннем адказу.
  ​
* Чырвонакамандны тэставанне: практыка актыўнага тэставання сістэм штучнага інтэлекту шляхам мадэлявання варожых атакаў для выяўлення ўразлівасцей.
  ​
* SBOM (Спіс матэрыялаў праграмнага забеспячэння): афіцыйны дакумент, які змяшчае дэталі і адносіны ў ланцужку паставак розных кампанентаў, якія выкарыстоўваюцца пры стварэнні праграмнага забеспячэння або мадэляў ШІ.
  ​
* SHAP (SHapley Additive exPlanations): гульнявы тэарэтычны падыход для тлумачэння выніку любой мадэлі машыннага навучання шляхам вылічэння ўнёску кожнай асаблівасці ў прагноз.
  ​
* Атака на ланцуг паставак: Кампраметацыя сістэмы праз нацэленыя дзеянні на менш абароненыя элементы ў яе ланцугу паставак, такія як бібліятэкі трэціх бакоў, наборы даных або папярэдне навучаныя мадэлі.
  ​
* Перадача навучання: тэхніка, пры якой мадэль, распрацаваная для адной задачы, выкарыстоўваецца як адпраўная кропка для мадэлі па другой задачы.
  ​
* Вектарная база даных: спецыялізаваная база даных, прызначаная для захоўвання вектароў высокай вымярэнні (эмадый) і эфектыўнага выканання пошукаў па падобнасці.
  ​
* Сканаванне ўразлівасцей: аўтаматызаваныя інструменты, якія выяўляюць вядомыя ўразлівасці бяспекі ў праграмных кампанентах, уключаючы AI-фреймворкі і залежнасці.
  ​
* Водзяныя знакі: метады ўбудовы непрыкметных маркераў у змесціва, створанае штучным інтэлектам, для адсочвання яго паходжання або выяўлення генерацыі AI.
  ​
* Уразлівасць нулявага дня: Раней невядомая ўразлівасць, якую атакуючыя могуць выкарыстаць да таго, як распрацоўшчыкі створяць і разгортваюць абнаўленне бяспекі.

