# 10 مقاومت در برابر حملات خصمانه و دفاع از حریم خصوصی

## هدف کنترل

اطمینان حاصل کنید که مدل‌های هوش مصنوعی هنگام مواجهه با حملات دور زدن، استنتاج، استخراج یا مسمومیت، قابل اعتماد، حافظ حریم خصوصی و مقاوم در برابر سوء استفاده باقی بمانند.

---

## ۱۰.۱ همسویی مدل و ایمنی

مراقب خروجی‌های مضر یا نقض‌کننده سیاست‌ها باشید.

|   #    | Description                                                                                                                                                      | Level | Role |
| :----: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.1.1 | تأیید کنید که یک مجموعه آزمایشی هم‌راستایی (دستورات تیم قرمز، آزمایش‌های فرار از محدودیت، محتوای ممنوع) تحت کنترل نسخه قرار دارد و در هر انتشار مدل اجرا می‌شود. |   1   | D/V  |
| 10.1.2 | تأیید کنید که سازوکارهای منع و محافظت در اتمام ایمن به درستی اجرا شده‌اند.                                                                                       |   1   |  D   |
| 10.1.3 | تأیید کنید که یک ارزیاب خودکار نرخ محتوای مضر را اندازه‌گیری کرده و افت کیفیت‌هایی را که از یک آستانه مشخص فراتر می‌روند، علامت‌گذاری می‌کند.                    |   2   | D/V  |
| 10.1.4 | تأیید کنید که آموزش مقابله با جیلبریک مستندسازی شده و قابل بازتولید باشد.                                                                                        |   2   |  D   |
| 10.1.5 | اطمینان حاصل کنید که اثبات‌های رسمی تطابق با سیاست یا پایش معتبر، حوزه‌های حیاتی را پوشش می‌دهند.                                                                |   3   |  V   |

---

## 10.2 سخت‌سازی در برابر نمونه‌های مخرب

افزایش مقاومت در برابر ورودی‌های دستکاری شده. آموزش متقابل مقاوم و امتیازدهی معیار فعلی بهترین روش است.

|   #    | Description                                                                                                                | Level | Role |
| :----: | -------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.2.1 | تأیید کنید که مخازن پروژه شامل پیکربندی‌های آموزش دشمن‌محور با دانه‌های بازتولیدپذیر هستند.                                |   1   |  D   |
| 10.2.2 | تأیید کنید که تشخیص نمونه‌های مخرب در خط‌های تولید هشدارهای مسدودکننده ایجاد می‌کند.                                       |   2   | D/V  |
| 10.2.4 | اطمینان حاصل کنید که اثبات‌های پایدار-تصدیق‌شده یا گواهی‌های محدوده‌فاصله حداقل کلاس‌های بحرانی برتر را پوشش می‌دهند.      |   3   |  V   |
| 10.2.5 | اطمینان حاصل کنید که تست‌های رگرسیون از حملات تطبیقی برای تأیید عدم وجود کاهش قابل اندازه‌گیری در استحکام استفاده می‌کنند. |   3   |  V   |

---

## 10.3 کاهش استنتاج عضویت

محدود کردن توانایی تصمیم‌گیری در مورد اینکه آیا یک رکورد در داده‌های آموزشی بوده است. حریم خصوصی تفاضلی و ماسک‌گذاری نمره اطمینان همچنان مؤثرترین روش‌های دفاعی شناخته‌شده باقی می‌مانند.

|   #    | Description                                                                                                                  | Level | Role |
| :----: | ---------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.3.1 | تأیید کنید که تنظیم انحراف آنتروپی به ازای هر پرسش یا مقیاس‌بندی دما، پیش‌بینی‌های بیش از حد مطمئن را کاهش می‌دهد.           |   1   |  D   |
| 10.3.2 | اطمینان حاصل کنید که آموزش از بهینه‌سازی دارای حریم خصوصی تفاضلی با کران ε برای داده‌های حساس استفاده می‌کند.                |   2   |  D   |
| 10.3.3 | اطمینان حاصل کنید که شبیه‌سازی‌های حمله (مدل سایه یا جعبه‌سیاه) نشان‌دهنده AUC حمله ≤ 0.60 روی داده‌های نگه‌داشته شده هستند. |   2   |  V   |

---

## 10.4 مقاومت در برابر وارونگی مدل

جلوگیری از بازسازی ویژگی‌های خصوصی. نظرسنجی‌های اخیر بر کوتاه‌سازی خروجی و تضمین‌های DP به عنوان دفاع‌های عملی تأکید دارند.

|   #    | Description                                                                                                                                            | Level | Role |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------------------ | :---: | :--: |
| 10.4.1 | اطمینان حاصل کنید که ویژگی‌های حساس هرگز به‌طور مستقیم خروجی داده نمی‌شوند؛ در صورت نیاز، از دسته‌بندی‌ها (buckets) یا تبدیل‌های یک‌طرفه استفاده کنید. |   1   |  D   |
| 10.4.2 | تأیید کنید که محدودیت‌های نرخ پرس‌وجو، پرس‌وجوهای تطبیقی تکراری از همان شخصیت (principal) را محدود می‌کنند.                                            |   1   | D/V  |
| 10.4.3 | اطمینان حاصل کنید که مدل با نویز حفظ حریم خصوصی آموزش دیده است.                                                                                        |   2   |  D   |

---

## 10.5 دفاع در برابر استخراج مدل

شناسایی و جلوگیری از کپی‌برداری غیرمجاز. استفاده از واترمارکینگ و تحلیل الگوی پرس‌وجو توصیه می‌شود.

|   #    | Description                                                                                                                                                | Level | Role |
| :----: | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.5.1 | اطمینان حاصل کنید که دروازه‌های استنتاج محدودیت‌های نرخ کلی و محدودیت‌های نرخ ویژه هر کلید API را که با آستانه حفظ حافظه مدل تنظیم شده‌اند، اعمال می‌کنند. |   1   |  D   |
| 10.5.2 | تأیید کنید که آمارهای انتروپی پرس و جو و چندشکلی ورودی به یک شناسگر استخراج خودکار تغذیه می‌شوند.                                                          |   2   | D/V  |
| 10.5.3 | تأیید کنید که نشانه‌های آبی شکننده یا احتمالاتی با p < 0.01 در ≤ 1 000 پرس و جو علیه یک نسخه مشکوک قابل اثبات باشند.                                       |   2   |  V   |
| 10.5.4 | اطمینان حاصل کنید که کلیدهای واترمارک و مجموعه‌های تریگر در یک ماژول امنیت سخت‌افزاری ذخیره شده و سالانه تعویض می‌شوند.                                    |   3   |  D   |
| 10.5.5 | تأیید کنید که رویدادهای استخراج-هشدار شامل پرس‌وجوهای متخلف بوده و با راهنماهای واکنش به رخدادها یکپارچه شده‌اند.                                          |   3   |  V   |

---

## 10.6 تشخیص داده‌های آلوده در زمان استنتاج

شناسایی و خنثی‌سازی ورودی‌های دارای درب پشتی یا آلوده شده.

|   #    | Description                                                                                                                   | Level | Role |
| :----: | ----------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.6.1 | اطمینان حاصل کنید که ورودی‌ها قبل از استنتاج مدل از طریق یک آشکارساز ناهنجاری (مانند STRIP، امتیازدهی سازگاری) عبور می‌کنند.  |   1   |  D   |
| 10.6.2 | تأیید کنید که آستانه‌های آشکارساز بر روی مجموعه‌های اعتبارسنجی تمیز/آلوده تنظیم شده‌اند تا کمتر از ۵٪ مثبت کاذب داشته باشند.  |   1   |  V   |
| 10.6.3 | تأیید کنید که ورودی‌هایی که به عنوان آلوده علامت‌گذاری شده‌اند، باعث فعال شدن مسدودسازی نرم و فرآیندهای بررسی انسانی می‌شوند. |   2   |  D   |
| 10.6.4 | اطمینان حاصل کنید که آشکارسازها با حملات دزدراه تطبیقی و بدون محرک مورد آزمون فشار قرار گرفته‌اند.                            |   2   |  V   |
| 10.6.5 | تأیید کنید که معیارهای اثربخشی تشخیص ثبت شده و به طور دوره‌ای با اطلاعات تهدید تازه بازبینی می‌شوند.                          |   3   |  D   |

---

## 10.7 تطبیق پویا سیاست امنیتی

به‌روزرسانی‌های سیاست امنیتی به صورت زمان واقعی بر اساس اطلاعات تهدید و تحلیل رفتاری.

|   #    | Description                                                                                                                               | Level | Role |
| :----: | ----------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.7.1 | تأیید کنید که سیاست‌های امنیتی می‌توانند به‌صورت پویا بدون راه‌اندازی مجدد عامل به‌روزرسانی شوند در حالی که صحت نسخه سیاست حفظ می‌شود.    |   1   | D/V  |
| 10.7.2 | تأیید کنید که به‌روزرسانی‌های سیاست‌ها توسط پرسنل مجاز امنیتی به‌صورت رمزنگاری‌شده امضا شده و قبل از اعمال مورد اعتبارسنجی قرار می‌گیرند. |   2   | D/V  |
| 10.7.3 | تأیید کنید که تغییرات سیاست پویا با ردپای کامل حسابرسی شامل توجیه، زنجیره‌های تأیید و رویه‌های بازگرداندن ثبت می‌شوند.                    |   2   | D/V  |
| 10.7.4 | تأیید کنید که مکانیزم‌های امنیتی تطبیقی حساسیت تشخیص تهدید را بر اساس زمینه ریسک و الگوهای رفتاری تنظیم می‌کنند.                          |   3   | D/V  |
| 10.7.5 | تأیید کنید که تصمیمات تطبیق سیاست قابل توضیح باشند و شامل شواهدی برای بررسی تیم امنیتی باشند.                                             |   3   | D/V  |

---

## 10.8 تحلیل امنیت مبتنی بر بازتاب

اعتبارسنجی امنیت از طریق خوداندیشی عامل و تحلیل فراشناختی.

|   #    | Description                                                                                                                | Level | Role |
| :----: | -------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.8.1 | اطمینان حاصل کنید که مکانیزم‌های بازتاب عامل شامل ارزیابی خودمحور متمرکز بر امنیت از تصمیمات و اقدامات هستند.              |   1   | D/V  |
| 10.8.2 | تأیید کنید که خروجی‌های بازتابی برای جلوگیری از دستکاری مکانیزم‌های خودارزیابی توسط ورودی‌های مخرب اعتبارسنجی می‌شوند.     |   2   | D/V  |
| 10.8.3 | اطمینان حاصل کنید که تحلیل امنیت فراشناختی، تعصبات بالقوه، دستکاری یا نفوذ در فرآیندهای استدلال عامل را شناسایی می‌کند.    |   2   | D/V  |
| 10.8.4 | تأیید کنید که هشدارهای امنیتی مبتنی بر بازتاب، باعث فعال شدن نظارت پیشرفته و جریان‌های کاری احتمالی مداخله انسانی می‌شوند. |   3   | D/V  |
| 10.8.5 | تأیید کنید که یادگیری مداوم از بازتاب‌های امنیتی، تشخیص تهدید را بهبود می‌بخشد بدون اینکه عملکرد قانونی را کاهش دهد.       |   3   | D/V  |

---

## ۱۰.۹ امنیت توسعه و خودبهبودی

کنترل‌های امنیتی برای سیستم‌های عامل که قادر به خودتغییری و تکامل هستند.

|   #    | Description                                                                                           | Level | Role |
| :----: | ----------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.9.1 | تأیید کنید که قابلیت‌های خودتغییر تنها به مناطق ایمن مشخص شده با محدوده‌های تأیید رسمی محدود شده‌اند. |   1   | D/V  |
| 10.9.2 | تأیید کنید که پیشنهادهای تکاملی پیش از اجرا تحت ارزیابی تأثیر امنیتی قرار می‌گیرند.                   |   2   | D/V  |
| 10.9.3 | اطمینان حاصل کنید که مکانیسم‌های خودبهبودی شامل قابلیت‌های بازگشت به حالت قبلی با تأیید صحت باشند.    |   2   | D/V  |
| 10.9.4 | تأیید کنید که امنیت یادگیری فراگیر از دستکاری خصمانه الگوریتم‌های بهبود جلوگیری می‌کند.               |   3   | D/V  |
| 10.9.5 | تایید کنید که بهبود خودبازگشتی محدود به قیود ایمنی رسمی است با اثبات‌های ریاضی همگرایی.               |   3   | D/V  |

---

### مراجع

* [MITRE ATLAS adversary tactics for ML](https://atlas.mitre.org/)
* [NIST AI Risk Management Framework 1.0, 2023](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [OWASP Top 10 for LLM Applications, 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
* [Adversarial Training: A Survey — Zhao et al., 2024](https://arxiv.org/abs/2410.15042)
* [RobustBench adversarial-robustness benchmark](https://robustbench.github.io/)
* [Membership-Inference & Model-Inversion Risk Survey, 2025](https://www.sciencedirect.com/science/article/abs/pii/S0950705125003867)
* [PURIFIER: Confidence-Score Defense against MI Attacks — AAAI 2023](https://ojs.aaai.org/index.php/AAAI/article/view/26289)
* [Model-Inversion Attacks & Defenses Survey — AI Review, 2025](https://link.springer.com/article/10.1007/s10462-025-11248-0)
* [Comprehensive Defense Framework Against Model Extraction — IEEE TDSC 2024](https://doi.org/10.1109/TDSC.2023.3261327)
* [Fragile Model Watermarking Survey — 2025](https://www.sciencedirect.com/science/article/abs/pii/S0165168425002026)
* [Data Poisoning in Deep Learning: A Survey — Zhao et al., 2025](https://arxiv.org/abs/2503.22759)
* [BDetCLIP: Multimodal Prompting Backdoor Detection — Niu et al., 2024](https://arxiv.org/abs/2405.15269)

