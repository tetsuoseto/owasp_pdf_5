# 10 مقاومت مقابل حملات متخاصم و دفاع حریم خصوصی

## هدف کنترل

اطمینان حاصل کنید که مدل‌های هوش مصنوعی هنگام مواجهه با حملات اجتناب، استنتاج، استخراج یا مسموم‌سازی، قابل اعتماد، محافظت‌کننده از حریم خصوصی و مقاوم در برابر سوءاستفاده باقی می‌مانند.

---

## ۱۰.۱ هم‌راستایی مدل و ایمنی

از تولید خروجی‌های مضر یا خلاف سیاست‌ها جلوگیری کنید.

|   #    | Description                                                                                                                                                               | Level | Role |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.1.1 | اطمینان حاصل کنید که یک مجموعه آزمایش هم‌راستایی (پرسش‌های تیم قرمز، آزمایش‌های فرار از زندان، محتوای ممنوعه) تحت کنترل نسخه قرار دارد و در هر نسخه جدید مدل اجرا می‌شود. |   1   | D/V  |
| 10.1.2 | تأیید کنید که قوانین حفاظتی مربوط به رد درخواست و اتمام ایمن به درستی اعمال شده باشند.                                                                                    |   1   |  D   |
| 10.1.3 | اطمینان حاصل کنید که یک ارزیاب خودکار نرخ محتوای مضر را اندازه‌گیری کرده و پسرفت‌ها را فراتر از یک حد آستانه مشخص علامت‌گذاری می‌کند.                                     |   2   | D/V  |
| 10.1.4 | اطمینان حاصل کنید که آموزش مقابله با دور زدن محدودیت‌ها مستند و قابل تکرار است.                                                                                           |   2   |  D   |
| 10.1.5 | اطمینان حاصل کنید که اثبات‌های رعایت سیاست‌های رسمی یا نظارت‌های گواهی‌شده حوزه‌های حیاتی را پوشش می‌دهند.                                                                |   3   |  V   |

---

## ۱۰.۲ سخت‌سازی در برابر نمونه‌های خصمانه

افزایش مقاومت در برابر ورودی‌های دستکاری شده. آموزش مقاوم در برابر حملات مخالف و امتیازدهی معیار فعلی بهترین روش‌ها هستند.

|   #    | Description                                                                                                          | Level | Role |
| :----: | -------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.2.1 | تأیید کنید که مخازن پروژه شامل پیکربندی‌های آموزش متخاصم با دانه‌های تولید مجدد‌پذیر هستند.                          |   1   |  D   |
| 10.2.2 | تأیید کنید که تشخیص مثال‌های مخرب در خطوط تولید هشدارهای مسدودکننده ایجاد می‌کند.                                    |   2   | D/V  |
| 10.2.4 | اطمینان حاصل کنید که اثبات‌های پایداری تاییدشده یا گواهی‌های بازه‌ای حداقل کلاس‌های بحرانی برتر را پوشش می‌دهند.     |   3   |  V   |
| 10.2.5 | تأیید کنید که آزمون‌های رگرسیون از حملات تطبیقی برای اطمینان از عدم افت قابل اندازه‌گیری در پایداری استفاده می‌کنند. |   3   |  V   |

---

## ۱۰.۳ کاهش حملات استنباط عضویت

محدود کردن توانایی تصمیم‌گیری درباره اینکه آیا یک رکورد در داده‌های آموزش بوده است یا خیر. حفظ حریم خصوصی تفاضلی و ماسک کردن نمره اطمینان همچنان مؤثرترین روش‌های دفاعی شناخته‌شده هستند.

|   #    | Description                                                                                                                     | Level | Role |
| :----: | ------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.3.1 | تأیید کنید که تنظیم منظم انتروپی به ازای هر پرسش یا مقیاس‌بندی دما، پیش‌بینی‌های بیش‌اعتماد به نفس را کاهش می‌دهد.              |   1   |  D   |
| 10.3.2 | اطمینان حاصل کنید که آموزش از بهینه‌سازی با حفظ حریم خصوصی تفاضلی با کران ε برای مجموعه داده‌های حساس استفاده می‌کند.           |   2   |  D   |
| 10.3.3 | اطمینان حاصل کنید که شبیه‌سازی‌های حمله (مدل سایه یا جعبه سیاه) نشان دهندۀ مقدار AUC حمله ≤ 0.60 روی داده‌های نگهداری شده باشد. |   2   |  V   |

---

## 10.4 مقاومت در برابر وارون‌سازی مدل

جلوگیری از بازسازی ویژگی‌های خصوصی. نظرسنجی‌های اخیر بر برش خروجی و تضمین‌های حریم خصوصی تفاضلی (DP) به عنوان روش‌های عملی دفاع تأکید دارند.

|   #    | Description                                                                                                                        | Level | Role |
| :----: | ---------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.4.1 | تأیید کنید که ویژگی‌های حساس هرگز به طور مستقیم خروجی داده نشوند؛ در صورت نیاز، از دسته‌بندی‌ها یا تبدیل‌های یک طرفه استفاده کنید. |   1   |  D   |
| 10.4.2 | تأیید کنید که محدودیت‌های نرخ پرس و جو، پرس و جوهای تطبیقی تکراری از همان نهاد اصلی را محدود می‌کنند.                              |   1   | D/V  |
| 10.4.3 | اطمینان حاصل کنید که مدل با نویز حفظ‌کننده حریم خصوصی آموزش داده شده است.                                                          |   2   |  D   |

---

## 10.5 دفاع در برابر استخراج مدل

شناسایی و جلوگیری از کلونینگ غیرمجاز. استفاده از علامت‌گذاری (واترمارکینگ) و تحلیل الگوی پرسش‌ها توصیه می‌شود.

|   #    | Description                                                                                                                                | Level | Role |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------ | :---: | :--: |
| 10.5.1 | تأیید کنید که دروازه‌های استنتاج محدودیت‌های نرخ کلی و به‌ازای هر کلید API را اعمال می‌کنند که بر اساس آستانه حفظ حافظه مدل تنظیم شده‌اند. |   1   |  D   |
| 10.5.2 | اطمینان حاصل کنید که آمارهای انتروپی پرس‌وجو و کثرت ورودی به یک آشکارساز استخراج خودکار تغذیه می‌شوند.                                     |   2   | D/V  |
| 10.5.3 | تأیید کنید که واترمارک‌های شکننده یا احتمالی می‌توانند با p < 0.01 در ≤ ۱۰۰۰ پرس‌وجو علیه کلون مشکوک ثابت شوند.                            |   2   |  V   |
| 10.5.4 | اطمینان حاصل کنید که کلیدهای واترمارک و مجموعه‌های محرک در یک ماژول امنیت سخت‌افزاری ذخیره شده و سالیانه تعویض می‌شوند.                    |   3   |  D   |
| 10.5.5 | اطمینان حاصل کنید که رویدادهای استخراج-هشدار شامل پرس‌وجوهای متخلف بوده و با راهنماهای پاسخ به حادثه یکپارچه شده‌اند.                      |   3   |  V   |

---

## ۱۰.۶ تشخیص داده‌های آلوده در زمان استنتاج

ورودی‌های دارای درِ پشتی یا مسموم را شناسایی و خنثی کنید.

|   #    | Description                                                                                                                   | Level | Role |
| :----: | ----------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.6.1 | اطمینان حاصل کنید که ورودی‌ها قبل از استنتاج مدل از طریق یک آشکارساز ناهنجاری (مانند STRIP، نمره‌دهی سازگاری) عبور کنند.      |   1   |  D   |
| 10.6.2 | اطمینان حاصل کنید که آستانه‌های آشکارساز روی مجموعه‌های اعتبارسنجی پاک/سمی تنظیم شده‌اند تا کمتر از ۵٪ مثبت کاذب داشته باشند. |   1   |  V   |
| 10.6.3 | اطمینان حاصل کنید که ورودی‌های علامت‌گذاری شده به عنوان آلوده، فرآیندهای مسدودسازی نرم و بررسی انسانی را فعال می‌کنند.        |   2   |  D   |
| 10.6.4 | اطمینان حاصل کنید که آشکارسازها با حملات پشتی غیرمستقیم تطبیقی بدون نیاز به ماشه تحت فشار قرار گرفته‌اند.                     |   2   |  V   |
| 10.6.5 | اطمینان حاصل کنید که معیارهای اثربخشی تشخیص ثبت می‌شوند و به‌طور دوره‌ای با اطلاعات روز تهدید مجدداً ارزیابی می‌گردند.        |   3   |  D   |

---

## ۱۰.۷ تطبیق پویا سیاست امنیتی

به‌روزرسانی‌های سیاست امنیتی به‌صورت بلادرنگ بر اساس اطلاعات تهدید و تحلیل رفتاری.

|   #    | Description                                                                                                                                           | Level | Role |
| :----: | ----------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.7.1 | اطمینان حاصل کنید که سیاست‌های امنیتی می‌توانند به‌طور پویا به‌روزرسانی شوند بدون نیاز به راه‌اندازی مجدد عامل، در حالی که صحت نسخه سیاست حفظ می‌شود. |   1   | D/V  |
| 10.7.2 | اطمینان حاصل کنید که به‌روزرسانی‌های سیاست‌ها به صورت رمزنگاری شده توسط افراد مجاز امنیتی امضا شده و قبل از اعمال تأیید شوند.                         |   2   | D/V  |
| 10.7.3 | تأیید کنید که تغییرات سیاست پویا با مسیرهای کامل حسابرسی شامل توجیه، زنجیره‌های تأیید و رویه‌های بازگشت به عقب ثبت می‌شوند.                           |   2   | D/V  |
| 10.7.4 | تأیید کنید که مکانیزم‌های امنیت تطبیقی حساسیت تشخیص تهدید را بر اساس زمینه ریسک و الگوهای رفتار تنظیم می‌کنند.                                        |   3   | D/V  |
| 10.7.5 | اطمینان حاصل کنید که تصمیمات تطبیق سیاست قابل توضیح بوده و شامل شواهد مستندی برای بازبینی تیم امنیتی باشند.                                           |   3   | D/V  |

---

## ۱۰.۸ تحلیل امنیت مبتنی بر بازتاب

اعتبارسنجی امنیت از طریق خود بازتابی عامل و تحلیل فراشناختی.

|   #    | Description                                                                                                                     | Level | Role |
| :----: | ------------------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.8.1 | تأیید کنید که مکانیزم‌های بازتاب عامل شامل ارزیابی خودمحور با تمرکز بر امنیت برای تصمیمات و اقدامات باشد.                       |   1   | D/V  |
| 10.8.2 | اطمینان حاصل کنید که خروجی‌های بازتابی اعتبارسنجی شده‌اند تا از دستکاری مکانیزم‌های خودارزیابی توسط ورودی‌های مخرب جلوگیری شود. |   2   | D/V  |
| 10.8.3 | تأیید کنید که تحلیل امنیت فراشناختی، تعصبات احتمالی، دستکاری یا تضعیف در فرایندهای استدلال عامل را شناسایی می‌کند.              |   2   | D/V  |
| 10.8.4 | تأیید کنید که هشدارهای امنیتی مبتنی بر بازتاب، نظارت پیشرفته و روندهای احتمالی مداخله انسانی را فعال می‌کنند.                   |   3   | D/V  |
| 10.8.5 | تأیید کنید که یادگیری پیوسته از بازتاب‌های امنیتی، تشخیص تهدید را بهبود می‌بخشد بدون اینکه عملکرد قانونی را کاهش دهد.           |   3   | D/V  |

---

## ۱۰.۹ امنیت تکامل و خودبهبودی

کنترل‌های امنیتی برای سیستم‌های عامل با قابلیت خودتغییری و تکامل.

|   #    | Description                                                                                                         | Level | Role |
| :----: | ------------------------------------------------------------------------------------------------------------------- | :---: | :--: |
| 10.9.1 | تایید کنید که قابلیت‌های خودتغییری محدود به مناطق امن مشخص شده با مرزهای رسمی تایید هستند.                          |   1   | D/V  |
| 10.9.2 | اطمینان حاصل کنید که پیشنهادهای توسعه قبل از اجرا تحت ارزیابی تأثیر امنیتی قرار می‌گیرند.                           |   2   | D/V  |
| 10.9.3 | اطمینان حاصل کنید که مکانیزم‌های خودبهبودی شامل قابلیت بازگشت با تأیید صحت هستند.                                   |   2   | D/V  |
| 10.9.4 | تأیید کنید که امنیت یادگیری فرامتا از دستکاری خصمانه الگوریتم‌های بهبود جلوگیری می‌کند.                             |   3   | D/V  |
| 10.9.5 | اثبات کنید که بهبود بازگشتی خودکار توسط محدودیت‌های ایمنی رسمی محافظت می‌شود و با شواهد ریاضی همگرایی آن تایید شود. |   3   | D/V  |

---

### منابع

* [MITRE ATLAS adversary tactics for ML](https://atlas.mitre.org/)
* [NIST AI Risk Management Framework 1.0, 2023](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [OWASP Top 10 for LLM Applications, 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
* [Adversarial Training: A Survey — Zhao et al., 2024](https://arxiv.org/abs/2410.15042)
* [RobustBench adversarial-robustness benchmark](https://robustbench.github.io/)
* [Membership-Inference & Model-Inversion Risk Survey, 2025](https://www.sciencedirect.com/science/article/abs/pii/S0950705125003867)
* [PURIFIER: Confidence-Score Defense against MI Attacks — AAAI 2023](https://ojs.aaai.org/index.php/AAAI/article/view/26289)
* [Model-Inversion Attacks & Defenses Survey — AI Review, 2025](https://link.springer.com/article/10.1007/s10462-025-11248-0)
* [Comprehensive Defense Framework Against Model Extraction — IEEE TDSC 2024](https://doi.org/10.1109/TDSC.2023.3261327)
* [Fragile Model Watermarking Survey — 2025](https://www.sciencedirect.com/science/article/abs/pii/S0165168425002026)
* [Data Poisoning in Deep Learning: A Survey — Zhao et al., 2025](https://arxiv.org/abs/2503.22759)
* [BDetCLIP: Multimodal Prompting Backdoor Detection — Niu et al., 2024](https://arxiv.org/abs/2405.15269)

