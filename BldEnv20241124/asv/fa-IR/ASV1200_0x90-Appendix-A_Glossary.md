# پیوست الف: واژه‌نامه

> این فرهنگ لغت جامع تعاریف اصطلاحات کلیدی هوش مصنوعی، یادگیری ماشین، و امنیت را که در سراسر AISVS استفاده شده‌اند، برای اطمینان از وضوح و فهم مشترک ارائه می‌دهد.
> ​
* نمونه مخرب: ورودی که به‌طور عمدی ساخته شده است تا باعث اشتباه یک مدل هوش مصنوعی شود، اغلب با افزودن تغییرات جزئی که برای انسان‌ها قابل‌تشخیص نیستند.
  ​
* مقاومت در برابر حملات خصمانه – مقاومت در برابر حملات خصمانه در هوش مصنوعی به توانایی یک مدل برای حفظ عملکرد خود و مقاومت در برابر فریب خوردن یا دستکاری توسط ورودی‌های مخرب، که به عمد طراحی شده‌اند تا خطا ایجاد کنند، اشاره دارد.
  ​
* عامل – عوامل هوش مصنوعی سیستم‌های نرم‌افزاری هستند که از هوش مصنوعی برای پیگیری اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. آنها توانایی استدلال، برنامه‌ریزی و حافظه دارند و درجه‌ای از خودمختاری برای اتخاذ تصمیم، یادگیری و سازگاری را دارا هستند.
  ​
* هوش مصنوعی عاملی: سیستم‌های هوش مصنوعی که می‌توانند با درجه‌ای از خودمختاری عمل کنند تا اهداف را دنبال کنند، اغلب تصمیم‌گیری کرده و اقداماتی را بدون دخالت مستقیم انسان انجام می‌دهند.
  ​
* کنترل دسترسی مبتنی بر ویژگی‌ها (ABAC): یک الگوی کنترل دسترسی که تصمیم‌گیری‌های مجوزدهی بر اساس ویژگی‌های کاربر، منبع، اقدام و محیط انجام می‌شود و در زمان پرس‌وجو ارزیابی می‌گردد.
  ​
* حمله درب‌پشتی: نوعی حمله مسموم‌سازی داده که در آن مدل به گونه‌ای آموزش می‌بیند که به محرک‌های خاصی به شکل مشخصی پاسخ دهد در حالی که در سایر موارد به طور عادی رفتار می‌کند.
  ​
* تعصب: خطاهای سیستماتیکی در خروجی مدل‌های هوش مصنوعی که می‌توانند منجر به نتایج ناعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های مشخص شوند.
  ​
* بهره‌برداری از تعصبات: تکنیکی حمله که از تعصبات شناخته‌شده در مدل‌های هوش مصنوعی برای دستکاری خروجی‌ها یا نتایج استفاده می‌کند.
  ​
* Cedar: زبان سیاست و موتور آمازون برای مجوزهای دقیق در اعمال ABAC برای سیستم‌های هوش مصنوعی.
  ​
* زنجیره تفکر: یک تکنیک برای بهبود استدلال در مدل‌های زبانی با تولید گام‌های میانی استدلال قبل از ارائه پاسخ نهایی.
  ​
* قطع‌کننده‌ها: مکانیزم‌هایی که به‌طور خودکار عملیات سیستم هوش مصنوعی را زمانی که آستانه‌های ریسک خاصی بیشتر شوند، متوقف می‌کنند.
  ​
* نشت داده: افشای ناخواسته اطلاعات حساس از طریق خروجی‌ها یا رفتار مدل هوش مصنوعی.
  ​
* آلوده‌سازی داده‌ها: فساد عمدی داده‌های آموزشی به منظور آسیب رساندن به یکپارچگی مدل، که اغلب به منظور نصب درهای پشتی یا کاهش عملکرد انجام می‌شود.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی چارچوبی ریاضیاتی دقیق برای انتشار اطلاعات آماری درباره داده‌ها است در حالی که حریم خصوصی افراد را حفظ می‌کند. این چارچوب به دارنده داده امکان می‌دهد الگوهای تجمعی گروه را به اشتراک بگذارد در حالی که اطلاعاتی که درباره افراد خاص فاش می‌شود را محدود می‌کند.
  ​
* بردارهای جاسازی شده: نمایش‌های برداری متراکم از داده‌ها (متن، تصاویر و غیره) که معنای معنایی را در فضای با ابعاد بالا ثبت می‌کنند.
  ​
* توضیح‌پذیری – توضیح‌پذیری در هوش مصنوعی به توانایی یک سیستم هوش مصنوعی در ارائه دلایل قابل فهم برای انسان‌ها درباره تصمیمات و پیش‌بینی‌هایش گفته می‌شود، که بینشی در مورد عملکرد درونی آن ارائه می‌دهد.
  ​
* هوش مصنوعی قابل تبیین (XAI): سیستم‌های هوش مصنوعی که به منظور ارائه توضیحات قابل درک برای انسان درباره تصمیمات و رفتارهای خود، از طریق تکنیک‌ها و چارچوب‌های مختلف طراحی شده‌اند.
  ​
* یادگیری فدرال: رویکردی در یادگیری ماشین که در آن مدل‌ها بر روی دستگاه‌های غیرمتمرکز متعدد که نمونه‌های داده محلی را نگه می‌دارند آموزش داده می‌شوند، بدون اینکه خود داده‌ها مبادله شوند.
  ​
* محدودیت‌ها: قیدهایی که به منظور جلوگیری از تولید خروجی‌های مضر، جانبدارانه یا سایر خروجی‌های نامطلوب توسط سیستم‌های هوش مصنوعی اعمال می‌شوند.
  ​
* توهم – توهم هوش مصنوعی به پدیده‌ای اطلاق می‌شود که در آن یک مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده‌ای تولید می‌کند که بر اساس داده‌های آموزشی یا واقعیت‌های عینی نیست.
  ​
* انسان در حلقه (HITL): سیستم‌هایی که برای نیاز به نظارت، تأیید یا مداخله انسانی در نقاط تصمیم‌گیری حیاتی طراحی شده‌اند.
  ​
* زیرساخت به عنوان کد (IaC): مدیریت و تأمین زیرساخت از طریق کد به جای فرآیندهای دستی، که امکان اسکن امنیتی و استقرارهای سازگار را فراهم می‌کند.
  ​
* جیل‌بریک: تکنیک‌هایی که برای دور زدن سدهای ایمنی در سیستم‌های هوش مصنوعی، به‌ویژه در مدل‌های زبان بزرگ، استفاده می‌شوند تا محتوای ممنوعه تولید کنند.
  ​
* حداقل امتیاز: اصل امنیتی اعطای فقط حداقل حقوق دسترسی لازم به کاربران و فرایندها.
  ​
* LIME (توضیحات قابل تفسیر مدل-عام محلی): تکنیکی برای توضیح پیش‌بینی‌های هر طبقه‌بند یادگیری ماشین با تقریب زدن محلی آن با یک مدل قابل تفسیر.
  ​
* حمله استنتاج عضویت: حمله‌ای که هدف آن تعیین این است که آیا یک داده مشخص در آموزش مدل یادگیری ماشین استفاده شده است یا خیر.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدات خصمانه برای سیستم‌های هوش مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های خصمانه علیه سیستم‌های هوش مصنوعی.
  ​
* کارت مدل – کارت مدل یک سند است که اطلاعات استاندارد شده‌ای درباره عملکرد مدل هوش مصنوعی، محدودیت‌ها، کاربردهای مورد نظر و ملاحظات اخلاقی آن ارائه می‌دهد تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج کند.
  ​
* استخراج مدل: حمله‌ای که در آن یک مهاجم به طور مکرر به مدل هدف پرسش می‌دهد تا نسخه‌ای عملکردی مشابه و بدون اجازه از آن ایجاد کند.
  ​
* معکوس‌سازی مدل: حمله‌ای که با تحلیل خروجی‌های مدل تلاش می‌کند داده‌های آموزشی را بازسازی کند.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل هوش مصنوعی فرآیند نظارت بر تمام مراحل وجود یک مدل هوش مصنوعی است، از جمله طراحی، توسعه، استقرار، نظارت، نگهداری و نهایتاً بازنشستگی آن، به منظور اطمینان از اینکه مدل همچنان مؤثر و هم‌راستا با اهداف باقی می‌ماند.
  ​
* آلوده‌سازی مدل: وارد کردن آسیب‌پذیری‌ها یا درهای پشتی به طور مستقیم در یک مدل طی فرآیند آموزش.
  ​
* سرقت/دزدیدن مدل: استخراج نسخه یا تقریب مدل اختصاصی از طریق پرس‌وجوهای مکرر.
  ​
* سامانه چندعامله: سیستمی متشکل از چندین عامل هوش مصنوعی تعاملی که هر کدام ممکن است قابلیت‌ها و اهداف متفاوتی داشته باشند.
  ​
* OPA (اپن پالیسی ایجنت): یک موتور سیاست منبع باز که امکان اجرای یکپارچه سیاست‌ها در سراسر پشته را فراهم می‌کند.
  ​
* یادگیری ماشین حفظ حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و استقرار مدل‌های یادگیری ماشین با حفظ حریم خصوصی داده‌های آموزشی.
  ​
* تزریق پرامپت: حمله‌ای که در آن دستورالعمل‌های مخرب در ورودی‌ها جاسازی می‌شوند تا رفتار مورد نظر مدل را لغو کنند.
  ​
* RAG (تولید تقویت‌شده با بازیابی): تکنیکی که مدل‌های زبان بزرگ را با بازیابی اطلاعات مرتبط از منابع دانش خارجی قبل از تولید پاسخ، بهبود می‌بخشد.
  ​
* رد تیمینگ: عملیاتی که شامل آزمایش فعال سیستم‌های هوش مصنوعی با شبیه‌سازی حملات خصمانه برای شناسایی آسیب‌پذیری‌ها است.
  ​
* فهرست مواد نرم‌افزاری (SBOM): یک سند رسمی حاوی جزئیات و روابط زنجیره تأمین اجزای مختلف استفاده شده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی.
  ​
* SHAP (توضیحات جبرانی شاپلی): رویکردی مبتنی بر نظریه بازی‌ها برای تبیین خروجی هر مدل یادگیری ماشین با محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* حمله زنجیره تامین: به خطر انداختن یک سیستم با هدف قرار دادن عناصر کمتر امن در زنجیره تامین آن، مانند کتابخانه‌های شخص ثالث، مجموعه داده‌ها یا مدل‌های پیش‌آموزش‌دیده.
  ​
* یادگیری انتقالی: تکنیکی که در آن مدل توسعه یافته برای یک وظیفه به عنوان نقطه شروع برای مدل‌سازی روی وظیفه دوم مورد استفاده مجدد قرار می‌گیرد.
  ​
* پایگاه داده برداری: یک پایگاه داده تخصصی طراحی شده برای ذخیره بردارهای با ابعاد بالا (برداشت‌ها) و انجام جستجوی مشابهت به صورت کارآمد.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکار که آسیب‌پذیری‌های امنیتی شناخته‌شده در اجزای نرم‌افزاری، از جمله چارچوب‌ها و وابستگی‌های هوش مصنوعی را شناسایی می‌کنند.
  ​
* واترمارکینگ: تکنیک‌هایی برای جاسازی نشانگرهای غیرقابل‌تشخیص در محتوای تولیدشده توسط هوش مصنوعی به منظور ردیابی منبع آن یا شناسایی تولید توسط هوش مصنوعی.
  ​
* آسیب‌پذیری صفر روزه: آسیب‌پذیری‌ای که قبلاً ناشناخته بوده و مهاجمان می‌توانند پیش از آنکه توسعه‌دهندگان وصله‌ای ایجاد و اعمال کنند، از آن سوءاستفاده نمایند.

