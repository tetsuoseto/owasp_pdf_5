# پیوست الف: فرهنگ اصطلاحات

> این واژه‌نامه جامع، تعاریف اصطلاحات کلیدی هوش مصنوعی، یادگیری ماشین و امنیت را که در سراسر AISVS به کار رفته‌اند، برای اطمینان از وضوح و درک مشترک ارائه می‌دهد.
> ​
* مثال خصمانه: ورودی‌ای که عمداً طراحی شده تا مدل هوش مصنوعی را وادار به اشتباه کند، اغلب با افزودن اختلالات ظریف و غیرقابل‌تشخیص برای انسان‌ها.
  ​
* مقاومت مقابله‌ای – مقاومت مقابله‌ای در هوش مصنوعی به توانایی مدل در حفظ عملکرد خود و مقاومت در برابر فریب خوردن یا دستکاری توسط ورودی‌های عمدی و مخرب گفته می‌شود که با هدف ایجاد خطا طراحی شده‌اند.
  ​
* عامل – عامل‌های هوش مصنوعی سیستم‌های نرم‌افزاری هستند که از هوش مصنوعی برای دنبال کردن اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. آن‌ها استدلال، برنامه‌ریزی و حافظه را نشان می‌دهند و دارای سطحی از خودمختاری برای اتخاذ تصمیمات، یادگیری و سازگاری هستند.
  ​
* هوش مصنوعی عامل‌دار (Agentic AI): سیستم‌های هوش مصنوعی که می‌توانند با درجه‌ای از خودمختاری عمل کنند تا اهدافی را محقق سازند، اغلب با اتخاذ تصمیم‌ها و انجام اقدامات بدون مداخله مستقیم انسان.
  ​
* کنترل دسترسی مبتنی بر ویژگی (ABAC): یک الگوی کنترل دسترسی است که تصمیمات مجوزدهی بر اساس ویژگی‌های کاربر، منبع، عمل و محیط گرفته می‌شود و در زمان درخواست ارزیابی می‌گردد.
  ​
* حمله درب‌پشتی: نوعی حمله مسموم‌سازی داده که در آن مدل به گونه‌ای آموزش داده می‌شود که به محرک‌های خاص به صورت مخصوص پاسخ دهد در حالی که در شرایط عادی رفتار طبیعی دارد.
  ​
* جانبداری: خطاهای سیستماتیکی در خروجی‌های مدل هوش مصنوعی که می‌توانند به نتایج ناعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های مشخص منجر شوند.
  ​
* سوء استفاده از تعصب: یک تکنیک حمله که از تعصبات شناخته شده در مدل‌های هوش مصنوعی برای دستکاری خروجی‌ها یا نتایج استفاده می‌کند.
  ​
* Cedar: زبان سیاست آمازون و موتور آن برای دسترسی‌های دقیق و جزئی که در پیاده‌سازی ABAC برای سیستم‌های هوش مصنوعی استفاده می‌شود.
  ​
* زنجیره تفکر: تکنیکی برای بهبود استدلال در مدل‌های زبانی از طریق تولید مراحل میانی استدلال قبل از ارائه پاسخ نهایی.
  ​
* شکست‌گرهای مدار: مکانیزم‌هایی که به طور خودکار عملیات سیستم هوش مصنوعی را زمانی که آستانه‌های خاصی از ریسک فراتر می‌رود متوقف می‌کنند.
  ​
* نشت داده: افشای ناخواسته اطلاعات حساس از طریق خروجی‌ها یا رفتار مدل هوش مصنوعی.
  ​
* آلوده‌سازی داده‌ها: تخریب عمدی داده‌های آموزشی به منظور به خطر انداختن صحت مدل، که اغلب برای نصب درهای پشتی یا کاهش عملکرد انجام می‌شود.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی چارچوبی ریاضیاتی محکم برای انتشار اطلاعات آماری درباره مجموعه داده‌ها است در حالی که حریم خصوصی افراد داده‌شده را حفظ می‌کند. این امکان را برای دارنده داده فراهم می‌کند تا الگوهای جمعی گروه را به اشتراک بگذارد در حالی که اطلاعات فاش شده درباره افراد خاص را محدود می‌کند.
  ​
* توکارهای جاسازی شده: نمایش‌های برداری متراکم از داده‌ها (متن، تصاویر و غیره) که معنای معنایی را در یک فضای با ابعاد بالا ضبط می‌کنند.
  ​
* قابلیت توضیح‌پذیری – قابلیت توضیح‌پذیری در هوش مصنوعی به توانایی یک سامانه هوش مصنوعی در ارائه دلایل قابل فهم برای انسان درباره تصمیمات و پیش‌بینی‌های خود اشاره دارد که دیدگاهی درباره عملکرد داخلی آن فراهم می‌کند.
  ​
* هوش مصنوعی قابل توضیح (XAI): سیستم‌های هوش مصنوعی که برای ارائه توضیحات قابل فهم توسط انسان درباره تصمیمات و رفتارهای خود از طریق تکنیک‌ها و چارچوب‌های مختلف طراحی شده‌اند.
  ​
* یادگیری فدراتیو: رویکردی در یادگیری ماشین که در آن مدل‌ها در چندین دستگاه غیرمتمرکز که نمونه‌های داده محلی را نگهداری می‌کنند، آموزش داده می‌شوند، بدون اینکه خود داده‌ها مبادله شوند.
  ​
* محدودیت‌ها: محدودیت‌هایی که برای جلوگیری از تولید خروجی‌های مضر، جانبدارانه یا غیرقابل قبول توسط سیستم‌های هوش مصنوعی اعمال می‌شوند.
  ​
* توهم – توهم در هوش مصنوعی به پدیده‌ای اطلاق می‌شود که در آن مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده‌ای تولید می‌کند که بر اساس داده‌های آموزشی یا واقعیت‌های علمی نیست.
  ​
* انسان در حلقه (Human-in-the-Loop - HITL): سیستم‌هایی که به گونه‌ای طراحی شده‌اند که نیاز به نظارت، تایید یا دخالت انسانی در نقاط حیاتی تصمیم‌گیری دارند.
  ​
* زیرساخت به عنوان کد (IaC): مدیریت و تأمین زیرساخت از طریق کد به جای فرآیندهای دستی، که امکان اسکن امنیتی و استقرارهای سازگار را فراهم می‌کند.
  ​
* فرار از محدودیت: تکنیک‌هایی که برای دور زدن محافظ‌های ایمنی در سیستم‌های هوش مصنوعی، به‌ویژه در مدل‌های زبانی بزرگ، برای تولید محتوای ممنوعه استفاده می‌شوند.
  ​
* حداقل امتیاز: اصل امنیتی اعطای فقط حداقل حقوق دسترسی لازم به کاربران و فرایندها.
  ​
* LIME (توضیحات مدل-عاملی تفسیرپذیر محلی): تکنیکی برای توضیح پیش‌بینی‌های هر مدل طبقه‌بندی یادگیری ماشین با تقریب زدن محلی آن با مدلی قابل تفسیر است.
  ​
* حمله استنتاج عضویت: حمله‌ای که هدف آن تعیین این است که آیا یک داده خاص برای آموزش مدل یادگیری ماشین استفاده شده است یا خیر.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدهای خصمانه برای سیستم‌های هوش مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های خصمانه علیه سیستم‌های هوش مصنوعی.
  ​
* کارت مدل – کارت مدل یک سند است که اطلاعات استاندارد شده‌ای در مورد عملکرد مدل هوش مصنوعی، محدودیت‌ها، کاربردهای مورد نظر و ملاحظات اخلاقی آن فراهم می‌کند تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج دهد.
  ​
* استخراج مدل: حمله‌ای که در آن یک مهاجم به طور مکرر از مدل هدف پرسش می‌کند تا یک کپی عملکردی مشابه بدون مجوز ایجاد کند.
  ​
* وارون‌سازی مدل: حمله‌ای که تلاش می‌کند با تحلیل خروجی‌های مدل، داده‌های آموزشی را بازسازی کند.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل هوش مصنوعی فرایند نظارت بر تمامی مراحل وجود یک مدل هوش مصنوعی است، از جمله طراحی، توسعه، استقرار، پایش، نگهداری و بازنشستگی نهایی آن، به منظور اطمینان از اثربخشی و تطابق مدل با اهداف.
  ​
* آلوده‌سازی مدل: وارد کردن آسیب‌پذیری‌ها یا درهای پشتی به طور مستقیم در مدل در طول فرآیند آموزش.
  ​
* سرقت/دزدی مدل: استخراج یک نسخه یا تقریب از یک مدل اختصاصی از طریق پرس و جوهای مکرر.
  ​
* سیستم چندعامله: سیستمی متشکل از چندین عامل هوش مصنوعی که با یکدیگر تعامل دارند و هر کدام ممکن است قابلیت‌ها و اهداف متفاوتی داشته باشند.
  ​
* OPA (عامل سیاست‌گذاری باز): یک موتور سیاست‌گذاری متن‌باز که امکان اجرای یکپارچه سیاست‌ها را در کل ساختار فراهم می‌کند.
  ​
* یادگیری ماشینی حفظ حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و استقرار مدل‌های یادگیری ماشینی در حالی که حریم خصوصی داده‌های آموزشی حفظ می‌شود.
  ​
* تزریق دستور: حمله‌ای که در آن دستورات مخرب در ورودی‌ها جاسازی می‌شوند تا رفتار مورد نظر مدل را نادیده بگیرند.
  ​
* RAG (تولید تقویت‌شده با بازیابی): تکنیکی که مدل‌های زبان بزرگ را با بازیابی اطلاعات مرتبط از منابع دانش خارجی قبل از تولید پاسخ بهبود می‌بخشد.
  ​
* رد-تیمینگ: عملیاتی برای آزمایش فعال سیستم‌های هوش مصنوعی از طریق شبیه‌سازی حملات خصمانه به منظور شناسایی آسیب‌پذیری‌ها.
  ​
* SBOM (فهرست مواد نرم‌افزاری): یک سند رسمی شامل جزئیات و روابط زنجیره تأمین اجزای مختلف مورد استفاده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی.
  ​
* SHAP (توضیحات جمع‌پذیر شاپلی): رویکردی مبتنی بر نظریه بازی‌ها برای توضیح خروجی هر مدل یادگیری ماشین با محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* حمله زنجیره تأمین: به خطر انداختن یک سیستم با هدف قرار دادن عناصر کمتر ایمن در زنجیره تأمین آن، مانند کتابخانه‌های شخص ثالث، داده‌مجموعه‌ها یا مدل‌های پیش‌آموزش دیده.
  ​
* یادگیری انتقالی: تکنیکی که در آن مدلی که برای یک وظیفه توسعه یافته است، به عنوان نقطه شروع برای مدلی در وظیفه دوم مورد استفاده مجدد قرار می‌گیرد.
  ​
* پایگاه داده برداری: یک پایگاه داده تخصصی طراحی شده برای ذخیره بردارهای با ابعاد زیاد (تعبیه‌ها) و انجام جستجوهای مشابهت کارآمد.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکار که آسیب‌پذیری‌های امنیتی شناخته‌شده در اجزای نرم‌افزاری، از جمله چارچوب‌ها و وابستگی‌های هوش مصنوعی را شناسایی می‌کنند.
  ​
* نشانه‌گذاری دیجیتال: تکنیک‌هایی برای جاسازی نشانگرهای غیرقابل تشخیص در محتوای تولید شده توسط هوش مصنوعی به منظور ردیابی منشا آن یا شناسایی تولید توسط هوش مصنوعی.
  ​
* آسیب‌پذیری صفر روز: یک آسیب‌پذیری که قبلاً ناشناخته بوده و مهاجمان می‌توانند قبل از اینکه توسعه‌دهندگان وصله‌ای ایجاد و منتشر کنند، از آن سوء استفاده کنند.

