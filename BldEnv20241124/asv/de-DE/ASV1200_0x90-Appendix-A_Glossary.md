# Anhang A: Glossar

> Dieses umfassende Glossar liefert Definitionen wichtiger Begriffe aus den Bereichen KI, ML und Sicherheit, die im gesamten AISVS verwendet werden, um Klarheit und ein gemeinsames Verständnis zu gewährleisten.
> ​
* Adversariales Beispiel: Eine absichtlich gestaltete Eingabe, die ein KI-Modell dazu bringt, einen Fehler zu machen, oft durch Hinzufügen subtiler Störungen, die für Menschen nicht wahrnehmbar sind.
  ​
* Adversarielle Robustheit – Adversarielle Robustheit in der KI bezeichnet die Fähigkeit eines Modells, seine Leistung aufrechtzuerhalten und resistent dagegen zu sein, durch absichtlich gestaltete, bösartige Eingaben, die Fehler verursachen sollen, getäuscht oder manipuliert zu werden.
  ​
* Agent – KI-Agenten sind Softwaresysteme, die KI nutzen, um Ziele zu verfolgen und Aufgaben im Auftrag von Nutzern zu erledigen. Sie zeigen Fähigkeiten wie Schlussfolgern, Planen und Erinnern und verfügen über ein Maß an Autonomie, um Entscheidungen zu treffen, zu lernen und sich anzupassen.
  ​
* Agentische KI: KI-Systeme, die mit einem gewissen Grad an Autonomie agieren können, um Ziele zu erreichen, häufig Entscheidungen treffen und Maßnahmen ergreifen, ohne direkte menschliche Intervention.
  ​
* Attributbasierte Zugriffskontrolle (ABAC): Ein Zugriffskontrollparadigma, bei dem Autorisierungsentscheidungen auf Attributen des Benutzers, der Ressource, der Aktion und der Umgebung basieren und zur Abfragezeit bewertet werden.
  ​
* Backdoor-Angriff: Eine Art von Datenvergiftungsangriff, bei dem das Modell darauf trainiert wird, auf bestimmte Auslöser auf eine spezifische Weise zu reagieren, während es sich ansonsten normal verhält.
  ​
* Bias: Systematische Fehler in den Ausgaben von KI-Modellen, die zu unfairen oder diskriminierenden Ergebnissen für bestimmte Gruppen oder in spezifischen Kontexten führen können.
  ​
* Bias-Ausnutzung: Eine Angriffstechnik, die bekannte Verzerrungen in KI-Modellen ausnutzt, um Ausgaben oder Ergebnisse zu manipulieren.
  ​
* Cedar: Amazons Richtliniensprache und -motor für fein abgestufte Berechtigungen, die bei der Implementierung von ABAC für KI-Systeme verwendet wird.
  ​
* Gedankenkette: Eine Technik zur Verbesserung des Denkvermögens in Sprachmodellen, bei der Zwischenschritte des Denkprozesses erzeugt werden, bevor eine endgültige Antwort gegeben wird.
  ​
* Circuit Breaker: Mechanismen, die den Betrieb von KI-Systemen automatisch stoppen, wenn bestimmte Risikoschwellen überschritten werden.
  ​
* Datenleckage: Unbeabsichtigte Offenlegung sensibler Informationen durch die Ausgaben oder das Verhalten von KI-Modellen.
  ​
* Datenvergiftung: Die absichtliche Korruption von Trainingsdaten, um die Modellintegrität zu beeinträchtigen, häufig um Hintertüren einzubauen oder die Leistung zu verschlechtern.
  ​
* Differenzielle Privatsphäre – Differenzielle Privatsphäre ist ein mathematisch rigoroser Rahmen zur Veröffentlichung statistischer Informationen über Datensätze, während die Privatsphäre einzelner Datenpersonen geschützt wird. Sie ermöglicht einem Dateninhaber, aggregierte Muster der Gruppe zu teilen und gleichzeitig die Weitergabe von Informationen über einzelne Personen zu begrenzen.
  ​
* Einbettungen: Dichtere Vektor-Darstellungen von Daten (Text, Bilder usw.), die semantische Bedeutungen in einem hochdimensionalen Raum erfassen.
  ​
* Erklärbarkeit – Erklärbarkeit in der KI ist die Fähigkeit eines KI-Systems, für seine Entscheidungen und Vorhersagen nachvollziehbare Gründe zu liefern und Einblicke in seine inneren Abläufe zu geben.
  ​
* Erklärbare KI (XAI): KI-Systeme, die entwickelt wurden, um durch verschiedene Techniken und Rahmenwerke menschenverständliche Erklärungen für ihre Entscheidungen und ihr Verhalten zu liefern.
  ​
* Föderiertes Lernen: Ein Ansatz des maschinellen Lernens, bei dem Modelle über mehrere dezentralisierte Geräte mit lokalen Datensätzen trainiert werden, ohne die Daten selbst auszutauschen.
  ​
* Leitplanken: Einschränkungen, die implementiert werden, um zu verhindern, dass KI-Systeme schädliche, voreingenommene oder anderweitig unerwünschte Ergebnisse erzeugen.
  ​
* Halluzination – Eine KI-Halluzination bezeichnet ein Phänomen, bei dem ein KI-Modell falsche oder irreführende Informationen generiert, die weder auf seinen Trainingsdaten noch auf der tatsächlichen Realität basieren.
  ​
* Mensch-in-der-Schleife (HITL): Systeme, die darauf ausgelegt sind, menschliche Aufsicht, Überprüfung oder Eingriffe an entscheidenden Entscheidungspunkten zu erfordern.
  ​
* Infrastructure as Code (IaC): Verwaltung und Bereitstellung von Infrastruktur durch Code anstelle manueller Prozesse, wodurch Sicherheitsüberprüfungen und konsistente Bereitstellungen ermöglicht werden.
  ​
* Jailbreak: Techniken, die verwendet werden, um Sicherheitsvorkehrungen in KI-Systemen, insbesondere bei großen Sprachmodellen, zu umgehen und verbotene Inhalte zu erzeugen.
  ​
* Least Privilege: Das Sicherheitsprinzip, nur die minimal notwendigen Zugriffsrechte für Benutzer und Prozesse zu gewähren.
  ​
* LIME (Lokale Interpretable Modell-agnostische Erklärungen): Eine Technik zur Erklärung der Vorhersagen beliebiger maschineller Lernklassifikatoren durch lokale Annäherung mit einem interpretierbaren Modell.
  ​
* Mitgliedschafts-Inferenzangriff: Ein Angriff, der darauf abzielt zu bestimmen, ob ein bestimmter Datenpunkt zum Trainieren eines Machine-Learning-Modells verwendet wurde.
  ​
* MITRE ATLAS: Bedrohungslandschaft für künstliche Intelligenzsysteme durch gegnerische Angriffe; eine Wissensdatenbank zu gegnerischen Taktiken und Techniken gegen KI-Systeme.
  ​
* Modellkarte – Eine Modellkarte ist ein Dokument, das standardisierte Informationen über die Leistung, Einschränkungen, vorgesehenen Anwendungen und ethischen Überlegungen eines KI-Modells bereitstellt, um Transparenz und verantwortungsbewusste KI-Entwicklung zu fördern.
  ​
* Modellauslesung: Ein Angriff, bei dem ein Angreifer wiederholt Anfragen an ein Zielmodell stellt, um ohne Autorisierung eine funktional ähnliche Kopie zu erstellen.
  ​
* Modellinversion: Ein Angriff, der versucht, Trainingsdaten durch Analyse der Modellausgaben zu rekonstruieren.
  ​
* Modell-Lebenszyklus-Management – Das AI Modell-Lebenszyklus-Management ist der Prozess der Überwachung aller Phasen der Existenz eines KI-Modells, einschließlich Design, Entwicklung, Bereitstellung, Überwachung, Wartung und schließlich der Außerdienststellung, um sicherzustellen, dass es effektiv bleibt und mit den Zielen übereinstimmt.
  ​
* Modellvergiftung: Direkte Einführung von Schwachstellen oder Hintertüren in ein Modell während des Trainingsprozesses.
  ​
* Modell-Diebstahl/-Entwendung: Das Extrahieren einer Kopie oder einer Annäherung eines proprietären Modells durch wiederholte Abfragen.
  ​
* Multi-Agenten-System: Ein System, das aus mehreren interagierenden KI-Agenten besteht, die jeweils unterschiedliche Fähigkeiten und Ziele haben können.
  ​
* OPA (Open Policy Agent): Eine Open-Source-Policy-Engine, die eine einheitliche Durchsetzung von Richtlinien über den gesamten Stack hinweg ermöglicht.
  ​
* Datenschutzfreundliches Maschinelles Lernen (PPML): Techniken und Methoden zur Schulung und Bereitstellung von ML-Modellen unter Wahrung der Privatsphäre der Trainingsdaten.
  ​
* Prompt Injection: Ein Angriff, bei dem bösartige Anweisungen in Eingaben eingebettet werden, um das beabsichtigte Verhalten eines Modells zu überschreiben.
  ​
* RAG (Retrieval-Augmented Generation): Eine Technik, die große Sprachmodelle verbessert, indem sie relevante Informationen aus externen Wissensquellen abruft, bevor eine Antwort generiert wird.
  ​
* Red-Teaming: Die Praxis, KI-Systeme aktiv zu testen, indem simulierte Angriffsszenarien verwendet werden, um Schwachstellen zu identifizieren.
  ​
* SBOM (Software-Stückliste): Ein formeller Nachweis, der die Details und Lieferkettenbeziehungen verschiedener Komponenten enthält, die beim Erstellen von Software oder KI-Modellen verwendet werden.
  ​
* SHAP (SHapley Additive exPlanations): Ein spieltheoretischer Ansatz zur Erklärung der Ausgabe eines beliebigen Machine-Learning-Modells, indem der Beitrag jeder Merkmalvariable zur Vorhersage berechnet wird.
  ​
* Lieferkettenangriff: Kompromittierung eines Systems durch gezielte Angriffe auf weniger sichere Elemente in seiner Lieferkette, wie beispielsweise Drittanbieter-Bibliotheken, Datensätze oder vortrainierte Modelle.
  ​
* Transferlernen: Eine Technik, bei der ein für eine Aufgabe entwickeltes Modell als Ausgangspunkt für ein Modell einer zweiten Aufgabe wiederverwendet wird.
  ​
* Vektordatenbank: Eine spezialisierte Datenbank, die zur Speicherung hochdimensionaler Vektoren (Einbettungen) entwickelt wurde und effiziente Ähnlichkeitssuchen durchführt.
  ​
* Schwachstellen-Scannen: Automatisierte Werkzeuge, die bekannte Sicherheitslücken in Softwarekomponenten, einschließlich KI-Frameworks und Abhängigkeiten, identifizieren.
  ​
* Wasserzeichen: Techniken, um nicht wahrnehmbare Markierungen in KI-generierte Inhalte einzubetten, um deren Ursprung zu verfolgen oder die KI-Generierung zu erkennen.
  ​
* Zero-Day-Schwachstelle: Eine zuvor unbekannte Schwachstelle, die Angreifer ausnutzen können, bevor Entwickler einen Fix erstellen und bereitstellen.

