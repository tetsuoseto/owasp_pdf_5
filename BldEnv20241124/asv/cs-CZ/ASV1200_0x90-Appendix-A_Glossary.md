# Příloha A: Slovník pojmů

> Tento komplexní glosář poskytuje definice klíčových pojmů v oblasti umělé inteligence (AI), strojového učení (ML) a bezpečnosti používaných v celém AISVS, aby bylo zajištěno jasné a společné porozumění.
> ​
* Adversariální příklad: Vstup záměrně vytvořený tak, aby způsobil chybu v AI modelu, často přidáním jemných, pro člověka nepostřehnutelných narušení.
  ​
* Robustnost proti protichůdným útokům – Robustnost proti protichůdným útokům v AI se týká schopnosti modelu udržet si svůj výkon a odolávat tomu, aby byl oklamán nebo manipulován úmyslně vytvořenými škodlivými vstupy navrženými k vyvolání chyb.
  ​
* Agent – AI agenti jsou softwarové systémy, které využívají umělou inteligenci k dosahování cílů a plnění úkolů za uživatele. Projevují schopnosti usuzování, plánování a paměti a mají určitou úroveň autonomie pro přijímání rozhodnutí, učení a přizpůsobování se.
  ​
* Agentní AI: AI systémy, které mohou fungovat s určitou mírou autonomie za účelem dosažení cílů, často přijímají rozhodnutí a provádějí akce bez přímého zásahu člověka.
  ​
* Řízení přístupu založené na atributech (ABAC): paradigma řízení přístupu, kde jsou rozhodnutí o autorizaci založena na atributech uživatele, zdroje, akce a prostředí, vyhodnocovaných v době dotazu.
  ​
* Zadní vchodový útok: Typ útoku otrávením dat, kde je model trénován tak, aby na určité spouštěče reagoval specifickým způsobem, zatímco jinak se chová normálně.
  ​
* Bias: Systematické chyby ve výstupech AI modelu, které mohou vést k nespravedlivým nebo diskriminačním výsledkům pro určité skupiny nebo v konkrétních kontextech.
  ​
* Využití zkreslení: Útočná technika, která využívá známá zkreslení v modelech umělé inteligence k manipulaci výstupů nebo výsledků.
  ​
* Cedar: Jazyk a engine zásad Amazonu pro podrobné oprávnění používané při implementaci ABAC pro AI systémy.
  ​
* Řetězec myšlení: Technika pro zlepšení uvažování v jazykových modelech tím, že se generují mezikroky uvažování před vytvořením konečné odpovědi.
  ​
* Přerušovače obvodu: Mechanismy, které automaticky zastaví provoz AI systému, když jsou překročeny specifické práh rizika.
  ​
* Únik dat: Nezamýšlené prozrazení citlivých informací prostřednictvím výstupů nebo chování AI modelu.
  ​
* Poisoning dat: Úmyslné poškození tréninkových dat za účelem narušení integrity modelu, často se záměrem instalovat zadní vrátka nebo zhoršit výkon.
  ​
* Diferenciální soukromí – Diferenciální soukromí je matematicky přesný rámec pro zveřejňování statistických informací o datech, který přitom chrání soukromí jednotlivých subjektů dat. Umožňuje držiteli dat sdílet souhrnné vzory skupiny při omezení úniku informací o konkrétních jednotlivcích.
  ​
* Embeddingy: Husté vektorové reprezentace dat (text, obrázky atd.), které zachycují sémantický význam v prostorů s vysokou dimenzionalitou.
  ​
* Vysvětlitelnost – Vysvětlitelnost v umělé inteligenci (AI) je schopnost systému AI poskytnout lidsky srozumitelné důvody pro svá rozhodnutí a předpovědi, čímž nabízí vhled do svých vnitřních procesů.
  ​
* Vysvětlitelná umělá inteligence (XAI): Systémy umělé inteligence navržené tak, aby poskytovaly lidem srozumitelná vysvětlení svých rozhodnutí a chování prostřednictvím různých technik a rámců.
  ​
* Federované učení: přístup strojového učení, při kterém jsou modely trénovány na více decentralizovaných zařízeních obsahujících lokální datové vzorky, aniž by docházelo k výměně samotných dat.
  ​
* Ochranné zábrany: Omezující opatření implementovaná k zabránění systémům umělé inteligence produkovat škodlivé, zaujaté nebo jinak nežádoucí výstupy.
  ​
* Halucinace – Halucinace AI se týká jevu, kdy AI model generuje nesprávné nebo zavádějící informace, které nejsou založeny na jeho tréninkových datech nebo faktické realitě.
  ​
* Human-in-the-Loop (HITL): Systémy navržené tak, aby vyžadovaly lidský dohled, ověření nebo zásah v klíčových rozhodovacích bodech.
  ​
* Infrastruktura jako kód (IaC): Správa a poskytování infrastruktury prostřednictvím kódu namísto manuálních procesů, což umožňuje bezpečnostní skenování a konzistentní nasazení.
  ​
* Jailbreak: Techniky používané k obcházení bezpečnostních opatření v systémech umělé inteligence, zejména u velkých jazykových modelů, za účelem vytváření zakázaného obsahu.
  ​
* Nejnižší potřebná oprávnění: bezpečnostní zásada udělování pouze minimálních nezbytných přístupových práv uživatelům a procesům.
  ​
* LIME (Lokální interpretable modelově-agnostické vysvětlení): Technika vysvětlení predikcí jakéhokoli klasifikátoru strojového učení pomocí jeho lokální aproximace interpretovatelným modelem.
  ​
* Útok na zjištění členství: útok, jehož cílem je určit, zda byl konkrétní datový bod použit k tréninku modelu strojového učení.
  ​
* MITRE ATLAS: Adversariální hrozebný krajina pro systémy umělé inteligence; databáze znalostí adversariálních taktik a technik proti AI systémům.
  ​
* Model Card – Model Card je dokument, který poskytuje standardizované informace o výkonu AI modelu, jeho omezeních, zamýšlených použitích a etických aspektech s cílem podpořit transparentnost a odpovědný vývoj AI.
  ​
* Extrahování modelu: útok, při kterém protivník opakovaně pokládá dotazy cílovému modelu, aby vytvořil funkčně podobnou kopii bez autorizace.
  ​
* Modelová inverze: útok, který se snaží rekonstruovat tréninková data analýzou výstupů modelu.
  ​
* Správa životního cyklu modelu – Správa životního cyklu AI modelu je proces dohledu nad všemi fázemi existence AI modelu, včetně jeho návrhu, vývoje, nasazení, monitorování, údržby a konečného vyřazení, s cílem zajistit, že model zůstává efektivní a v souladu s cíli.
  ​
* Poisoning modelu: Zavádění zranitelností nebo zadních vrátek přímo do modelu během procesu tréninku.
  ​
* Krádež modelu: Získání kopie nebo aproximace proprietárního modelu prostřednictvím opakovaných dotazů.
  ​
* Multi-agentní systém: Systém složený z více vzájemně komunikujících AI agentů, z nichž každý může mít odlišné schopnosti a cíle.
  ​
* OPA (Open Policy Agent): Open-source engine pro zásady, který umožňuje jednotné prosazování zásad napříč celým stackem.
  ​
* Strojové učení zachovávající soukromí (PPML): Techniky a metody pro trénování a nasazení modelů strojového učení při ochraně soukromí trénovacích dat.
  ​
* Vkládání podnětů (Prompt Injection): Útok, při kterém jsou do vstupů vloženy škodlivé instrukce za účelem přepsání zamýšleného chování modelu.
  ​
* RAG (Generování podporované vyhledáváním): Technika, která vylepšuje velké jazykové modely tím, že před generováním odpovědi získává relevantní informace z externích zdrojů znalostí.
  ​
* Red-Teaming: Praxe aktivního testování AI systémů simulací protivnických útoků za účelem identifikace zranitelností.
  ​
* SBOM (Seznam komponent softwaru): Formální záznam obsahující podrobnosti a vztahy v dodavatelském řetězci různých komponent použitých při vytváření softwaru nebo AI modelů.
  ​
* SHAP (SHapley Additive exPlanations): Hra teoretický přístup k vysvětlení výstupu jakéhokoli modelu strojového učení výpočtem příspěvku každé vlastnosti k predikci.
  ​
* Útok na dodavatelský řetězec: Kompromitace systému zaměřením na méně zabezpečené prvky v jeho dodavatelském řetězci, jako jsou knihovny třetích stran, datové sady nebo předtrénované modely.
  ​
* Přenose učení: Technika, kdy je model vyvinutý pro jeden úkol znovu použit jako výchozí bod pro model určený k druhému úkolu.
  ​
* Vektorová databáze: Specializovaná databáze navržená k ukládání vysoce dimenzionálních vektorů (embeddings) a provádění efektivního vyhledávání podobnosti.
  ​
* Skenování zranitelností: Automatizované nástroje, které identifikují známé bezpečnostní zranitelnosti v softwarových komponentách, včetně AI frameworků a závislostí.
  ​
* Vodoznakování: Techniky pro vložení nepostřehnutelných značek do obsahu generovaného umělou inteligencí za účelem sledování jeho původu nebo detekce generování AI.
  ​
* Zranitelnost Zero-Day: Dříve neznámá zranitelnost, kterou mohou útočníci zneužít dříve, než vývojáři vytvoří a nasadí bezpečnostní záplatu.

