# A priedas: Terminų žodynas

> Šis išsamus žodynas pateikia pagrindinių DI, ML ir saugumo terminų apibrėžimus, naudojamus visame AISVS, siekiant užtikrinti aiškumą ir bendrą supratimą.
> ​
* Priešininkų pavyzdys: Įvestis, sąmoningai sukurta tam, kad sukeltų AI modeliui klaidą, dažnai pridedant subtilias, žmogui nepastebimas perturbacijas.
  ​
* Priešinės atsparumo savybės – Priešinės atsparumo savybės dirbtiniame intelekte reiškia modelio gebėjimą išlaikyti savo veikimą ir atsispirti klastojimui ar manipuliacijai, kurią sukelia tyčia sukurti, žalingi įvesties duomenys, skirti sukelti klaidas.
  ​
* Agentas – DI agentai yra programinės įrangos sistemos, kurios naudoja dirbtinį intelektą siekdamos tikslų ir vykdydamos užduotis vartotojų vardu. Jie demonstruoja samprotavimą, planavimą ir atmintį bei turi tam tikrą autonomijos lygį priimti sprendimus, mokytis ir prisitaikyti.
  ​
* Agentinė DI: DI sistemos, kurios gali veikti su tam tikru savarankiškumo laipsniu siekiant tikslų, dažnai priimdamos sprendimus ir imdamiesi veiksmų be tiesioginės žmogaus įsikišimo.
  ​
* Prieigos kontrolė pagal atributus (ABAC): prieigos kontrolės modelis, kuriame autorizacijos sprendimai priimami remiantis vartotojo, išteklių, veiksmo ir aplinkos atributais, įvertintais užklausos metu.
  ​
* Galinis įsilaužimas: tam tikra duomenų užnuodijimo ataka, kurioje modelis mokomas reaguoti tam tikru būdu į tam tikrus sužadinimus, tuo tarpu elgiasi įprastai kitu atveju.
  ​
* Šališkumas: Sistematinės klaidos dirbtinio intelekto modelių rezultatų, kurios gali lemti neteisingus ar diskriminuojančius rezultatus tam tikroms grupėms arba specifinėse situacijose.
  ​
* Šališkumo išnaudojimas: atakos metodas, kuris pasinaudoja žinomomis AI modelių šališkumais, siekiant manipuliuoti rezultatais ar pasekmėmis.
  ​
* Cedar: „Amazon“ politikos kalba ir variklis smulkioms leidimų valdymo priemonėms, naudojamoms įgyvendinant ABAC AI sistemoms.
  ​
* Minties grandinė: technika, skirta pagerinti kalbos modelių samprotavimą, generuojant tarpinės sampratos žingsnius prieš pateikiant galutinį atsakymą.
  ​
* Grandininiai išjungikliai: Mechanizmai, kurie automatiškai sustabdo DI sistemos veikimą, kai viršijamos tam tikros rizikos ribos.
  ​
* Duomenų nutekėjimas: netyčinis jautrios informacijos atskleidimas per DI modelio rezultatus ar elgesį.
  ​
* Duomenų užnuodijimas: tyčinis mokymo duomenų sugadinimas, siekiant pažeisti modelio vientisumą, dažnai diegiant užpakalines duris arba bloginant našumą.
  ​
* Differencinė privatumo apsauga – Differencinė privatumo apsauga yra matematiškai griežtas pagrindas statistinės informacijos apie duomenų rinkinius atskleidimui, tuo pačiu apsaugant atskirų duomenų subjektų privatumą. Ji leidžia duomenų turėtojui dalintis grupės bendraisiais modeliais, ribojant informaciją apie konkrečius asmenis.
  ​
* Įterpiniai: tankių vektorių atvaizdavimai duomenų (teksto, vaizdų ir kt.), kurie perteikia semantinę prasmę aukštadimensinėje erdvėje.
  ​
* Paaiškinamumas – paaiškinamumas dirbtiniame intelekte yra DI sistemos sugebėjimas pateikti žmogui suprantamas priežastis savo sprendimams ir prognozėms, suteikiant įžvalgų apie jos vidinį veikimą.
  ​
* Paaiškinamoji DI (XAI): DI sistemos, sukurtos teikti žmonėms suprantamus paaiškinimus apie jų sprendimus ir elgseną, taikant įvairias technikas ir struktūras.
  ​
* Federuotas mokymasis: mašininio mokymosi metodas, kai modeliai mokomi daugelyje decentralizuotų įrenginių, turinčių vietinius duomenų pavyzdžius, nereikalaujant keistis pačiais duomenimis.
  ​
* Saugikliai: Apribojimai, įdiegti tam, kad užkirstų kelią DI sistemoms generuoti žalingą, šališką ar kitaip nepageidaujamą turinį.
  ​
* Halucinacija – AI halucinacija reiškia reiškinį, kai AI modelis sukuria neteisingą arba klaidinamą informaciją, kuri nėra pagrįsta jo mokymo duomenimis ar faktine realybe.
  ​
* Žmogus cikle (Human-in-the-Loop, HITL): Sistemos, sukurtos reikalauti žmogaus priežiūros, patvirtinimo ar įsikišimo svarbiausiuose sprendimų priėmimo taškuose.
  ​
* Infrastruktūra kaip Kodas (IaC): infrastruktūros valdymas ir teikimas per kodą, o ne rankinius procesus, leidžiantį vykdyti saugumo skanavimą ir užtikrinti nuoseklų diegimą.
  ​
* Jailbreak: Technikos, naudojamos apeiti saugumo apribojimus dirbtinio intelekto sistemose, ypač dideliuose kalbos modeliuose, siekiant sukurti draudžiamą turinį.
  ​
* Mažiausių privilegijų principas: saugumo principas, numatantis suteikti vartotojams ir procesams tik būtiniausias prieigos teises.
  ​
* LIME (vieta interpretuojamas modelio nepriklausomas paaiškinimas): technika, skirta paaiškinti bet kurio mašininio mokymosi klasifikatoriaus prognozes, apskaičiuojant jas lokaliai su interpretuojamu modeliu.
  ​
* Nario informacijos atskleidimo ataka: ataka, kurios tikslas yra nustatyti, ar tam tikras duomenų taškas buvo naudojamas mokant mašininio mokymosi modelį.
  ​
* MITRE ATLAS: Priešiškos grėsmės dirbtinio intelekto sistemoms kraštovaizdis; priešiškų taktikų ir technikų žinių bazė prieš DI sistemas.
  ​
* Modelio kortelė – tai dokumentas, suteikiantis standartizuotą informaciją apie dirbtinio intelekto modelio našumą, ribotumus, numatomą panaudojimą ir etinius aspektus, siekiant skatinti skaidrumą ir atsakingą DI vystymą.
  ​
* Modelio išgavimas: ataka, kai priešas kelis kartus užklausia taikinio modelį, siekdamas sukurti funkcionaliai panašią kopiją be leidimo.
  ​
* Modelio inversija: ataka, kuri bando rekonstruoti mokymo duomenis analizuodama modelio išvestis.
  ​
* Modelio gyvavimo ciklo valdymas – AI modelio gyvavimo ciklo valdymas yra procesas, apimantis visų AI modelio egzistavimo etapų priežiūrą, įskaitant jo dizainą, kūrimą, diegimą, stebėjimą, priežiūrą ir galutinį pasenimą, siekiant užtikrinti, kad jis išliktų efektyvus ir atitiktų tikslus.
  ​
* Modelio užnuodijimas: pažeidžiamumų arba užslėptų prieigų įvedimas tiesiogiai į modelį mokymo proceso metu.
  ​
* Modelio vagystė: nuosavo modelio kopijos arba artimos versijos gavimas per daugybinius užklausimus.
  ​
* Daugiagentė sistema: sistema, sudaryta iš kelių sąveikaujančių DI agentų, kiekvienas iš jų gali turėti skirtingas galimybes ir tikslus.
  ​
* OPA (Open Policy Agent): Atviro kodo politikos variklis, leidžiantis vieningai įgyvendinti politikos taisykles visame technologijų sluoksnyje.
  ​
* Privatumo saugojimo mašininis mokymasis (PPML): Technologijos ir metodai, skirti mokyti ir diegti ML modelius, saugant mokymo duomenų privatumą.
  ​
* Promptų injekcija: ataka, kai įvestyse įterpiamos kenkėjiškos instrukcijos, siekiant pakeisti modelio numatytą elgesį.
  ​
* RAG (retrieval-augmented generation): Technika, kuri pagerina didelius kalbos modelius, prieš generuodama atsakymą gaunant aktualią informaciją iš išorinių žinių šaltinių.
  ​
* Red-teamingas: Praktika aktyviai testuoti DI sistemas imituojant priešiškus puolimus, siekiant nustatyti pažeidžiamumus.
  ​
* SBOM (Programinės įrangos sudedamųjų dalių sąrašas): Oficialus įrašas, kuriame pateikiama informacija apie įvairias komponentes ir jų tiekimo grandinės ryšius, naudojamus kuriant programinę įrangą arba dirbtinio intelekto modelius.
  ​
* SHAP (SHapley pridedančios paaiškinimo metodas): žaidimų teorijos pagrindu sukurta priemonė, skirta paaiškinti bet kurio mašininio mokymosi modelio rezultatą apskaičiuojant kiekvieno požymio indėlį į prognozę.
  ​
* Tiekimo grandinės ataka: sistemos pažeidimas taikant silpniausiems jos tiekimo grandinės elementams, tokiems kaip trečiųjų šalių bibliotekos, duomenų rinkiniai ar iš anksto apmokyti modeliai.
  ​
* Perkėlimo mokymasis: technika, kai vienam uždaviniui sukurta modelis naudojamas kaip pradinis taškas modeliui kitam uždaviniui.
  ​
* Vektorinė duomenų bazė: specializuota duomenų bazė, skirta saugoti daugiamates vektorius (įterpimus) ir efektyviai atlikti panašumo paieškas.
  ​
* Pažeidžiamumo nuskaitymas: automatizuoti įrankiai, kurie identifikuoja žinomas saugumo spragas programinės įrangos komponentuose, įskaitant dirbtinio intelekto pagrindus ir priklausomybes.
  ​
* Vandens žymėjimas: technikos, skirtos įterpti nepastebimus žymenis į AI sugeneruotą turinį, siekiant sekti jo kilmę arba aptikti AI generavimą.
  ​
* Nulinės dienos pažeidžiamumas: Anksčiau nežinoma pažeidžiamumo spraga, kurią atakuotojai gali išnaudoti prieš tai, kai kūrėjai sukuria ir įdiegia pataisą.

