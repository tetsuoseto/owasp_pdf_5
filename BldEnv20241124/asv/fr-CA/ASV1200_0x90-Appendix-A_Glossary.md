# Annexe A : Glossaire

> Ce glossaire complet fournit des définitions des termes clés liés à l'IA, à l'apprentissage automatique et à la sécurité utilisés dans l'ensemble de l'AISVS afin de garantir la clarté et une compréhension commune.
> ​
* Exemple d'attaque : Une entrée délibérément conçue pour amener un modèle d'IA à commettre une erreur, souvent en ajoutant des perturbations subtiles imperceptibles pour les humains.
  ​
* Robustesse face aux attaques adverses – La robustesse face aux attaques adverses en IA fait référence à la capacité d'un modèle à maintenir ses performances et à résister à la tromperie ou à la manipulation par des entrées malveillantes intentionnellement conçues pour provoquer des erreurs.
  ​
* Agent – Les agents IA sont des systèmes logiciels qui utilisent l'IA pour poursuivre des objectifs et accomplir des tâches au nom des utilisateurs. Ils démontrent des capacités de raisonnement, de planification et de mémoire, et disposent d'un certain niveau d'autonomie pour prendre des décisions, apprendre et s'adapter.
  ​
* IA agentique : systèmes d'IA capables de fonctionner avec un certain degré d'autonomie pour atteindre des objectifs, prenant souvent des décisions et agissant sans intervention humaine directe.
  ​
* Contrôle d'accès basé sur les attributs (ABAC) : un paradigme de contrôle d'accès où les décisions d'autorisation sont basées sur les attributs de l'utilisateur, de la ressource, de l'action et de l'environnement, évalués au moment de la requête.
  ​
* Attaque par porte dérobée : un type d'attaque par empoisonnement de données où le modèle est entraîné à répondre d'une manière spécifique à certains déclencheurs tout en se comportant normalement sinon.
  ​
* Biais : Erreurs systématiques dans les résultats des modèles d'IA pouvant entraîner des résultats injustes ou discriminatoires pour certains groupes ou dans des contextes spécifiques.
  ​
* Exploitation des biais : une technique d'attaque qui tire parti des biais connus dans les modèles d'IA pour manipuler les résultats ou les sorties.
  ​
* Cedar : le langage de politique et le moteur d'Amazon pour des autorisations granulaires utilisées dans la mise en œuvre de l'ABAC pour les systèmes d'IA.
  ​
* Chaîne de pensée : une technique pour améliorer le raisonnement dans les modèles de langage en générant des étapes intermédiaires de raisonnement avant de produire une réponse finale.
  ​
* Disjoncteurs : Mécanismes qui arrêtent automatiquement les opérations du système d'IA lorsque des seuils de risque spécifiques sont dépassés.
  ​
* Fuite de données : exposition non intentionnelle d'informations sensibles via les résultats ou le comportement d'un modèle d'IA.
  ​
* Empoisonnement des données : la corruption délibérée des données d'entraînement afin de compromettre l'intégrité du modèle, souvent pour installer des portes dérobées ou dégrader les performances.
  ​
* Confidentialité différentielle – La confidentialité différentielle est un cadre mathématiquement rigoureux pour la diffusion d'informations statistiques sur les ensembles de données tout en protégeant la vie privée des individus. Elle permet au détenteur des données de partager des tendances agrégées du groupe tout en limitant les informations divulguées sur des individus spécifiques.
  ​
* Embarquements : Représentations vectorielles denses des données (texte, images, etc.) qui capturent le sens sémantique dans un espace à haute dimension.
  ​
* Explicabilité – L'explicabilité en IA est la capacité d'un système d'IA à fournir des raisons compréhensibles par l'humain pour ses décisions et prédictions, offrant ainsi des éclaircissements sur son fonctionnement interne.
  ​
* Intelligence Artificielle Explicable (XAI) : Systèmes d'IA conçus pour fournir des explications compréhensibles par l'humain concernant leurs décisions et comportements, à travers diverses techniques et cadres.
  ​
* Apprentissage Fédéré : Une approche d'apprentissage automatique où les modèles sont entraînés sur plusieurs dispositifs décentralisés contenant des échantillons de données locales, sans échanger les données elles-mêmes.
  ​
* Garde-fous : Contraintes mises en place pour empêcher les systèmes d'IA de produire des résultats nuisibles, biaisés ou autrement indésirables.
  ​
* Hallucination – Une hallucination d'IA fait référence à un phénomène où un modèle d'IA génère des informations incorrectes ou trompeuses qui ne sont pas basées sur ses données d'entraînement ni sur la réalité factuelle.
  ​
* Humain dans la boucle (HITL) : systèmes conçus pour nécessiter une supervision, une vérification ou une intervention humaine à des points décisionnels cruciaux.
  ​
* Infrastructure as Code (IaC) : Gestion et provisionnement de l'infrastructure via du code au lieu de processus manuels, permettant l'analyse de sécurité et des déploiements cohérents.
  ​
* Jailbreak : Techniques utilisées pour contourner les dispositifs de sécurité dans les systèmes d'IA, en particulier dans les grands modèles de langage, afin de produire du contenu interdit.
  ​
* Principe du moindre privilège : le principe de sécurité consistant à accorder uniquement les droits d'accès minimum nécessaires aux utilisateurs et aux processus.
  ​
* LIME (Explications Locales Interprétables Indépendantes du Modèle) : une technique pour expliquer les prédictions de n'importe quel classificateur d'apprentissage automatique en l'approximant localement avec un modèle interprétable.
  ​
* Attaque par inférence d'appartenance : une attaque visant à déterminer si un point de données spécifique a été utilisé pour entraîner un modèle d'apprentissage automatique.
  ​
* MITRE ATLAS : Paysage des menaces adversariales pour les systèmes d'intelligence artificielle ; une base de connaissances sur les tactiques et techniques adversariales contre les systèmes d'IA.
  ​
* Fiche de modèle – Une fiche de modèle est un document qui fournit des informations standardisées sur la performance, les limites, les usages prévus et les considérations éthiques d'un modèle d'IA afin de promouvoir la transparence et le développement responsable de l'IA.
  ​
* Extraction de modèle : une attaque où un adversaire interroge de manière répétée un modèle cible afin de créer une copie fonctionnellement similaire sans autorisation.
  ​
* Inversion de modèle : une attaque qui tente de reconstruire les données d'entraînement en analysant les sorties du modèle.
  ​
* Gestion du cycle de vie du modèle – La gestion du cycle de vie des modèles d'IA est le processus de supervision de toutes les étapes de l'existence d'un modèle d'IA, y compris sa conception, son développement, son déploiement, sa surveillance, sa maintenance et sa mise hors service éventuelle, afin de garantir qu'il reste efficace et aligné sur les objectifs.
  ​
* Empoisonnement de modèle : introduction de vulnérabilités ou de portes dérobées directement dans un modèle pendant le processus d'entraînement.
  ​
* Vol de modèle : Extraire une copie ou une approximation d'un modèle propriétaire par des requêtes répétées.
  ​
* Système multi-agent : un système composé de plusieurs agents d'IA interagissant, chacun pouvant avoir des capacités et des objectifs différents.
  ​
* OPA (Open Policy Agent) : Un moteur de politique open-source qui permet une application unifiée des politiques à travers l'ensemble de la pile.
  ​
* Apprentissage automatique respectueux de la vie privée (PPML) : Techniques et méthodes pour entraîner et déployer des modèles d'apprentissage automatique tout en protégeant la confidentialité des données d'entraînement.
  ​
* Injection de prompt : une attaque où des instructions malveillantes sont intégrées dans les entrées pour contourner le comportement prévu d’un modèle.
  ​
* RAG (Génération Augmentée par Recherche) : Une technique qui améliore les grands modèles de langage en récupérant des informations pertinentes à partir de sources de connaissances externes avant de générer une réponse.
  ​
* Red-Teaming : La pratique de tester activement les systèmes d'IA en simulant des attaques adverses afin d'identifier les vulnérabilités.
  ​
* SBOM (Liste des composants logiciels) : Un enregistrement formel contenant les détails et les relations dans la chaîne d'approvisionnement des différents composants utilisés dans la construction de logiciels ou de modèles d'IA.
  ​
* SHAP (Explications Additives de Shapley) : Une approche basée sur la théorie des jeux pour expliquer la sortie de n'importe quel modèle d'apprentissage automatique en calculant la contribution de chaque caractéristique à la prédiction.
  ​
* Attaque sur la chaîne d'approvisionnement : compromettre un système en ciblant des éléments moins sécurisés de sa chaîne d'approvisionnement, tels que des bibliothèques tierces, des jeux de données ou des modèles pré-entraînés.
  ​
* Apprentissage par transfert : une technique où un modèle développé pour une tâche est réutilisé comme point de départ pour un modèle destiné à une seconde tâche.
  ​
* Base de données vectorielle : une base de données spécialisée conçue pour stocker des vecteurs haute dimension (embeddings) et effectuer des recherches de similarité efficaces.
  ​
* Analyse de vulnérabilités : Outils automatisés qui identifient les vulnérabilités de sécurité connues dans les composants logiciels, y compris les frameworks d'IA et leurs dépendances.
  ​
* Filigrane : Techniques pour intégrer des marqueurs imperceptibles dans le contenu généré par l'IA afin de suivre son origine ou de détecter la génération par IA.
  ​
* Vulnérabilité Zero-Day : Une vulnérabilité auparavant inconnue que les attaquants peuvent exploiter avant que les développeurs ne créent et déploient un correctif.

