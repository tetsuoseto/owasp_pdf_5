# Apéndice A: Glosario

> Este glosario integral ofrece definiciones de términos clave de IA, ML y seguridad utilizados en todo el AISVS para asegurar claridad y un entendimiento común.
> ​
* Ejemplo Adversarial: Una entrada deliberadamente diseñada para causar que un modelo de IA cometa un error, a menudo agregando perturbaciones sutiles imperceptibles para los humanos.
  ​
* Robustez adversarial: La robustez adversarial en IA se refiere a la capacidad de un modelo para mantener su rendimiento y resistir ser engañado o manipulado por entradas maliciosas diseñadas intencionalmente para provocar errores.
  ​
* Agente – Los agentes de IA son sistemas de software que emplean inteligencia artificial para perseguir objetivos y completar tareas en nombre de los usuarios. Demuestran razonamiento, planificación y memoria, y poseen un nivel de autonomía para tomar decisiones, aprender y adaptarse.
  ​
* IA agentiva: Sistemas de IA que pueden operar con cierto grado de autonomía para alcanzar objetivos, a menudo tomando decisiones y realizando acciones sin intervención humana directa.
  ​
* Control de Acceso Basado en Atributos (ABAC): Un paradigma de control de acceso donde las decisiones de autorización se basan en atributos del usuario, recurso, acción y entorno, evaluados en tiempo de consulta.
  ​
* Ataque de puerta trasera: Un tipo de ataque de envenenamiento de datos donde el modelo se entrena para responder de una manera específica a ciertos desencadenantes mientras se comporta normalmente en otras circunstancias.
  ​
* Sesgo: Errores sistemáticos en las salidas de modelos de IA que pueden conducir a resultados injustos o discriminatorios para ciertos grupos o en contextos específicos.
  ​
* Explotación de sesgos: una técnica de ataque que aprovecha sesgos conocidos en los modelos de IA para manipular resultados o salidas.
  ​
* Cedar: lenguaje de políticas y motor de Amazon para permisos detallados utilizados en la implementación de ABAC para sistemas de IA.
  ​
* Cadena de Pensamiento: Una técnica para mejorar el razonamiento en modelos de lenguaje generando pasos intermedios de razonamiento antes de producir una respuesta final.
  ​
* Interruptores automáticos: Mecanismos que detienen automáticamente las operaciones del sistema de IA cuando se superan ciertos umbrales de riesgo.
  ​
* Fuga de datos: exposición no intencionada de información sensible a través de salidas o comportamiento de modelos de IA.
  ​
* Envenenamiento de datos: La corrupción deliberada de los datos de entrenamiento para comprometer la integridad del modelo, a menudo para instalar puertas traseras o degradar el rendimiento.
  ​
* Privacidad diferencial: la privacidad diferencial es un marco matemáticamente riguroso para divulgar información estadística sobre conjuntos de datos mientras se protege la privacidad de los sujetos individuales de datos. Permite que un poseedor de datos comparta patrones agregados del grupo mientras limita la información que se filtra sobre individuos específicos.
  ​
* Incrustaciones: Representaciones vectoriales densas de datos (texto, imágenes, etc.) que capturan el significado semántico en un espacio de alta dimensión.
  ​
* Explicabilidad: La explicabilidad en la IA es la capacidad de un sistema de IA para proporcionar razones comprensibles por humanos para sus decisiones y predicciones, ofreciendo perspectivas sobre su funcionamiento interno.
  ​
* IA Explicable (XAI): Sistemas de IA diseñados para proporcionar explicaciones comprensibles para los humanos sobre sus decisiones y comportamientos mediante diversas técnicas y marcos.
  ​
* Aprendizaje Federado: Un enfoque de aprendizaje automático donde los modelos se entrenan en múltiples dispositivos descentralizados que contienen muestras de datos locales, sin intercambiar los datos en sí.
  ​
* Guardarraíles: restricciones implementadas para evitar que los sistemas de IA produzcan resultados dañinos, sesgados o de otro modo indeseables.
  ​
* Alucinación – Una alucinación de IA se refiere a un fenómeno donde un modelo de IA genera información incorrecta o engañosa que no se basa en sus datos de entrenamiento ni en la realidad factual.
  ​
* Humano-en-el-Bucle (HITL): Sistemas diseñados para requerir supervisión, verificación o intervención humana en puntos decisivos cruciales.
  ​
* Infraestructura como Código (IaC): Gestión y provisión de infraestructura mediante código en lugar de procesos manuales, permitiendo el escaneo de seguridad y despliegues consistentes.
  ​
* Jailbreak: Técnicas utilizadas para eludir las barreras de seguridad en los sistemas de IA, especialmente en los grandes modelos de lenguaje, para generar contenido prohibido.
  ​
* Principio de Menor Privilegio: El principio de seguridad que consiste en otorgar únicamente los derechos de acceso mínimos necesarios para usuarios y procesos.
  ​
* LIME (Explicaciones Locales Interpretables y Agnósticas al Modelo): Una técnica para explicar las predicciones de cualquier clasificador de aprendizaje automático al aproximarlo localmente con un modelo interpretable.
  ​
* Ataque de inferencia de membresía: un ataque que tiene como objetivo determinar si un punto de datos específico fue utilizado para entrenar un modelo de aprendizaje automático.
  ​
* MITRE ATLAS: Panorama de Amenazas Adversarias para Sistemas de Inteligencia Artificial; una base de conocimientos de tácticas y técnicas adversarias contra sistemas de IA.
  ​
* Tarjeta de Modelo – Una tarjeta de modelo es un documento que proporciona información estandarizada sobre el rendimiento, las limitaciones, los usos previstos y las consideraciones éticas de un modelo de IA para promover la transparencia y el desarrollo responsable de la IA.
  ​
* Extracción de Modelo: Un ataque donde un adversario consulta repetidamente un modelo objetivo para crear una copia funcionalmente similar sin autorización.
  ​
* Inversión de modelo: Un ataque que intenta reconstruir los datos de entrenamiento analizando las salidas del modelo.
  ​
* Gestión del Ciclo de Vida del Modelo – La Gestión del Ciclo de Vida del Modelo de IA es el proceso de supervisar todas las etapas de la existencia de un modelo de IA, incluyendo su diseño, desarrollo, implementación, monitoreo, mantenimiento y eventual retiro, para asegurar que permanezca efectivo y alineado con los objetivos.
  ​
* Envenenamiento del modelo: Introducción de vulnerabilidades o puertas traseras directamente en un modelo durante el proceso de entrenamiento.
  ​
* Robo/Robo de Modelos: Extraer una copia o aproximación de un modelo propietario mediante consultas repetidas.
  ​
* Sistema multiagente: un sistema compuesto por múltiples agentes de IA que interactúan entre sí, cada uno con capacidades y objetivos potencialmente diferentes.
  ​
* OPA (Open Policy Agent): Un motor de políticas de código abierto que permite la aplicación unificada de políticas en toda la pila.
  ​
* Aprendizaje Automático Preservando la Privacidad (PPML): Técnicas y métodos para entrenar y desplegar modelos de aprendizaje automático mientras se protege la privacidad de los datos de entrenamiento.
  ​
* Inyección de indicaciones: un ataque donde se incorporan instrucciones maliciosas en las entradas para anular el comportamiento previsto de un modelo.
  ​
* RAG (Generación Aumentada por Recuperación): Una técnica que mejora los modelos de lenguaje grandes al recuperar información relevante de fuentes externas de conocimiento antes de generar una respuesta.
  ​
* Red-Teaming: La práctica de probar activamente sistemas de IA simulando ataques adversarios para identificar vulnerabilidades.
  ​
* SBOM (Lista de Materiales de Software): Un registro formal que contiene los detalles y las relaciones de la cadena de suministro de varios componentes utilizados en la construcción de software o modelos de IA.
  ​
* SHAP (Explicaciones aditivas basadas en valores de Shapley): Un enfoque teórico de juegos para explicar la salida de cualquier modelo de aprendizaje automático calculando la contribución de cada característica a la predicción.
  ​
* Ataque a la cadena de suministro: comprometer un sistema al atacar elementos menos seguros en su cadena de suministro, como bibliotecas de terceros, conjuntos de datos o modelos preentrenados.
  ​
* Aprendizaje por transferencia: una técnica donde un modelo desarrollado para una tarea se reutiliza como punto de partida para un modelo en una segunda tarea.
  ​
* Base de Datos Vectorial: Una base de datos especializada diseñada para almacenar vectores de alta dimensión (embeddings) y realizar búsquedas de similitud de manera eficiente.
  ​
* Escaneo de Vulnerabilidades: Herramientas automatizadas que identifican vulnerabilidades de seguridad conocidas en componentes de software, incluyendo frameworks de IA y dependencias.
  ​
* Marca de agua: Técnicas para insertar marcadores imperceptibles en contenido generado por IA para rastrear su origen o detectar generación por IA.
  ​
* Vulnerabilidad de Día Cero: Una vulnerabilidad previamente desconocida que los atacantes pueden explotar antes de que los desarrolladores creen y desplieguen un parche.

