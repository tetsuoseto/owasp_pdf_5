# 附錄 A：詞彙表

> 本綜合術語表提供了AISVS中使用的關鍵人工智慧、機器學習及安全術語的定義，以確保清晰度和共同理解。
> ​
* 敵對範例：一種刻意製作的輸入，目的是讓人工智慧模型出錯，通常透過添加人類難以察覺的細微擾動。
  ​
* 對抗魯棒性 – AI 中的對抗魯棒性指的是模型維持其性能並抵抗被故意設計用來引起錯誤的惡意輸入欺騙或操縱的能力。
  ​
* 代理人 – AI代理人是使用人工智慧來代表使用者追求目標並完成任務的軟體系統。它們展現推理、規劃和記憶能力，並具備一定程度的自主性以進行決策、學習和適應。
  ​
* 自主代理型人工智慧（Agentic AI）：能夠以某種程度的自主性運作以達成目標的人工智慧系統，通常在沒有直接人類干預的情況下做出決策並採取行動。
  ​
* 基於屬性的存取控制（ABAC）：一種存取控制範式，其授權決策基於使用者、資源、操作和環境的屬性，並在查詢時進行評估。
  ​
* 後門攻擊：一種資料中毒攻擊，模型在訓練時被設計對特定觸發條件做出特定反應，而在其他情況下表現正常。
  ​
* 偏差：人工智慧模型輸出中的系統性錯誤，可能導致特定群體或特定情境下的不公平或歧視性結果。
  ​
* 偏見利用：一種利用人工智慧模型中已知偏見來操控輸出或結果的攻擊技術。
  ​
* Cedar：亞馬遜用於實現 AI 系統基於屬性的存取控制（ABAC）之細粒度權限策略語言和引擎。
  ​
* 連鎖思考：一種通過在產生最終答案之前生成中間推理步驟，以提高語言模型推理能力的技術。
  ​
* 斷路器：當超過特定風險閾值時，自動停止人工智慧系統運作的機制。
  ​
* 資料外洩：透過人工智慧模型輸出或行為，無意中洩露敏感資訊。
  ​
* 數據投毒：故意破壞訓練數據以損害模型完整性，通常用於安裝後門或降低性能。
  ​
* 差分隱私 – 差分隱私是一種數學上嚴謹的框架，用於釋放關於資料集的統計資訊，同時保護個別資料主體的隱私。它使資料持有者能夠共享群體的整體模式，同時限制洩露關於特定個體的資訊。
  ​
* 嵌入向量：數據（文本、圖像等）的密集向量表示，在高維空間中捕捉語義意義。
  ​
* 可解釋性 – 可解釋性在人工智慧中指的是AI系統能夠提供人類可理解的決策及預測原因，並揭示其內部運作機制的能力。
  ​
* 可解釋人工智慧（XAI）：透過各種技術與框架，設計出能夠為其決策與行為提供人類可理解解釋的人工智慧系統。
  ​
* 聯邦學習：一種機器學習方法，模型在多個持有本地數據樣本的去中心化設備上進行訓練，而不交換數據本身。
  ​
* 防護欄：為防止人工智慧系統產生有害、偏頗或其他不良輸出而實施的限制措施。
  ​
* 幻覺——AI幻覺是指AI模型產生錯誤或誤導性資訊的現象，這些資訊並非基於其訓練數據或事實現實。
  ​
* 人類在環路（HITL）：設計用於在關鍵決策點需要人類監督、驗證或干預的系統。
  ​
* 基礎設施即程式碼 (IaC)：透過程式碼管理和配置基礎設施，而非手動操作，從而實現安全掃描和一致的部署。
  ​
* 越獄技術：用於繞過人工智慧系統中，特別是在大型語言模型中的安全防護措施，以生成被禁止的內容的方法。
  ​
* 最小權限原則：授予使用者和程序僅具備最低必要存取權限的安全原則。
  ​
* LIME（局部可解釋模型無關解釋）：一種通過在局部使用可解釋模型來近似任何機器學習分類器的預測結果的技術。
  ​
* 成員推斷攻擊：一種旨在判斷特定數據點是否被用於訓練機器學習模型的攻擊。
  ​
* MITRE ATLAS：人工智慧系統的對抗性威脅景觀；一個針對人工智慧系統的對抗戰術與技術知識庫。
  ​
* 模型卡 — 模型卡是一份文件，提供有關人工智慧模型的性能、限制、預期用途及倫理考量的標準化資訊，以促進透明度和負責任的人工智慧開發。
  ​
* 模型提取：一種攻擊方式，攻擊者透過反覆查詢目標模型，以未經授權的方式建立一個功能相似的複製品。
  ​
* 模型反轉：一種透過分析模型輸出來試圖重建訓練數據的攻擊。
  ​
* 模型生命週期管理 – AI 模型生命週期管理是指監督 AI 模型存在的所有階段，包括設計、開發、部署、監控、維護及最終退役，以確保模型保持有效且符合目標的過程。
  ​
* 模型中毒：在訓練過程中直接向模型引入漏洞或後門。
  ​
* 模型盜用/竊取：通過反覆查詢提取專有模型的副本或近似模型。
  ​
* 多代理系統：由多個相互作用的人工智慧代理組成的系統，每個代理可能具有不同的能力和目標。
  ​
* OPA（開放政策代理）：一個開源的政策引擎，可實現整個堆棧的統一政策執行。
  ​
* 隱私保護機器學習（PPML）：在保護訓練資料隱私的同時，訓練和部署機器學習模型的技術和方法。
  ​
* 提示注入：一種攻擊方式，透過在輸入中嵌入惡意指令，以覆蓋模型的預期行為。
  ​
* RAG（檢索增強生成）：一種技術，通過在生成回應之前從外部知識來源檢索相關信息來增強大型語言模型的能力。
  ​
* 紅隊演練：透過模擬對抗性攻擊主動測試人工智慧系統，以識別系統的弱點。
  ​
* SBOM（軟體材料清單）：一份正式記錄，包含用於構建軟體或人工智慧模型的各種組件的詳細資料及其供應鏈關係。
  ​
* SHAP（Shapley 加法解釋）：一種博弈論方法，通過計算每個特徵對預測結果的貢獻來解釋任何機器學習模型的輸出。
  ​
* 供應鏈攻擊：透過針對供應鏈中安全性較弱的元素，如第三方函式庫、資料集或預訓練模型，來入侵系統。
  ​
* 遷移學習：一種技術，將為一項任務開發的模型重新用作第二項任務模型的起始點。
  ​
* 向量資料庫：一種專門設計用於儲存高維向量（嵌入向量）並執行高效相似度搜尋的資料庫。
  ​
* 漏洞掃描：自動化工具，用於識別軟體組件中的已知安全漏洞，包括人工智慧框架和依賴項。
  ​
* 水印技術：在 AI 生成內容中嵌入難以察覺的標記，用於追蹤其來源或檢測 AI 生成痕跡的方法。
  ​
* 零時差漏洞：一種先前未知的漏洞，攻擊者可以在開發人員創建並部署修補程式之前加以利用。

