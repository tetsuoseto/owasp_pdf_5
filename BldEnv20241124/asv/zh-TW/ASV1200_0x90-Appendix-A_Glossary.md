# 附錄 A：術語表

> 這本全面的詞彙表提供了整個AISVS中使用的關鍵人工智慧、機器學習和安全術語的定義，以確保清晰度和共識。
> ​
* 對抗樣本：一種刻意製作的輸入，旨在使人工智慧模型犯錯，通常透過加入人類難以察覺的微小擾動來實現。
  ​
* 對抗魯棒性 – 在人工智慧中，對抗魯棒性指的是模型維持其性能並抵抗經過精心設計的惡意輸入所導致錯誤的能力。
  ​
* 代理人 – AI代理人是使用人工智能來追求目標並代表使用者完成任務的軟體系統。它們具備推理、規劃和記憶能力，並具有一定程度的自主性，可以做出決策、學習和適應。
  ​
* 代理式人工智慧：能夠在某種程度上自主運作以達成目標的人工智慧系統，通常在未經直接人類干預的情況下作出決策並採取行動。
  ​
* 基於屬性的存取控制（ABAC）：一種存取控制範式，授權決策基於使用者、資源、操作和環境的屬性，並在查詢時進行評估。
  ​
* 後門攻擊：一種資料中毒攻擊，模型被訓練成對某些觸發條件作出特定反應，而在其他情況下表現正常。
  ​
* 偏差：AI模型輸出中的系統性錯誤，可能導致對特定群體或特定情境的不公平或歧視性結果。
  ​
* 偏見利用：一種利用已知的 AI 模型偏見來操控輸出或結果的攻擊技術。
  ​
* Cedar：亞馬遜用於實現 AI 系統屬性基存取控制（ABAC）的細粒度權限政策語言及引擎。
  ​
* 思考鏈：一種透過在產生最終答案前生成中間推理步驟來提升語言模型推理能力的技術。
  ​
* 斷路器：當超過特定風險閾值時，自動停止人工智慧系統運作的機制。
  ​
* 資料外洩：透過 AI 模型輸出或行為意外暴露敏感資訊。
  ​
* 資料毒化：故意破壞訓練資料以損害模型完整性，通常用於安裝後門或降低性能。
  ​
* 差分隱私 – 差分隱私是一種在數學上嚴謹的框架，用於在保護個別資料對象隱私的同時，發布關於資料集的統計資訊。它使資料持有者能夠分享群體的彙總模式，同時限制泄露關於特定個體的信息。
  ​
* 嵌入向量：資料（文字、圖像等）的密集向量表示，能在高維空間中捕捉語義意義。
  ​
* 可解釋性 – 在人工智慧中，可解釋性是指AI系統能夠提供人類可理解的決策與預測理由，並揭示其內部運作機制的能力。
  ​
* 可解釋人工智慧（XAI）：設計用於通過各種技術和架構，為其決策和行為提供人類可理解解釋的人工智慧系統。
  ​
* 聯邦學習：一種機器學習方法，模型在多個分散的設備上訓練，這些設備持有本地數據樣本，但不交換數據本身。
  ​
* 安全防護措施：實施的限制，用以防止人工智慧系統產生有害、偏見或其他不理想的輸出結果。
  ​
* 幻覺 — AI幻覺指的是AI模型生成不正確或誤導性資訊的現象，這些資訊並非基於其訓練數據或事實現實。
  ​
* 人類在環回（HITL）：設計為在關鍵決策點需要人類監督、驗證或介入的系統。
  ​
* 基礎設施即程式碼（IaC）：透過程式碼管理和配置基礎設施，而非手動流程，從而實現安全掃描和一致性部署。
  ​
* 越獄技術：用於繞過人工智慧系統中安全防護措施的技術，特別是在大型語言模型中，以生成被禁止的內容。
  ​
* 最小權限原則：授予用戶和程序僅有的最低必要訪問權限的安全原則。
  ​
* LIME（局部可解釋模型無關解釋）：一種通過使用可解釋模型在局部範圍內近似，來解釋任何機器學習分類器預測結果的技術。
  ​
* 成員推斷攻擊：一種旨在判斷特定數據點是否被用於訓練機器學習模型的攻擊。
  ​
* MITRE ATLAS：人工智慧系統的對抗性威脅景觀；一個關於針對人工智慧系統之對抗性策略與技術的知識庫。
  ​
* 模型卡 — 模型卡是一份文件，提供關於人工智慧模型的性能、限制、預期用途及倫理考量的標準化資訊，以促進透明度及負責任的人工智慧開發。
  ​
* 模型提取：一種攻擊手法，攻擊者透過反覆查詢目標模型，未經授權地創建一個功能上相似的複製品。
  ​
* 模型反演：一種通過分析模型輸出以試圖重建訓練數據的攻擊。
  ​
* 模型生命周期管理——AI模型生命周期管理是指監督AI模型整個存在階段的過程，包括其設計、開發、部署、監控、維護及最終退役，以確保模型持續有效並與目標保持一致。
  ​
* 模型中毒：在訓練過程中直接向模型引入漏洞或後門。
  ​
* 模型竊取/盜用：透過反覆查詢提取專有模型的複製品或近似版本。
  ​
* 多代理系統：由多個互動的 AI 代理組成的系統，每個代理可能具備不同的能力和目標。
  ​
* OPA（Open Policy Agent）：一個開源的政策引擎，能夠在整個系統堆疊中實現統一的政策執行。
  ​
* 隱私保護機器學習（PPML）：在保護訓練數據隱私的同時，訓練和部署機器學習模型的技術與方法。
  ​
* 提示注入：一種攻擊，其中惡意指令被嵌入輸入中，以覆蓋模型的預期行為。
  ​
* RAG（檢索增強生成）：一種通過在生成回應之前從外部知識來源檢索相關信息來增強大型語言模型的技術。
  ​
* 紅隊測試：通過模擬對抗性攻擊積極測試人工智慧系統以識別漏洞的做法。
  ​
* SBOM（軟體材料清單）：一份正式記錄，包含在構建軟體或人工智慧模型時所使用各種元件的詳細資料及供應鏈關係。
  ​
* SHAP（Shapley 加法解釋）：一種基於博弈論的方法，用於解釋任何機器學習模型的輸出，通過計算每個特徵對預測的貢獻。
  ​
* 供應鏈攻擊：透過鎖定供應鏈中較不安全的元素（如第三方函式庫、資料集或預訓練模型）來攻破系統。
  ​
* 遷移學習：一種技術，將為一項任務開發的模型作為第二項任務模型的起點加以重用。
  ​
* 向量資料庫：一種專門設計用於儲存高維向量（嵌入）並執行高效相似性搜尋的資料庫。
  ​
* 漏洞掃描：使用自動化工具識別軟體元件中已知的安全漏洞，包括人工智慧框架和依賴項。
  ​
* 浮水印技術：在 AI 生成的內容中嵌入難以察覺的標記，用以追蹤其來源或檢測 AI 生成痕跡。
  ​
* 零日漏洞：一種先前未知的漏洞，攻擊者能在開發者創建並部署修補程式之前利用該漏洞。

