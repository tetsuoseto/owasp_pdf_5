# ภาคผนวก ก: อภิธานศัพท์

> พจนานุกรมฉบับสมบูรณ์นี้ให้คำนิยามของคำสำคัญในด้านปัญญาประดิษฐ์ การเรียนรู้ของเครื่อง และความปลอดภัยที่ใช้ตลอดเอกสาร AISVS เพื่อให้มั่นใจในความชัดเจนและความเข้าใจร่วมกัน
> ​
* ตัวอย่างปฏิเสธ: อินพุตที่ถูกสร้างขึ้นมาโดยจงใจเพื่อทำให้โมเดล AI ตัดสินผิดพลาด โดยมักเพิ่มการเปลี่ยนแปลงเล็กน้อยที่มนุษย์ไม่สามารถสังเกตเห็นได้
  ​
* ความทนทานต่อการโจมตี – ความทนทานต่อการโจมตีในปัญญาประดิษฐ์หมายถึงความสามารถของโมเดลในการรักษาประสิทธิภาพและต่อต้านการถูกหลอกหรือถูกจัดการโดยข้อมูลป้อนที่ถูกออกแบบมาอย่างมีเจตนาร้ายเพื่อก่อให้เกิดข้อผิดพลาด
  ​
* เอเจนต์ – เอไอเอเจนต์คือระบบซอฟต์แวร์ที่ใช้ปัญญาประดิษฐ์ในการดำเนินการบรรลุเป้าหมายและทำงานแทนผู้ใช้ พวกมันแสดงความสามารถในการให้เหตุผล การวางแผน และความจำ และมีระดับความเป็นอิสระในการตัดสินใจ เรียนรู้ และปรับตัว
  ​
* เอเยนติก AI: ระบบ AI ที่สามารถทำงานด้วยระดับของความเป็นอิสระบางส่วนเพื่อบรรลุเป้าหมาย โดยมักตัดสินใจและดำเนินการโดยไม่ต้องมีการแทรกแซงโดยตรงจากมนุษย์
  ​
* การควบคุมการเข้าถึงแบบอิงตามคุณสมบัติ (Attribute-Based Access Control - ABAC): รูปแบบการควบคุมการเข้าถึงที่ตัดสินใจอนุญาตโดยพิจารณาจากคุณสมบัติของผู้ใช้ ทรัพยากร การกระทำ และสภาพแวดล้อม โดยทำการประเมินในขณะที่มีการร้องขอข้อมูล
  ​
* การโจมตีแบบแบ็คดอร์: เป็นประเภทของการโจมตีโดยการปนเปื้อนข้อมูลซึ่งแบบจำลองถูกฝึกให้ตอบสนองในลักษณะเฉพาะต่อทริกเกอร์บางอย่างในขณะที่ทำงานตามปกติในสถานการณ์อื่น ๆ
  ​
* อคติ: ความผิดพลาดเชิงระบบในผลลัพธ์ของโมเดลปัญญาประดิษฐ์ที่อาจนำไปสู่ผลลัพธ์ที่ไม่เป็นธรรมหรือการเลือกปฏิบัติต่อกลุ่มใดกลุ่มหนึ่งหรือในบริบทเฉพาะบางอย่าง
  ​
* การใช้ประโยชน์จากอคติ: เทคนิคการโจมตีที่ใช้ประโยชน์จากอคติที่ทราบในโมเดล AI เพื่อควบคุมผลลัพธ์หรือผลลัพธ์ที่ได้
  ​
* Cedar: ภาษาและเครื่องมือของ Amazon สำหรับกำหนดนโยบายที่ละเอียดเพื่ออนุญาตแบบละเอียดซึ่งใช้ในการนำ ABAC ไปใช้กับระบบปัญญาประดิษฐ์ (AI)
  ​
* สายความคิด: เทคนิคสำหรับการปรับปรุงกระบวนการวิเคราะห์ในโมเดลภาษาโดยการสร้างขั้นตอนการวิเคราะห์กลางก่อนที่จะให้คำตอบสุดท้าย
  ​
* เบรกเกอร์วงจร: กลไกที่หยุดการทำงานของระบบปัญญาประดิษฐ์โดยอัตโนมัติเมื่อเกินเกณฑ์ความเสี่ยงที่กำหนดไว้
  ​
* การรั่วไหลของข้อมูล: การเปิดเผยข้อมูลที่ละเอียดอ่อนโดยไม่ได้ตั้งใจผ่านผลลัพธ์หรือพฤติกรรมของโมเดล AI
  ​
* การปนเปื้อนข้อมูล: การทำลายข้อมูลการฝึกอบรมอย่างตั้งใจเพื่อทำลายความสมบูรณ์ของโมเดล โดยมักจะทำเพื่อแฝงช่องโหว่หรือทำให้ประสิทธิภาพลดลง
  ​
* ความเป็นส่วนตัวแบบแตกต่าง – ความเป็นส่วนตัวแบบแตกต่างเป็นกรอบทางคณิตศาสตร์ที่เข้มงวดสำหรับการเผยแพร่ข้อมูลสถิติเกี่ยวกับชุดข้อมูลในขณะที่ปกป้องความเป็นส่วนตัวของแต่ละบุคคล มันช่วยให้ผู้ถือข้อมูลสามารถแบ่งปันรูปแบบรวมของกลุ่มได้ในขณะที่จำกัดข้อมูลที่รั่วไหลเกี่ยวกับบุคคลเฉพาะเจาะจง
  ​
* การฝังตัว: การแทนข้อมูลในรูปแบบเวกเตอร์ที่หนาแน่น (ข้อความ, รูปภาพ ฯลฯ) ซึ่งจับความหมายเชิงภาษาศาสตร์ในพื้นที่มิติสูง
  ​
* ความสามารถในการอธิบาย – ความสามารถในการอธิบายในปัญญาประดิษฐ์ (AI) คือความสามารถของระบบ AI ในการให้เหตุผลที่มนุษย์เข้าใจได้สำหรับการตัดสินใจและการทำนายของมัน โดยให้ข้อมูลเชิงลึกเกี่ยวกับกระบวนการภายในของระบบนั้น
  ​
* AI ที่อธิบายได้ (Explainable AI หรือ XAI): ระบบ AI ที่ออกแบบมาเพื่อให้คำอธิบายที่มนุษย์เข้าใจได้เกี่ยวกับการตัดสินใจและพฤติกรรมของระบบผ่านเทคนิคและกรอบงานต่างๆ
  ​
* การเรียนรู้แบบสหกรณ์: วิธีการเรียนรู้ของเครื่องที่โมเดลถูกฝึกฝนผ่านอุปกรณ์กระจายตัวหลายเครื่องที่ถือข้อมูลตัวอย่างในเครื่องท้องถิ่นโดยไม่ต้องแลกเปลี่ยนข้อมูลนั้นเอง
  ​
* เกราะป้องกัน: ข้อจำกัดที่นำมาใช้เพื่อป้องกันระบบปัญญาประดิษฐ์ไม่ให้ผลิตผลลัพธ์ที่เป็นอันตราย มีอคติ หรือไม่พึงประสงค์อื่น ๆ
  ​
* ฮัลลูซิเนชัน – ฮัลลูซิเนชันของปัญญาประดิษฐ์หมายถึงปรากฏการณ์ที่โมเดล AI สร้างข้อมูลที่ไม่ถูกต้องหรือทำให้เข้าใจผิดซึ่งไม่ได้อิงจากข้อมูลการฝึกหรือความจริงตามข้อเท็จจริง
  ​
* มนุษย์ในวงจร (Human-in-the-Loop, HITL): ระบบที่ออกแบบมาให้ต้องมีการตรวจสอบ การยืนยัน หรือการแทรกแซงของมนุษย์ในจุดตัดสินใจที่สำคัญ
  ​
* โครงสร้างพื้นฐานเป็นโค้ด (IaC): การจัดการและจัดเตรียมโครงสร้างพื้นฐานผ่านโค้ดแทนกระบวนการด้วยมือ เพื่อเปิดใช้งานการสแกนความปลอดภัยและการปรับใช้อย่างสม่ำเสมอ
  ​
* การปลดล็อก: เทคนิคที่ใช้เพื่อหลีกเลี่ยงมาตรการความปลอดภัยในระบบ AI โดยเฉพาะในโมเดลภาษาขนาดใหญ่ เพื่อสร้างเนื้อหาที่ถูกห้าม
  ​
* สิทธิ์น้อยที่สุด: หลักการด้านความปลอดภัยที่กำหนดให้มอบสิทธิ์การเข้าถึงที่จำเป็นขั้นต่ำสุดแก่ผู้ใช้และกระบวนการเท่านั้น
  ​
* LIME (คำอธิบายแบบจำลองที่ไม่ขึ้นกับโมเดลและอธิบายได้ในพื้นที่ท้องถิ่น): เทคนิคหนึ่งในการอธิบายการทำนายของเครื่องมือจำแนกโดยใช้การเรียนรู้ของเครื่องใดๆ โดยประมาณค่าอย่างใกล้เคียงในพื้นที่ท้องถิ่นด้วยแบบจำลองที่เข้าใจได้ง่าย
  ​
* การโจมตีการสืบค้นการเป็นสมาชิก: การโจมตีที่มีเป้าหมายเพื่อระบุว่าข้อมูลเฉพาะจุดใดถูกใช้ในการฝึกโมเดลการเรียนรู้ของเครื่องหรือไม่
  ​
* MITRE ATLAS: ภูมิทัศน์ภัยคุกคามฝ่ายตรงข้ามสำหรับระบบปัญญาประดิษฐ์; ฐานความรู้เกี่ยวกับยุทธวิธีและเทคนิคของฝ่ายตรงข้ามต่อระบบ AI
  ​
* บัตรโมเดล – บัตรโมเดลคือเอกสารที่ให้ข้อมูลมาตรฐานเกี่ยวกับประสิทธิภาพ ข้อจำกัด การใช้งานที่ตั้งใจไว้ และการพิจารณาทางจริยธรรมของโมเดลปัญญาประดิษฐ์ เพื่อส่งเสริมความโปร่งใสและการพัฒนาปัญญาประดิษฐ์อย่างรับผิดชอบ
  ​
* การสกัดแบบจำลอง: การโจมตีที่ฝ่ายตรงข้ามทำการสอบถามแบบจำลองเป้าหมายซ้ำ ๆ เพื่อสร้างสำเนาที่มีฟังก์ชันการทำงานคล้ายกันโดยไม่ได้รับอนุญาต
  ​
* การย้อนกลับแบบจำลอง: การโจมตีที่พยายามสร้างข้อมูลการฝึกอบรมใหม่โดยการวิเคราะห์ผลลัพธ์ของแบบจำลอง
  ​
* การจัดการวงจรชีวิตโมเดล – การจัดการวงจรชีวิตโมเดล AI คือกระบวนการดูแลทุกขั้นตอนของการมีอยู่ของโมเดล AI รวมถึงการออกแบบ การพัฒนา การนำไปใช้ การติดตามผล การบำรุงรักษา และการปลดระวางในที่สุด เพื่อให้แน่ใจว่าโมเดลยังคงมีประสิทธิภาพและสอดคล้องกับวัตถุประสงค์
  ​
* การโจมตีแบบปนเปื้อนโมเดล: การแทรกช่องโหว่หรือประตูหลังลงในโมเดลโดยตรงในระหว่างกระบวนการฝึกสอน
  ​
* การขโมย/ลักลอบโมเดล: การดึงข้อมูลสำเนาหรือรูปแบบประมาณของโมเดลที่เป็นกรรมสิทธิ์ผ่านการสอบถามซ้ำๆ
  ​
* ระบบตัวแทนหลายตัว: ระบบที่ประกอบด้วยตัวแทนปัญญาประดิษฐ์หลายตัวที่มีปฏิสัมพันธ์กัน โดยแต่ละตัวอาจมีความสามารถและเป้าหมายที่แตกต่างกัน
  ​
* OPA (Open Policy Agent): เป็นเครื่องยนต์นโยบายแบบโอเพนซอร์สที่ช่วยให้การบังคับใช้นโยบายแบบรวมศูนย์ทั่วทั้งสแตกได้อย่างเป็นเอกภาพ
  ​
* การเรียนรู้ของเครื่องที่รักษาความเป็นส่วนตัว (PPML): เทคนิคและวิธีการในการฝึกอบรมและใช้งานโมเดลเรียนรู้ของเครื่องโดยคุ้มครองความเป็นส่วนตัวของข้อมูลการฝึกอบรม
  ​
* การโจมตีแบบ Prompt Injection: การโจมตีที่มีการฝังคำสั่งที่เป็นอันตรายลงในอินพุตเพื่อแทนที่พฤติกรรมที่ตั้งใจไว้ของโมเดล
  ​
* RAG (การสร้างเสริมโดยการเรียกข้อมูล): เทคนิคที่ช่วยเพิ่มประสิทธิภาพให้กับโมเดลภาษาขนาดใหญ่โดยการดึงข้อมูลที่เกี่ยวข้องจากแหล่งความรู้นอกระบบก่อนที่จะสร้างคำตอบ
  ​
* การทดสอบเชิงรุก (Red-Teaming): การปฏิบัติในการทดสอบระบบปัญญาประดิษฐ์โดยการจำลองการโจมตีจากฝ่ายตรงข้ามเพื่อระบุช่องโหว่ต่างๆ
  ​
* SBOM (บัญชีวัสดุซอฟต์แวร์): บันทึกอย่างเป็นทางการที่มีรายละเอียดและความสัมพันธ์ในห่วงโซ่อุปทานของส่วนประกอบต่างๆ ที่ใช้ในการสร้างซอฟต์แวร์หรือโมเดลปัญญาประดิษฐ์ (AI)
  ​
* SHAP (SHapley Additive exPlanations): วิธีการทางทฤษฎีเกมเพื่ออธิบายผลลัพธ์ของโมเดลการเรียนรู้ของเครื่องใดๆ โดยการคำนวณการมีส่วนร่วมของแต่ละคุณลักษณะต่อการทำนายผล
  ​
* การโจมตีห่วงโซ่อุปทาน: การแทรกแซงระบบโดยการมุ่งเป้าไปที่ส่วนที่มีความปลอดภัยน้อยกว่าในห่วงโซ่อุปทาน เช่น ไลบรารีบุคคลที่สาม ชุดข้อมูล หรือโมเดลที่ผ่านการฝึกสอนล่วงหน้า
  ​
* การเรียนรู้แบบถ่ายโอน: เทคนิคที่ใช้โมเดลที่พัฒนาขึ้นสำหรับงานหนึ่งมาใช้เป็นจุดเริ่มต้นสำหรับโมเดลในงานที่สอง
  ​
* ฐานข้อมูลเวกเตอร์: ฐานข้อมูลเฉพาะที่ออกแบบมาเพื่อเก็บเวกเตอร์มิติสูง (embeddings) และทำการค้นหาความคล้ายคลึงได้อย่างมีประสิทธิภาพ
  ​
* การสแกนช่องโหว่: เครื่องมืออัตโนมัติที่ระบุช่องโหว่ด้านความปลอดภัยที่ทราบแล้วในส่วนประกอบซอฟต์แวร์ รวมถึงกรอบงาน AI และการพึ่งพา
  ​
* การประทับลายน้ำ: เทคนิคในการฝังเครื่องหมายที่มองไม่เห็นลงในเนื้อหาที่สร้างโดยปัญญาประดิษฐ์เพื่อการติดตามแหล่งที่มา หรือการตรวจจับการสร้างโดย AI
  ​
* ช่องโหว่วันศูนย์: ช่องโหว่ที่ไม่เคยรู้จักมาก่อนซึ่งผู้โจมตีสามารถใช้ประโยชน์ได้ก่อนที่นักพัฒนาจะสร้างและปล่อยแพตช์แก้ไขออกมา

