# পরিশিষ্ট A: শব্দকোষ

> এই বিস্তৃত শব্দকোষটি AISVS এর জুড়ে ব্যবহৃত মূল AI, ML, এবং সুরক্ষা সম্পর্কিত শর্তাবলীর সংজ্ঞা প্রদান করে স্পষ্টতা এবং সাধারণ বোঝাপড়া নিশ্চিত করার জন্য।
> ​
* বৈপরীত্যমূলক উদাহরণ: একটি ইনপুট যা ইচ্ছাকৃতভাবে তৈরি করা হয় একটি AI মডেলকে ভুল করতে প্ররোচিত করার জন্য, প্রায়শই এমন সূক্ষ্ম পরিবর্তন যোগ করে যা মানুষের জন্য অনুধাবনযোগ্য নয়।
  ​
* বৈরী প্রতিরোধ ক্ষমতা – AI-তে বৈরী প্রতিরোধ ক্ষমতা বলতে একটি মডেলের সেই সক্ষমতাকে বোঝায় যা তার কর্মক্ষমতা বজায় রাখতে পারে এবং ইচ্ছাকৃতভাবে তৈরি করা দুর্বৃত্তপূর্ণ ইনপুট দ্বারা বিভ্রান্ত বা প্রভাবিত হওয়া থেকে রক্ষা পায়, যা ত্রুটি সৃষ্টির জন্য ডিজাইন করা হয়।
  ​
* এজেন্ট – AI এজেন্ট হলো সফটওয়্যার সিস্টেম যা ব্যবহারকারীদের পক্ষে উদ্দেশ্য পূরণ এবং কাজ সম্পাদনের জন্য AI ব্যবহার করে। তারা যুক্তি, পরিকল্পনা এবং স্মৃতি প্রদর্শন করে এবং সিদ্ধান্ত গ্রহণ, শেখা এবং মানিয়ে নিতে নির্দিষ্ট মাত্রায় স্বায়ত্তশাসন রাখে।
  ​
* এজেন্টিক AI: AI সিস্টেমগুলি যা কিছু পরিমাণ স্বায়ত্তশাসন নিয়ে কাজ করতে পারে উদ্দেশ্য অর্জনের জন্য, প্রায়ই সিদ্ধান্ত নিয়ে এবং সরাসরি মানব হস্তক্ষেপ ছাড়াই কর্ম গ্রহণ করে।
  ​
* অ্যাট্রিবিউট-ভিত্তিক অ্যাক্সেস কন্ট্রোল (ABAC): একটি অ্যাক্সেস কন্ট্রোল প্যারাডাইম যেখানে অনুমোদন সিদ্ধান্তগুলি ব্যবহারকারী, সম্পদ, ক্রিয়া, এবং পরিবেশের অ্যাট্রিবিউটের উপর ভিত্তি করে হয়, যা কোয়েরি সময়ে মূল্যায়ন করা হয়।
  ​
* ব্যাকডোর আক্রমণ: একটি ধরণের ডেটা পয়জনিং আক্রমণ যেখানে মডেলকে নির্দিষ্ট ট্রিগারগুলির প্রতি একটি নির্দিষ্ট ভাবে প্রতিক্রিয়া জানাতে প্রশিক্ষণ দেওয়া হয়, যখন অন্য সময় স্বাভাবিকভাবে আচরণ করে।
  ​
* পক্ষপাত: AI মডেল আউটপুটে সিস্টেম্যাটিক ভুল যা কিছু নির্দিষ্ট গোষ্ঠী বা নির্দিষ্ট প্রসঙ্গে অন্যায় বা বৈষম্যমূলক পরিণতি সৃষ্টি করতে পারে।
  ​
* বায়াস শোষণ: একটি আক্রমণ পদ্ধতি যা এআই মডেলগুলির পরিচিত পক্ষপাতের সুবিধা নিয়ে আউটপুট বা ফলাফল পরিচালনা করে।
  ​
* সি্ডার: অ্যামাজনের নীতি ভাষা এবং ইঞ্জিন যা AI সিস্টেমগুলোর জন্য ABAC বাস্তবায়নে সূক্ষ্ম-পরিমাণের অনুমতির জন্য ব্যবহৃত হয়।
  ​
* চেইন অফ থট: ভাষা মডেলগুলিতে যুক্তি উন্নত করার একটি প্রযুক্তি, যেখানে চূড়ান্ত উত্তর প্রদানের আগে মধ্যবর্তী যুক্তি ধাপ তৈরি করা হয়।
  ​
* সার্কিট ব্রেকার: যন্ত্রাংশ যা নির্দিষ্ট ঝুঁকি মাত্রা অতিক্রম করলে স্বয়ংক্রিয়ভাবে AI সিস্টেমের কার্যক্রম বন্ধ করে দেয়।
  ​
* ডেটা লিকেজ: AI মডেল আউটপুট বা আচরণের মাধ্যমে সংবেদনশীল তথ্যের অবাঞ্ছিত প্রকাশ।
  ​
* ডেটা পয়জনিং: মডেলের অখণ্ডতা ক্ষুণ্ণ করার জন্য প্রশিক্ষণ ডেটা ইচ্ছাকৃতভাবে দূষিত করা, যা প্রায়শই ব্যাকডোর ইনস্টল করা বা পারফরম্যান্স হ্রাস করার উদ্দেশ্যে হয়।
  ​
* ডিফারেনশিয়াল প্রাইভেসি – ডিফারেনশিয়াল প্রাইভেসি হল একটি গাণিতিকভাবে কঠোর কাঠামো যা ডেটাসেট সম্পর্কে পরিসংখ্যানগত তথ্য প্রকাশ করে একই সাথে পৃথক তথ্য বিষয়গুলোর গোপনীয়তা রক্ষা করে। এটি একটি ডেটা হোল্ডারকে গ্রুপের সামষ্টিক প্যাটার্ন শেয়ার করার অনুমতি দেয়, যখন নির্দিষ্ট ব্যক্তিদের সম্পর্কে লিক হওয়া তথ্য সীমাবদ্ধ রাখে।
  ​
* এম্বেডিংস: ডেটার (টেক্সট, ছবি ইত্যাদি) ঘনীভূত ভেক্টর উপস্থাপনা যা উচ্চ-মাত্রিক স্থানে অর্থবহ অর্থ ধারণ করে।
  ​
* ব্যাখ্যাযোগ্যতা – AI-তে ব্যাখ্যাযোগ্যতা হল একটি AI সিস্টেমের সেই ক্ষমতা যার মাধ্যমে এটি তার সিদ্ধান্ত এবং পূর্বাভাসের জন্য মানুষের বোঝার মতো কারণ প্রদান করতে পারে, এবং তার অভ্যন্তরীণ কাজকর্ম সম্পর্কে অন্তর্দৃষ্টি দেয়।
  ​
* বর্ণনাযোগ্য AI (XAI): AI সিস্টেমগুলি যা তাদের সিদ্ধান্ত এবং আচরণের জন্য মানুষের বোঝার উপযোগী ব্যাখ্যা প্রদান করার জন্য বিভিন্ন প্রযুক্তি এবং কাঠামোর মাধ্যমে ডিজাইন করা হয়েছে।
  ​
* ফেডারেটেড লার্নিং: একটি মেশিন লার্নিং পদ্ধতি যেখানে মডেলগুলি একাধিক বিকেন্দ্রীকৃত ডিভাইসে প্রশিক্ষিত হয়, যেগুলো নিজস্ব স্থানীয় ডেটা নমুনা ধারণ করে, তবে ডেটাগুলো বিনিময় করা হয় না।
  ​
* গার্ডরেইলস: সেসব শর্তাবলী যা এআই সিস্টেমগুলোকে ক্ষতিকর, পক্ষপাতমূলক বা অন্য কোনো অবাঞ্ছিত ফলাফল তৈরির থেকে রোধ করার জন্য প্রয়োগ করা হয়।
  ​
* হ্যালুসিনেশন – একটি AI হ্যালুসিনেশন একটি ঘটনাকে নির্দেশ করে যেখানে একটি AI মডেল তার প্রশিক্ষণ ডেটা বা বাস্তব তথ্যের উপর ভিত্তি না করে ভুল বা বিভ্রান্তিকর তথ্য তৈরি করে।
  ​
* হিউম্যান-ইন-দ্য-লুপ (HITL): সিস্টেমগুলি যা গুরুত্বপূর্ণ সিদ্ধান্ত গ্রহণের সময় মানব পর্যবেক্ষণ, যাচাই বা হস্তক্ষেপের প্রয়োজনীয়তা অনুযায়ী ডিজাইন করা হয়েছে।
  ​
* ইনফ্রাস্ট্রাকচার অ্যাজ কোড (IaC): ম্যানুয়াল প্রক্রিয়ার পরিবর্তে কোডের মাধ্যমে ইনফ্রাস্ট্রাকচার পরিচালনা এবং প্রদান, যা সিকিউরিটি স্ক্যানিং এবং সঙ্গতিপূর্ণ ডিপ্লয়মেন্ট সক্ষম করে।
  ​
* জেলব্রেক: এআই সিস্টেমগুলিতে, বিশেষ করে বড় ভাষা মডেলগুলোতে, নিরাপত্তা বাধাসমূহ এড়িয়ে নিষিদ্ধ বিষয়বস্তু তৈরি করার জন্য ব্যবহৃত প্রযুক্তি।
  ​
* সর্বনিম্ন অনুমতি: ব্যবহারকারী এবং প্রক্রিয়াগুলির জন্য শুধুমাত্র সর্বনিম্ন প্রয়োজনীয় প্রবেশাধিকার দেওয়ার নিরাপত্তা নীতি।
  ​
* LIME (লোকাল ইন্টারপ্রেটেবল মডেল-অ্যাগনস্টিক এক্সপ্ল্যানেশন): যেকোনো মেশিন লার্নিং ক্লাসিফায়ারের পূর্বাভাস ব্যাখ্যা করার একটি প্রযুক্তি, যা সেটিকে লোকালি একটি ইন্টারপ্রেটেবল মডেলের সাথে আনুমানিকভাবে বোঝায়।
  ​
* সদস্যপদ অনুসন্ধান আক্রমণ: একটি আক্রমণ যা নির্ধারণের লক্ষ্য রাখে যে একটি নির্দিষ্ট ডেটা পয়েন্ট মেশিন লার্নিং মডেল প্রশিক্ষণে ব্যবহৃত হয়েছে কিনা।
  ​
* MITRE ATLAS: কৃত্রিম বুদ্ধিমত্তা সিস্টেমের জন্য প্রতিদ্বন্দ্বী হুমকির দৃশ্যপট; AI সিস্টেমের বিরুদ্ধে প্রতিদ্বন্দ্বী কৌশল এবং প্রযুক্তিগুলোর একটি তথ্যভান্ডার।
  ​
* মডেল কার্ড – একটি মডেল কার্ড হল একটি ডকুমেন্ট যা একটি AI মডেলের কর্মক্ষমতা, সীমাবদ্ধতা, প্রত্যাশিত ব্যবহার এবং নৈতিক বিবেচনাগুলির বিষয়ে মানসম্মত তথ্য প্রদান করে, যা স্বচ্ছতা এবং দায়িত্বশীল AI উন্নয়নকে প্রচার করে।
  ​
* মডেল এক্সট্র্যাকশন: একটি আক্রমণ যেখানে একজন প্রতিপক্ষ বারংবার একটি লক্ষ্য মডেলকে প্রশ্ন করে অনুমোদন ছাড়াই একটি কার্যকরভাবে অনুরূপ কপি সৃষ্টি করে।
  ​
* মডেল ইনভার্সন: একটি আক্রমণ যা মডেল আউটপুট বিশ্লেষণ করে ট্রেনিং ডেটা পুনর্গঠন করার চেষ্টা করে।
  ​
* মডেল লাইফসাইকেল ম্যানেজমেন্ট – AI মডেল লাইফসাইকেল ম্যানেজমেন্ট হলো একটি AI মডেলের সমস্ত পর্যায়ের তদারকি করার প্রক্রিয়া, যার মধ্যে রয়েছে এর ডিজাইন, উন্নয়ন, মোতায়েন, পর্যবেক্ষণ, রক্ষণাবেক্ষণ এবং শেষ পর্যন্ত অবসান, যাতে এটি কার্যকর থাকে এবং লক্ষ্যগুলোর সাথে সামঞ্জস্যপূর্ণ থাকে।
  ​
* মডেল পয়জনিং: প্রশিক্ষণ প্রক্রিয়ার সময় সরাসরি মডেলের মধ্যে দুর্বলতা বা ব্যাকডোর প্রবর্তন।
  ​
* মডেল চুরি/চুরি: পুনরাবৃত্ত প্রশ্নের মাধ্যমে একটি মালিকানাধীন মডেলের একটি কপি বা আনুমানিক তথ্য আহরণ।
  ​
* মাল্টি-এজেন্ট সিস্টেম: একটি সিস্টেম যা একাধিক আন্তঃক্রিয়াশীল AI এজেন্ট দ্বারা গঠিত, যাদের প্রত্যেকটির বিভিন্ন সক্ষমতা এবং লক্ষ্য থাকতে পারে।
  ​
* OPA (Open Policy Agent): একটি ওপেন-সোর্স পলিসি ইঞ্জিন যা স্ট্যাক জুড়ে একীকৃত পলিসি প্রয়োগ সক্ষম করে।
  ​
* গোপনীয়তা রক্ষাকারী মেশিন লার্নিং (PPML): প্রশিক্ষণ ডেটার গোপনীয়তা রক্ষা করে এমএল মডেল প্রশিক্ষণ এবং মোতায়েন করার কৌশল এবং পদ্ধতি।
  ​
* প্রম্পট ইনজেকশন: একটি আক্রমণ যেখানে ম্যালিসিয়াস নির্দেশাবলী ইনপুটে প্রবেশ করিয়ে একটি মডেলের উদ্দেশ্য পরিবর্তিত আচরণ ওভাররাইট করা হয়।
  ​
* RAG (রিট্রিভাল-অগমেন্টেড জেনারেশন): একটি প্রযুক্তি যা বড় ভাষার মডেলগুলিকে উন্নত করে প্রাসঙ্গিক তথ্য বাইরের জ্ঞান উৎস থেকে সংগ্রহ করে প্রতিক্রিয়া তৈরির আগে।
  ​
* রেড-টিমিং: প্রতিকূল হামলার নকল করে AI সিস্টেমগুলো সক্রিয়ভাবে পরীক্ষা করার মাধ্যমে দুর্বলতাগুলো শনাক্ত করার অনুশীলন।
  ​
* SBOM (সফটওয়্যার বিল অফ ম্যাটেরিয়ালস): একটি আনুষ্ঠানিক রেকর্ড যা সফটওয়্যার বা AI মডেল তৈরি করতে ব্যবহৃত বিভিন্ন উপাদানের বিস্তারিত এবং সরবরাহ শৃঙ্খল সম্পর্ক ধারণ করে।
  ​
* SHAP (SHapley Additive exPlanations): যেকোনো মেশিন লার্নিং মডেলের আউটপুট ব্যাখ্যা করার জন্য একটি গেম তাত্ত্বিক পদ্ধতি, যা প্রত্যেকটি ফিচারের ভবিষ্যদ্বাণীতে অবদান নিরূপণ করে।
  ​
* সরবরাহ শৃঙ্খল আক্রমণ: এর সরবরাহ শৃঙ্খলের কম সুরক্ষিত উপাদানসমূহকে লক্ষ্য করে, যেমন তৃতীয়-পক্ষের লাইব্রেরি, ডেটাসেট, বা পূর্বপ্রশিক্ষিত মডেলগুলি, সিস্টেমকে কমিশ্রোমাইজ করা।
  ​
* ট্রান্সফার লার্নিং: একটি প্রযুক্তি যেখানে একটি কাজের জন্য উন্নত মডেলকে দ্বিতীয় কাজের জন্য মডেলের শুরু হিসাবে পুনঃব্যবহার করা হয়।
  ​
* ভেক্টর ডেটাবেস: একটি বিশেষায়িত ডেটাবেস যা উচ্চ-মাত্রিক ভেক্টর (এম্বেডিংস) সংরক্ষণ এবং কার্যকরী সাদৃশ্য অনুসন্ধান করার জন্য ডিজাইন করা হয়েছে।
  ​
* ভালনারেবিলিটি স্ক্যানিং: স্বয়ংক্রিয় সরঞ্জামগুলি যা সফ্টওয়্যার উপাদানগুলির মধ্যে পরিচিত সিকিউরিটি দুর্বলতাগুলি চিহ্নিত করে, এতে AI ফ্রেমওয়ার্ক এবং নির্ভরতা অন্তর্ভুক্ত।
  ​
* ওয়াটারমার্কিং: AI-উত্পাদিত কনটেন্টে অদৃশ্য মার্কার এমবেড করার প্রযুক্তি যা এর উৎস ট্র্যাক করার বা AI দ্বারা সৃষ্টির সনাক্তকরণে ব্যবহৃত হয়।
  ​
* জিরো-ডে দুর্বলতা: একটি পূর্বে অজানা দুর্বলতা যা আক্রমণকারীরা উন্নয়নকারীরা প্যাচ তৈরি এবং স্থাপন করার আগেই শোষণ করতে পারে।

