# Frontispice

## À propos de la norme

La Norme de Vérification de la Sécurité de l'Intelligence Artificielle (AISVS) est un catalogue de exigences de sécurité piloté par la communauté, que les data scientists, ingénieurs MLOps, architectes logiciels, développeurs, testeurs, professionnels de la sécurité, fournisseurs d'outils, régulateurs et consommateurs peuvent utiliser pour concevoir, construire, tester et vérifier des systèmes et applications fiables dotés d'IA. Elle fournit un langage commun pour spécifier les contrôles de sécurité tout au long du cycle de vie de l'IA — de la collecte des données et du développement des modèles à leur déploiement et à leur surveillance continue — afin que les organisations puissent mesurer et améliorer la résilience, la confidentialité et la sécurité de leurs solutions d'IA.

## Droits d'auteur et licence

Version 0.1 (Première ébauche publique - Travail en cours), 2025  

![license](../images/license.png)

Droits d'auteur © 2025 Le Projet AISVS.  

Publié sous la[Creative Commons Attribution‑ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).  
Pour toute réutilisation ou distribution, vous devez clairement communiquer les conditions de licence de ce travail aux autres.

## Chefs de projet

|            |                           |
| ---------- | ------------------------- |
| Jim Manico | Aras « Russ » Memisyazici |

## Contributeurs et évaluateurs

|                                    |                             |
| ---------------------------------- | --------------------------- |
| https://github.com/ottosulin       | https://github.com/mbhatt1  |
| https://github.com/vineethsai      | https://github.com/cciprofm |
| https://github.com/deepakrpandey12 |                             |

---

AISVS est une toute nouvelle norme créée spécifiquement pour répondre aux défis uniques de sécurité des systèmes d'intelligence artificielle. Bien qu'elle s'inspire des meilleures pratiques de sécurité plus larges, chaque exigence d'AISVS a été développée de zéro pour refléter le paysage des menaces liées à l'IA et aider les organisations à construire des solutions d'IA plus sûres et plus résilientes.

