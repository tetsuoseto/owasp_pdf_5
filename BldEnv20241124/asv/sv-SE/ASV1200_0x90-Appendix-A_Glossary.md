# Bilaga A: Ordlista

> Denna omfattande ordlista ger definitioner av viktiga AI-, ML- och säkerhetstermer som används i hela AISVS för att säkerställa tydlighet och en gemensam förståelse.
> ​
* Fientligt exempel: En indata som avsiktligt utformats för att få en AI-modell att göra ett misstag, ofta genom att lägga till subtila störningar som är omärkliga för människor.
  ​
* Motståndskraft mot angrepp – Motståndskraft mot angrepp inom AI avser en modells förmåga att bibehålla sin prestanda och motstå att bli lurad eller manipulerad av avsiktligt skapade, illvilliga indata som är utformade för att orsaka fel.
  ​
* Agent – AI-agenter är mjukvarusystem som använder AI för att driva mål och slutföra uppgifter för användares räkning. De uppvisar resonemang, planering och minne och har en nivå av autonomi för att fatta beslut, lära sig och anpassa sig.
  ​
* Agentisk AI: AI-system som kan fungera med viss grad av autonomi för att uppnå mål, ofta genom att fatta beslut och vidta åtgärder utan direkt mänsklig inblandning.
  ​
* Attributbaserad åtkomstkontroll (ABAC): Ett åtkomstkontrollparadigm där auktorisationsbeslut baseras på attribut hos användaren, resursen, åtgärden och miljön, som utvärderas vid förfrågningstid.
  ​
* Bakdörrsattack: En typ av dataförgiftningattack där modellen tränas att reagera på ett specifikt sätt på vissa triggers medan den beter sig normalt i övrigt.
  ​
* Bias: Systematiska fel i AI-modellernas resultat som kan leda till orättvisa eller diskriminerande utfall för vissa grupper eller i specifika sammanhang.
  ​
* Biasutnyttjande: En angreppsteknik som utnyttjar kända bias i AI-modeller för att manipulera resultat eller utfall.
  ​
* Cedar: Amazons policyspråk och motor för granulära behörigheter som används vid implementering av ABAC för AI-system.
  ​
* Tankekedja: En teknik för att förbättra resonemang i språkmodeller genom att generera mellanliggande resonemangssteg innan ett slutgiltigt svar produceras.
  ​
* Brytare: Mekanismer som automatiskt stoppar AI-systemets operationer när specifika risktrösklar överskrids.
  ​
* Dataintrång: Oavsiktlig exponering av känslig information genom AI-modellens output eller beteende.
  ​
* Databesmittning: Den avsiktliga förstörelsen av träningsdata för att äventyra modellens integritet, ofta för att installera bakdörrar eller försämra prestanda.
  ​
* Differential Privacy – Differential privacy är en matematiskt rigorös ramverk för att offentliggöra statistisk information om datamängder samtidigt som integriteten för enskilda datapunkter skyddas. Det möjliggör att en dataägare kan dela aggregerade mönster för gruppen samtidigt som information som avslöjas om specifika individer begränsas.
  ​
* Inbäddningar: Täta vektorrepresentationer av data (text, bilder, etc.) som fångar semantisk betydelse i ett högdimensionellt rum.
  ​
* Förklarbarhet – Förklarbarhet inom AI är en AI-systems förmåga att ge människligt begripliga orsaker till sina beslut och förutsägelser, vilket erbjuder insikter i dess interna funktioner.
  ​
* Förklarbar AI (XAI): AI-system utformade för att ge människoförståeliga förklaringar till sina beslut och beteenden genom olika tekniker och ramverk.
  ​
* Federerad inlärning: En maskininlärningsmetod där modeller tränas över flera decentraliserade enheter som innehar lokala dataprover, utan att själva data utbyts.
  ​
* Skyddsräcken: Begränsningar implementerade för att förhindra att AI-system genererar skadliga, partiska eller på annat sätt oönskade resultat.
  ​
* Hallucination – En AI-hallucination avser ett fenomen där en AI-modell genererar felaktig eller vilseledande information som inte är baserad på dess träningsdata eller faktisk verklighet.
  ​
* Människa-i-loopen (HITL): System som är utformade för att kräva mänsklig övervakning, verifiering eller ingripande vid avgörande beslutsfattande punkter.
  ​
* Infrastructure as Code (IaC): Hantering och tillhandahållande av infrastruktur genom kod istället för manuella processer, vilket möjliggör säkerhetsskanning och konsekventa distributioner.
  ​
* Jailbreak: Tekniker som används för att kringgå säkerhetsbegränsningar i AI-system, särskilt i stora språkmodeller, för att producera förbjudet innehåll.
  ​
* Minsta privilegium: Säkerhetsprincipen att endast ge användare och processer de minsta nödvändiga åtkomsträttigheterna.
  ​
* LIME (Lokal Tolkningsbar Modell-agnostisk Förklaring): En teknik för att förklara förutsägelser från vilken maskininlärningsklassificerare som helst genom att approximera den lokalt med en tolkbar modell.
  ​
* Medlemskapsinformationsattack: En attack som syftar till att avgöra om en specifik datapunkt användes för att träna en maskininlärningsmodell.
  ​
* MITRE ATLAS: Hotbild över adversariella hot mot artificiella intelligenssystem; en kunskapsbas över adversariella taktiker och tekniker mot AI-system.
  ​
* Modellkort – Ett modellkort är ett dokument som tillhandahåller standardiserad information om en AI-modells prestanda, begränsningar, avsedda användningsområden och etiska överväganden för att främja transparens och ansvarsfull AI-utveckling.
  ​
* Modelextraktion: En attack där en angripare upprepade gånger gör förfrågningar till en målmodell för att skapa en funktionellt liknande kopia utan tillåtelse.
  ​
* Modellinversion: En attack som försöker rekonstruera träningsdata genom att analysera modellens utdata.
  ​
* Modellens livscykelhantering – AI-modellens livscykelhantering är processen att övervaka alla stadier i en AI-modells existens, inklusive dess design, utveckling, implementering, övervakning, underhåll och slutliga avveckling, för att säkerställa att den förblir effektiv och i linje med målen.
  ​
* Modellförgiftning: Införande av sårbarheter eller bakdörrar direkt i en modell under träningsprocessen.
  ​
* Modellstöld/kapning: Extrahering av en kopia eller approximation av en proprietär modell genom upprepade förfrågningar.
  ​
* Multi-agent System: Ett system som består av flera interagerande AI-agenter, var och en med potentiellt olika kapaciteter och mål.
  ​
* OPA (Open Policy Agent): En öppen källkodspolicy motor som möjliggör enhetlig policysimplementering över hela stacken.
  ​
* Sekretessbevarande maskininlärning (PPML): Tekniker och metoder för att träna och distribuera ML-modeller samtidigt som träningsdata hålls privat.
  ​
* Promptinjektion: En attack där skadliga instruktioner bäddas in i indata för att åsidosätta en modells avsedda beteende.
  ​
* RAG (Retrieval-Augmented Generation): En teknik som förbättrar stora språkmodeller genom att hämta relevant information från externa kunskapskällor innan ett svar genereras.
  ​
* Rödlagstestning: Praktiken att aktivt testa AI-system genom att simulera fientliga attacker för att identifiera sårbarheter.
  ​
* SBOM (Programvarulista över material): En formell dokumentation som innehåller detaljer och leverantörskedjerelationer för olika komponenter som används vid utveckling av programvara eller AI-modeller.
  ​
* SHAP (SHapley Additive exPlanations): En spelteoretisk metod för att förklara resultatet av vilken maskininlärningsmodell som helst genom att beräkna varje egenskaps bidrag till prediktionen.
  ​
* Leveranskedjeattack: Att kompromettera ett system genom att rikta in sig på mindre säkra element i dess leveranskedja, såsom tredjepartsbibliotek, datasets eller förtränade modeller.
  ​
* Transferinlärning: En teknik där en modell utvecklad för en uppgift återanvänds som utgångspunkt för en modell på en annan uppgift.
  ​
* Vektordatabas: En specialiserad databas utformad för att lagra högdimensionella vektorer (inbäddningar) och utföra effektiva likhetssökningar.
  ​
* Sårbarhetsscanning: Automatiserade verktyg som identifierar kända säkerhetssårbarheter i mjukvarukomponenter, inklusive AI-ramverk och beroenden.
  ​
* Vattenmärkning: Tekniker för att infoga omärkliga markörer i AI-genererat innehåll för att spåra dess ursprung eller upptäcka AI-generering.
  ​
* Noll-dagars sårbarhet: En tidigare okänd sårbarhet som angripare kan utnyttja innan utvecklare har skapat och distribuerat en patch.

