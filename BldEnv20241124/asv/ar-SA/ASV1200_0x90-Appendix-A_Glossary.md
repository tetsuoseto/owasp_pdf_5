# الملحق أ: مسرد المصطلحات

> يوفر هذا المسرد الشامل تعريفات للمصطلحات الرئيسية في الذكاء الاصطناعي، وتعلم الآلة، والأمان المستخدمة في جميع أنحاء AISVS لضمان الوضوح والفهم المشترك.
> ​
* مثال معادٍ: مدخل مصمم عمدًا لجعل نموذج الذكاء الاصطناعي يرتكب خطأً، غالبًا من خلال إضافة تشويشات طفيفة لا يمكن للبشر إدراكها.
  ​
* الصلابة ضد الهجمات العدائية – تشير الصلابة ضد الهجمات العدائية في الذكاء الاصطناعي إلى قدرة النموذج على الحفاظ على أدائه ومقاومة الخداع أو التلاعب بواسطة مدخلات خبيثة ومصممة عمداً لإحداث أخطاء.
  ​
* الوكيل – وكلاء الذكاء الاصطناعي هم أنظمة برمجية تستخدم الذكاء الاصطناعي لتحقيق الأهداف وإنجاز المهام نيابةً عن المستخدمين. يظهرون القدرة على التفكير والتخطيط والتذكر ولديهم مستوى من الاستقلالية لاتخاذ القرارات والتعلم والتكيف.
  ​
* الذكاء الاصطناعي الوكلي: أنظمة الذكاء الاصطناعي التي يمكنها العمل بدرجة معينة من الاستقلالية لتحقيق الأهداف، وغالبًا ما تتخذ قرارات وتتخذ إجراءات بدون تدخل بشري مباشر.
  ​
* التحكم في الوصول المعتمد على السمات (ABAC): نموذج تحكم في الوصول يتم فيه اتخاذ قرارات التفويض بناءً على سمات المستخدم والموارد والإجراء والبيئة، ويتم تقييمها في وقت الاستعلام.
  ​
* هجوم الباب الخلفي: نوع من هجمات تسمم البيانات حيث يتم تدريب النموذج للاستجابة بطريقة معينة لمثيرات محددة مع التصرف بشكل طبيعي في الحالات الأخرى.
  ​
* التحيز: أخطاء منهجية في مخرجات نماذج الذكاء الاصطناعي قد تؤدي إلى نتائج غير عادلة أو تمييزية لمجموعات معينة أو في سياقات محددة.
  ​
* استغلال التحيز: تقنية هجوم تستغل التحيزات المعروفة في نماذج الذكاء الاصطناعي للتلاعب بالمخرجات أو النتائج.
  ​
* سيدار: لغة السياسة ومحرك الأمازون لتفويضات دقيقة المدى تُستخدم في تنفيذ التحكم بالوصول المعتمد على السمات (ABAC) لأنظمة الذكاء الاصطناعي.
  ​
* سلسلة التفكير: تقنية لتحسين الاستدلال في نماذج اللغة من خلال توليد خطوات استدلال وسيطة قبل إنتاج الإجابة النهائية.
  ​
* قفازات الدائرة: آليات توقف عمليات نظام الذكاء الاصطناعي تلقائيًا عندما يتم تجاوز حدود مخاطر محددة.
  ​
* تسرب البيانات: الكشف غير المقصود عن المعلومات الحساسة من خلال مخرجات أو سلوك نموذج الذكاء الاصطناعي.
  ​
* تسميم البيانات: الفساد المتعمد لبيانات التدريب للإضرار بسلامة النموذج، وغالبًا ما يكون لتثبيت أبواب خلفية أو تقليل الأداء.
  ​
* الخصوصية التفاضلية – الخصوصية التفاضلية هي إطار رياضي دقيق لنشر المعلومات الإحصائية حول مجموعات البيانات مع حماية خصوصية الأفراد المشاركين في البيانات. تتيح لمالك البيانات مشاركة الأنماط المجتمعية الإجمالية مع الحد من تسرب المعلومات عن الأفراد المحددين.
  ​
* التضمينات: تمثيلات متجهية كثيفة للبيانات (النصوص، الصور، إلخ) تلتقط المعنى الدلالي في فضاء عالي الأبعاد.
  ​
* الشرحية – الشرحية في الذكاء الاصطناعي هي قدرة نظام الذكاء الاصطناعي على تقديم أسباب مفهومة للبشر لقراراته وتنبؤاته، مما يوفر رؤى حول آلياته الداخلية.
  ​
* الذكاء الاصطناعي القابل للتفسير (XAI): نظم الذكاء الاصطناعي المصممة لتقديم تفسيرات يمكن للبشر فهمها لقراراتها وسلوكياتها من خلال تقنيات وأطر عمل مختلفة.
  ​
* التعلم الفدرالي: هو نهج في تعلم الآلة حيث يتم تدريب النماذج عبر عدة أجهزة لامركزية تحتفظ بعينات بيانات محلية، دون تبادل البيانات نفسها.
  ​
* خطوط الحماية: القيود المطبقة لمنع أنظمة الذكاء الاصطناعي من إنتاج مخرجات ضارة أو متحيزة أو غير مرغوب فيها بأي شكل آخر.
  ​
* الهلوسة – تشير هلوسة الذكاء الاصطناعي إلى ظاهرة يقوم فيها نموذج الذكاء الاصطناعي بتوليد معلومات غير صحيحة أو مضللة ليست مستندة إلى بيانات التدريب الخاصة به أو الواقع الفعلي.
  ​
* الإنسان في الحلقة (HITL): أنظمة مصممة لتتطلب إشرافاً أو تحققاً أو تدخلاً بشرياً في نقاط اتخاذ القرار الحاسمة.
  ​
* البنية التحتية كرمز (IaC): إدارة وتوفير البنية التحتية من خلال الكود بدلاً من العمليات اليدوية، مما يتيح فحص الأمان والتنفيذات المتسقة.
  ​
* الهروب من القيد: تقنيات تُستخدم لتجاوز الحواجز الأمنية في أنظمة الذكاء الاصطناعي، خصوصًا في نماذج اللغة الكبيرة، لإنتاج محتوى محظور.
  ​
* الحق الأدنى: مبدأ الأمان الذي يقتصر على منح أقل حقوق وصول ضرورية للمستخدمين والعمليات.
  ​
* لايم (تفسيرات نموذجية محلية مستقلة): تقنية لشرح تنبؤات أي مصنف تعلم آلي عن طريق تقريبها محليًا بنموذج قابل للتفسير.
  ​
* هجوم استنتاج العضوية: هجوم يهدف إلى تحديد ما إذا كانت نقطة بيانات معينة قد تم استخدامها لتدريب نموذج تعلم آلي.
  ​
* MITRE ATLAS: مشهد التهديدات العدائية لأنظمة الذكاء الاصطناعي؛ قاعدة معرفية للاستراتيجيات والتقنيات العدائية ضد أنظمة الذكاء الاصطناعي.
  ​
* بطاقة النموذج – بطاقة النموذج هي وثيقة توفر معلومات موحدة عن أداء نموذج الذكاء الاصطناعي، وقيوده، واستخداماته المقصودة، والاعتبارات الأخلاقية لتعزيز الشفافية وتطوير الذكاء الاصطناعي بمسؤولية.
  ​
* استخراج النموذج: هجوم يقوم فيه المعتدي باستمرار الاستعلام من نموذج مستهدف لإنشاء نسخة مشابهة وظيفيًا دون تفويض.
  ​
* عكس النموذج: هجوم يحاول إعادة بناء بيانات التدريب من خلال تحليل مخرجات النموذج.
  ​
* إدارة دورة حياة النموذج – إدارة دورة حياة نموذج الذكاء الاصطناعي هي عملية الإشراف على جميع مراحل وجود نموذج الذكاء الاصطناعي، بما في ذلك تصميمه وتطويره ونشره ورصده وصيانته وتقاعده النهائي، لضمان استمراره في الفعالية والتوافق مع الأهداف.
  ​
* تسميم النموذج: إدخال ثغرات أو أبواب خلفية مباشرة في النموذج أثناء عملية التدريب.
  ​
* سرقة/استنساخ النموذج: استخراج نسخة أو تقريب لنموذج مملوك من خلال الاستعلامات المتكررة.
  ​
* نظام متعدد الوكلاء: نظام يتكون من عدة وكلاء ذكاء اصطناعي متفاعلين، كل منهم قد يمتلك قدرات وأهداف مختلفة.
  ​
* OPA (عميل السياسة المفتوح): محرك سياسة مفتوح المصدر يتيح تنفيذ السياسات الموحدة عبر كامل البنية.
  ​
* تعلم الآلة مع الحفاظ على الخصوصية (PPML): تقنيات وأساليب لتدريب ونشر نماذج تعلم الآلة مع حماية خصوصية بيانات التدريب.
  ​
* حقن التعليمات: هجوم يتم فيه تضمين تعليمات خبيثة في المدخلات لتجاوز السلوك المقصود للنموذج.
  ​
* الاسترجاع المعزز بالتوليد (RAG): تقنية تعزز النماذج اللغوية الكبيرة من خلال استرجاع المعلومات ذات الصلة من مصادر المعرفة الخارجية قبل توليد الاستجابة.
  ​
* الهجوم الأحمر: ممارسات اختبار أنظمة الذكاء الاصطناعي بنشاط من خلال محاكاة الهجمات العدائية لتحديد نقاط الضعف.
  ​
* قائمة مكونات البرمجيات (SBOM): سجل رسمي يحتوي على تفاصيل وعلاقات سلسلة التوريد لمكونات مختلفة تستخدم في بناء البرمجيات أو نماذج الذكاء الاصطناعي.
  ​
* SHAP (تفسيرات شابلي الإضافية): نهج قائم على نظرية الألعاب لشرح مخرجات أي نموذج تعلم آلي عن طريق حساب مساهمة كل خاصية في التنبؤ.
  ​
* هجوم سلسلة التوريد: اختراق نظام عن طريق استهداف العناصر الأقل أمانًا في سلسلة التوريد الخاصة به، مثل المكتبات الطرف الثالث، مجموعات البيانات، أو النماذج المدربة مسبقًا.
  ​
* التعلم بالنقل: تقنية يتم فيها إعادة استخدام نموذج تم تطويره لمهمة معينة كنقطة انطلاق لنموذج في مهمة ثانية.
  ​
* قاعدة البيانات المتجهة: قاعدة بيانات متخصصة مصممة لتخزين المتجهات ذات الأبعاد العالية (التضمينات) وأداء عمليات البحث عن التشابه بكفاءة.
  ​
* مسح الثغرات الأمنية: أدوات مؤتمتة تقوم بتحديد الثغرات الأمنية المعروفة في مكونات البرامج، بما في ذلك أُطُر الذكاء الاصطناعي والاعتمادات.
  ​
* وضع العلامات المائية: تقنيات تضمين علامات غير ملحوظة في المحتوى الذي تم إنشاؤه بواسطة الذكاء الاصطناعي لتتبع مصدره أو الكشف عن توليده بواسطة الذكاء الاصطناعي.
  ​
* ثغرة يوم الصفر: ثغرة كانت مجهولة سابقًا يمكن للمهاجمين استغلالها قبل أن يقوم المطورون بإنشاء ونشر تصحيح لها.

