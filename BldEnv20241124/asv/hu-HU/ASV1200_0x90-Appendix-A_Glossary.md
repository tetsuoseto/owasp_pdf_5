# Függelék A: Szójegyzék

> Ez az átfogó szószedet meghatározásokat nyújt az AISVS-ben használt kulcsfontosságú MI, gépi tanulás és biztonsági kifejezésekhez, hogy biztosítsa a világosságot és a közös megértést.
> ​
* Adverzáriális példa: Egy bemenet, amelyet szándékosan úgy alakítanak ki, hogy egy MI modell hibát kövessen el, gyakran apró, az emberek számára észrevehetetlen zavaró hatások hozzáadásával.
  ​
* Adverzáriális robusztusság – Az adverzáriális robusztusság az MI-ben arra a képességre utal, hogy egy modell képes megőrizni teljesítményét és ellenállni az olyan szándékosan előállított, rosszindulatú bemenetek által okozott megtévesztésnek vagy manipulációnak, amelyek hibákat okoznak.
  ​
* Ügynök – Az AI ügynökök olyan szoftverrendszerek, amelyek mesterséges intelligenciát használnak a felhasználók nevében történő célkitűzések elérésére és feladatok elvégzésére. Képesek érvelésre, tervezésre és emlékezetre, valamint autonómia szinttel rendelkeznek a döntéshozatal, tanulás és alkalmazkodás terén.
  ​
* Agentikus mesterséges intelligencia: Olyan MI rendszerek, amelyek bizonyos fokú autonómiával működhetnek céljaik elérése érdekében, gyakran emberi beavatkozás nélkül hoznak döntéseket és hajtanak végre műveleteket.
  ​
* Attribútum alapú hozzáférés-vezérlés (ABAC): Egy hozzáférés-vezérlési paradigma, ahol az engedélyezési döntések a felhasználó, az erőforrás, a művelet és a környezet attribútumain alapulnak, amelyek a lekérdezés idején kerülnek értékelésre.
  ​
* Hátsó kapu támadás: Egy olyan adatmegfertőzési támadás, amely során a modellt úgy tanítják, hogy bizonyos kiváltó jelekre meghatározott módon reagáljon, miközben egyébként normálisan működik.
  ​
* Elfogultság: Szisztematikus hibák az MI modell kimeneteiben, amelyek igazságtalan vagy diszkriminatív eredményekhez vezethetnek bizonyos csoportok vagy specifikus kontextusok esetén.
  ​
* Elfogultság kihasználása: Egy támadási technika, amely kihasználja az AI modellek ismert elfogultságait az eredmények vagy kimenetek manipulálására.
  ​
* Cedar: Az Amazon szabályzati nyelve és motorja finomhangolt jogosultságokhoz, amelyet ABAC (attribútumalapú hozzáférés-vezérlés) megvalósításához használnak mesterséges intelligencia rendszerekben.
  ​
* Gondolatmenet: Egy olyan technika, amely a nyelvi modellek érvelésének javítására szolgál azáltal, hogy közbenső érvelési lépéseket generál a végső válasz megadása előtt.
  ​
* Áramkörmegszakítók: Olyan mechanizmusok, amelyek automatikusan leállítják a mesterséges intelligencia rendszer működését, amikor a meghatározott kockázati küszöbértékeket átlépik.
  ​
* Adatszivárgás: érzékeny információk nem szándékolt kiszivárogtatása AI modell kimenetei vagy viselkedése révén.
  ​
* Adat mérgezés: A tanító adatok szándékos megrongálása a modell integritásának veszélyeztetése érdekében, gyakran hátuljáratok telepítésére vagy a teljesítmény csökkentésére.
  ​
* Differenciális adatvédelem – A differenciális adatvédelem egy matematikailag szigorú keretrendszer statisztikai információk közzétételére adatállományokról, miközben védi az egyéni adattulajdonosok magánszféráját. Lehetővé teszi az adatszolgáltató számára, hogy megossza a csoport összesített mintázatait, miközben korlátozza az adott egyénekre vonatkozó információk kiszivárgását.
  ​
* Beágyazások: Adatok (szöveg, képek stb.) sűrű vektoros ábrázolásai, amelyek szemantikai jelentést ragadnak meg egy magas dimenziójú térben.
  ​
* Magyarázhatóság – Az AI magyarázhatósága annak a képessége, hogy egy mesterséges intelligencia rendszer ember által érthető okokat szolgáltasson döntéseihez és előrejelzéseihez, betekintést nyújtva belső működésébe.
  ​
* Magyarázható mesterséges intelligencia (XAI): Olyan MI rendszerek, amelyeket úgy terveztek, hogy különféle technikák és keretrendszerek segítségével ember számára érthető magyarázatokat nyújtsanak döntéseikről és viselkedésükről.
  ​
* Federált tanulás: Egy gépi tanulási megközelítés, ahol a modelleket több decentralizált eszközön képezik helyi adatminták felhasználásával, az adatok tényleges cseréje nélkül.
  ​
* Védőkorlátok: Olyan korlátozások, amelyeket az AI rendszerek által káros, elfogult vagy egyébként nem kívánatos kimenetek előállításának megelőzésére vezetnek be.
  ​
* Hallucináció – Az AI hallucináció olyan jelenségre utal, amikor egy mesterséges intelligencia modell hibás vagy félrevezető információt állít elő, amely nem alapul a tanítási adataiban vagy a tényleges valóságban.
  ​
* Ember a ciklusban (HITL): Olyan rendszerek, amelyeket emberi felügyelet, ellenőrzés vagy beavatkozás igényére terveztek kulcsfontosságú döntési pontokon.
  ​
* Infrastruktúra kódként (IaC): az infrastruktúra kezelése és előállítása kódon keresztül, manuális folyamatok helyett, lehetővé téve a biztonsági vizsgálatot és az egységes telepítéseket.
  ​
* Jailbreak: Azok a technikák, amelyeket az AI rendszerek, különösen a nagyméretű nyelvi modellek biztonsági korlátainak megkerülésére használnak tiltott tartalom előállítása érdekében.
  ​
* Legkisebb jogosultság elve: Az a biztonsági elv, amely szerint a felhasználóknak és folyamatoknak csak a minimálisan szükséges hozzáférési jogokat kell megadni.
  ​
* LIME (Helyi Értelmezhető Modellfüggetlen Magyarázatok): Egy olyan technika, amely bármely gépi tanulási osztályozó előrejelzéseit magyarázza meg, azáltal hogy helyileg egy értelmezhető modellel közelíti azt.
  ​
* Tagsági következtetési támadás: Olyan támadás, amelynek célja annak meghatározása, hogy egy adott adatpontot felhasználtak-e egy gépi tanulási modell betanításához.
  ​
* MITRE ATLAS: Ellenséges fenyegetési körkép mesterséges intelligencia rendszerekhez; egy tudásbázis az AI rendszerek elleni ellenséges taktikákról és technikákról.
  ​
* Modellkártya – A modellkártya egy olyan dokumentum, amely szabványosított információkat nyújt egy MI modell teljesítményéről, korlátairól, tervezett felhasználásáról és etikai megfontolásairól, elősegítve ezzel az átláthatóságot és a felelős MI fejlesztést.
  ​
* Modellkivonás: Olyan támadás, amely során a támadó ismételten lekérdez egy céltárgy modellből, hogy engedély nélkül létrehozzon egy funkcionálisan hasonló másolatot.
  ​
* Modellinverzió: Egy támadás, amely a modell kimeneteinek elemzésével próbálja rekonstruálni a tanító adatokat.
  ​
* Modell életciklus kezelése – Az AI modell életciklus kezelése az AI modell létezésének minden szakaszának felügyeletét jelenti, beleértve a tervezést, fejlesztést, alkalmazást, monitorozást, karbantartást és végső nyugdíjazást, annak érdekében, hogy a modell hatékony maradjon és összhangban legyen a kitűzött célokkal.
  ​
* Modelmérgezés: Sérülékenységek vagy hátsóajtók közvetlen bevezetése egy modellbe a tanulási folyamat során.
  ​
* Modelllopás/Tolvajlás: Egy tulajdonosi modell másolatának vagy közelítésének kinyerése ismételt lekérdezések révén.
  ​
* Többügynökös rendszer: Olyan rendszer, amely több, egymással kölcsönhatásban álló mesterséges intelligencia ügynökből áll, amelyek egyenként eltérő képességekkel és célokkal rendelkezhetnek.
  ​
* OPA (Open Policy Agent): Egy nyílt forráskódú szabályozási motor, amely egységes szabályozás végrehajtást tesz lehetővé az egész rendszerben.
  ​
* Adatvédelmet biztosító gépi tanulás (PPML): Módszerek és technikák a gépi tanulási modellek képzésére és alkalmazására úgy, hogy közben megóvják a képzési adatok magánéletét.
  ​
* Promptinjektálás: Egy támadási mód, ahol rosszindulatú utasítások kerülnek beágyazásra a bemenetekbe, hogy felülírják a modell eredetileg szánt viselkedését.
  ​
* RAG (Lekérdezés-Alapú Generálás): Egy olyan technika, amely a nagy nyelvi modelleket úgy fejleszti, hogy a válasz generálása előtt releváns információkat szerez külső tudásforrásokból.
  ​
* Red-Teaming: Az AI rendszerek aktív tesztelésének gyakorlata, amely során ellenséges támadásokat szimulálnak a sebezhetőségek azonosítása érdekében.
  ​
* SBOM (Szoftver-összetevők jegyzéke): Egy hivatalos nyilvántartás, amely tartalmazza a szoftver vagy AI modellek készítéséhez használt különböző összetevők részleteit és ellátási lánc kapcsolatait.
  ​
* SHAP (SHapley Additive exPlanations): Egy játékelméleti megközelítés a gépi tanulási modell bármely kimenetének magyarázatára, amely a predikcióhoz való hozzájárulását számolja ki az egyes jellemzőknek.
  ​
* Ellátási lánc támadás: Egy rendszer kompromittálása az ellátási lánc kevésbé biztonságos elemeinek célozásával, például harmadik fél által biztosított könyvtárak, adatállományok vagy előre betanított modellek révén.
  ​
* Átviteli tanulás: Egy olyan technika, ahol egy adott feladatra fejlesztett modellt újrahasznosítanak kiindulópontként egy második feladathoz tartozó modell esetében.
  ​
* Vektoradatbázis: Olyan speciális adatbázis, amely magasdimenziós vektorok (beágyazások) tárolására és hatékony hasonlósági keresések végrehajtására szolgál.
  ​
* Sebezhetőség-vizsgálat: Automatikus eszközök, amelyek azonosítják a szoftverkomponensek, beleértve a mesterséges intelligencia keretrendszereket és függőségeket, ismert biztonsági sebezhetőségeit.
  ​
* Vízjelzés: Olyan technikák, amelyek segítségével érzékelhetetlen jeleket ágyaznak be AI által generált tartalmakba az eredetük nyomon követése vagy az AI általi generálás felismerése érdekében.
  ​
* Zero-Day sérülékenység: Egy korábban ismeretlen sérülékenység, amelyet a támadók kihasználhatnak, mielőtt a fejlesztők javítást készítenének és telepítenének.

