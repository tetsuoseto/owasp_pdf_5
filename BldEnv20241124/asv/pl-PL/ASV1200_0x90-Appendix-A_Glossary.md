# Aneks A: Słownik terminów

> Ten obszerny glosariusz zawiera definicje kluczowych terminów związanych z AI, ML i bezpieczeństwem używanych w całym AISVS, aby zapewnić jasność i wspólne zrozumienie.
> ​
* Przykład adwersarialny: Dane wejściowe celowo stworzone w celu wywołania błędu w modelu sztucznej inteligencji, często poprzez dodanie subtelnych zakłóceń niewidocznych dla ludzi.
  ​
* Odporność na ataki adwersarialne – Odporność na ataki adwersarialne w sztucznej inteligencji odnosi się do zdolności modelu do utrzymania swojej wydajności oraz odporności na bycie oszukanym lub zmanipulowanym przez celowo zaprojektowane, złośliwe dane wejściowe mające na celu wywołanie błędów.
  ​
* Agent – Agenci AI to systemy programowe wykorzystujące sztuczną inteligencję do realizacji celów i wykonywania zadań w imieniu użytkowników. Wykazują się zdolnościami rozumowania, planowania i pamięci oraz posiadają poziom autonomii pozwalający na podejmowanie decyzji, uczenie się i adaptację.
  ​
* Sztuczna inteligencja agentowa: systemy AI, które mogą działać z pewnym stopniem autonomii w celu realizacji celów, często podejmując decyzje i podejmując działania bez bezpośredniej interwencji człowieka.
  ​
* Kontrola dostępu oparta na atrybutach (ABAC): paradygmat kontroli dostępu, w którym decyzje dotyczące autoryzacji są podejmowane na podstawie atrybutów użytkownika, zasobu, akcji i środowiska, ocenianych w momencie zapytania.
  ​
* Atak tylnego wejścia (Backdoor Attack): Rodzaj ataku zatruwania danych, w którym model jest trenowany, aby reagować w określony sposób na konkretne wyzwalacze, zachowując się normalnie w innych przypadkach.
  ​
* Stronniczość: Systematyczne błędy w wynikach modeli AI, które mogą prowadzić do niesprawiedliwych lub dyskryminujących rezultatów dla niektórych grup lub w określonych kontekstach.
  ​
* Wykorzystanie uprzedzeń: technika ataku wykorzystująca znane uprzedzenia w modelach sztucznej inteligencji do manipulowania wynikami lub efektami.
  ​
* Cedar: język polityki i silnik Amazona do precyzyjnego zarządzania uprawnieniami, używany do wdrażania ABAC w systemach sztucznej inteligencji.
  ​
* Łańcuch myśli: technika poprawiająca rozumowanie w modelach językowych poprzez generowanie pośrednich kroków rozumowania przed wygenerowaniem ostatecznej odpowiedzi.
  ​
* Wyłączniki awaryjne: Mechanizmy, które automatycznie zatrzymują działanie systemu AI, gdy przekroczone zostaną określone progi ryzyka.
  ​
* Wycieki danych: Niezamierzone ujawnienie poufnych informacji za pomocą wyników lub zachowania modelu AI.
  ​
* Zatrucie danych: celowe uszkodzenie danych treningowych w celu naruszenia integralności modelu, często w celu wprowadzenia tylnego wejścia lub pogorszenia wydajności.
  ​
* Prywatność różnicowa – Prywatność różnicowa to matematycznie rygorystyczne ramy umożliwiające udostępnianie informacji statystycznych o zbiorach danych przy jednoczesnej ochronie prywatności poszczególnych osób. Pozwala ona posiadaczowi danych na udostępnianie zbiorczych wzorców grupy przy ograniczaniu informacji ujawnianych o konkretnych jednostkach.
  ​
* Osadzenia: Gęste reprezentacje wektorowe danych (tekstów, obrazów itp.), które uchwytują znaczenie semantyczne w przestrzeni o wysokim wymiarze.
  ​
* Wyjaśnialność – Wyjaśnialność w AI to zdolność systemu sztucznej inteligencji do dostarczania zrozumiałych dla człowieka powodów swoich decyzji i prognoz, oferując wgląd w jego wewnętrzne działanie.
  ​
* Wyjaśnialna sztuczna inteligencja (XAI): systemy AI zaprojektowane w celu dostarczania zrozumiałych dla człowieka wyjaśnień swoich decyzji i zachowań za pomocą różnych technik i ram.
  ​
* Uczenie federacyjne: podejście do uczenia maszynowego, w którym modele są trenowane na wielu zdecentralizowanych urządzeniach posiadających lokalne próbki danych, bez wymiany samych danych.
  ​
* Ograniczenia: ograniczenia wprowadzone w celu zapobiegania generowaniu przez systemy AI szkodliwych, stronniczych lub w inny sposób niepożądanych wyników.
  ​
* Halucynacja – Halucynacja AI odnosi się do zjawiska, w którym model AI generuje nieprawidłowe lub wprowadzające w błąd informacje, które nie są oparte na jego danych treningowych ani na rzeczywistości faktograficznej.
  ​
* Human-in-the-Loop (HITL): Systemy zaprojektowane tak, aby wymagały nadzoru, weryfikacji lub interwencji człowieka w kluczowych punktach decyzyjnych.
  ​
* Infrastruktura jako Kod (IaC): Zarządzanie i dostarczanie infrastruktury za pomocą kodu zamiast procesów ręcznych, co umożliwia skanowanie pod kątem bezpieczeństwa i spójne wdrożenia.
  ​
* Jailbreak: Techniki stosowane do obchodzenia zabezpieczeń w systemach sztucznej inteligencji, szczególnie w dużych modelach językowych, w celu tworzenia zabronionych treści.
  ​
* Najmniejsze uprawnienia: zasada bezpieczeństwa polegająca na przyznawaniu użytkownikom i procesom tylko minimalnych niezbędnych praw dostępu.
  ​
* LIME (Lokalne Interpretowalne Wyjaśnienia Niezależne od Modelu): Technika wyjaśniająca przewidywania dowolnego klasyfikatora uczenia maszynowego poprzez lokalne przybliżenie go modelem interpretowalnym.
  ​
* Atak wnioskowania członkostwa: atak mający na celu ustalenie, czy konkretny punkt danych został użyty do trenowania modelu uczenia maszynowego.
  ​
* MITRE ATLAS: Krajobraz zagrożeń przeciwnika dla systemów sztucznej inteligencji; baza wiedzy dotycząca taktyk i technik ataku na systemy AI.
  ​
* Karta modelu – Karta modelu to dokument zawierający ustandaryzowane informacje o wydajności modelu AI, jego ograniczeniach, przeznaczeniu oraz aspektach etycznych, mające na celu promowanie przejrzystości i odpowiedzialnego rozwoju AI.
  ​
* Model Extraction: Atak, w którym przeciwnik wielokrotnie zapytuje docelowy model, aby stworzyć funkcjonalnie podobną kopię bez autoryzacji.
  ​
* Inwersja modelu: atak, który próbuje odtworzyć dane treningowe poprzez analizę wyników modelu.
  ​
* Zarządzanie cyklem życia modelu – Zarządzanie cyklem życia modelu AI to proces nadzorowania wszystkich etapów istnienia modelu AI, w tym jego projektowania, rozwoju, wdrażania, monitorowania, utrzymania oraz ostatecznego wycofania, aby zapewnić jego skuteczność i zgodność z celami.
  ​
* Zatrucie modelu: Wprowadzanie luk bezpieczeństwa lub tylnego wejścia bezpośrednio do modelu podczas procesu treningowego.
  ​
* Kradzież modelu: Uzyskanie kopii lub przybliżenia modelu własnościowego poprzez wielokrotne zapytania.
  ​
* System wieloagentowy: system składający się z wielu współdziałających agentów AI, z których każdy może mieć różne możliwości i cele.
  ​
* OPA (Open Policy Agent): Otwartoźródłowy silnik polityk, który umożliwia jednolite egzekwowanie polityk w całym stosie.
  ​
* Uczące się maszyny zachowujące prywatność (PPML): Techniki i metody trenowania oraz wdrażania modeli ML, jednocześnie chroniące prywatność danych treningowych.
  ​
* Wstrzyknięcie promptu: Atak, w którym złośliwe instrukcje są osadzane w danych wejściowych w celu zastąpienia zamierzonego zachowania modelu.
  ​
* RAG (Generacja wspomagana wyszukiwaniem): technika, która usprawnia duże modele językowe poprzez pobieranie istotnych informacji z zewnętrznych źródeł wiedzy przed wygenerowaniem odpowiedzi.
  ​
* Red-Teaming: Praktyka aktywnego testowania systemów AI poprzez symulowanie ataków przeciwnika w celu identyfikacji podatności.
  ​
* SBOM (Lista Materiałowa Oprogramowania): Formalny zapis zawierający szczegóły oraz relacje w łańcuchu dostaw różnych komponentów używanych do tworzenia oprogramowania lub modeli AI.
  ​
* SHAP (SHapley Additive exPlanations): Podejście oparte na teorii gier do wyjaśniania wyniku dowolnego modelu uczenia maszynowego poprzez obliczenie wkładu każdej cechy w prognozę.
  ​
* Atak na łańcuch dostaw: Kompromitacja systemu poprzez celowanie w mniej zabezpieczone elementy jego łańcucha dostaw, takie jak biblioteki zewnętrzne, zestawy danych lub modele wstępnie wytrenowane.
  ​
* Uczenie transferowe: technika, w której model opracowany do jednego zadania jest ponownie wykorzystywany jako punkt wyjścia dla modelu do zadania drugiego.
  ​
* Baza danych wektorowych: Specjalistyczna baza danych zaprojektowana do przechowywania wysokowymiarowych wektorów (osadzeń) i wykonywania efektywnych wyszukiwań podobieństwa.
  ​
* Skanowanie podatności: Automatyczne narzędzia identyfikujące znane luki bezpieczeństwa w komponentach oprogramowania, w tym w ramach AI i zależnościach.
  ​
* Znakowanie wodne: Techniki osadzania nieczytelnych znaczników w treściach generowanych przez SI w celu śledzenia ich pochodzenia lub wykrywania generacji przez SI.
  ​
* Luka zero-day: wcześniej nieznana luka, którą atakujący mogą wykorzystać, zanim deweloperzy stworzą i wdrożą poprawkę.

