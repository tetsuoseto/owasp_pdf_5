# Приложение A: Глоссарий

> Этот всеобъемлющий глоссарий содержит определения ключевых терминов в области ИИ, МО и безопасности, используемых в AISVS, чтобы обеспечить ясность и общее понимание.
> ​
* Противоречивый пример: входные данные, специально созданные для того, чтобы вызвать ошибку модели ИИ, часто путем добавления тонких возмущений, незаметных для человека.
  ​
* Адвесариальная устойчивость – адвесариальная устойчивость в ИИ относится к способности модели сохранять свою производительность и противостоять обману или манипуляциям с помощью специально созданных вредоносных входных данных, предназначенных для вызова ошибок.
  ​
* Агент – AI-агенты — это программные системы, использующие искусственный интеллект для достижения целей и выполнения задач от имени пользователей. Они демонстрируют умозаключения, планирование и память, а также обладают уровнем автономии для принятия решений, обучения и адаптации.
  ​
* Агентный ИИ: системы искусственного интеллекта, которые могут работать с некоторой степенью автономности для достижения целей, часто принимая решения и предпринимая действия без прямого вмешательства человека.
  ​
* Контроль доступа на основе атрибутов (ABAC): парадигма контроля доступа, при которой решения об авторизации принимаются на основе атрибутов пользователя, ресурса, действия и окружающей среды, оцениваемых во время запроса.
  ​
* Атака с задней дверью: тип атаки с отравлением данных, при котором модель обучается реагировать определённым образом на определённые триггеры, при этом в остальном ведя себя нормально.
  ​
* Смещение: Систематические ошибки в результатах работы моделей ИИ, которые могут приводить к несправедливым или дискриминационным последствиям для определённых групп или в конкретных контекстах.
  ​
* Эксплуатация смещений: техника атаки, использующая известные смещения в моделях ИИ для манипулирования результатами или исходами.
  ​
* Cedar: язык политики и механизм Amazon для точных разрешений, используемых при реализации ABAC для ИИ-систем.
  ​
* Цепочка рассуждений: метод улучшения логического вывода в языковых моделях путем генерации промежуточных этапов рассуждения перед получением окончательного ответа.
  ​
* Автоматические отключатели: механизмы, которые автоматически останавливают работу ИИ-системы при превышении определённых порогов риска.
  ​
* Утечка данных: непреднамеренное раскрытие конфиденциальной информации через выходные данные или поведение модели ИИ.
  ​
* Отравление данных: намеренное искажение обучающих данных с целью компрометации целостности модели, часто для внедрения скрытых уязвимостей или ухудшения производительности.
  ​
* Дифференциальная приватность – это математически строгая концепция для публикации статистической информации о наборах данных с защитой конфиденциальности отдельных субъектов данных. Она позволяет владельцу данных делиться агрегированными паттернами группы, ограничивая утечку информации о конкретных индивидах.
  ​
* Встраивания: плотные векторные представления данных (текста, изображений и т.д.), которые захватывают семантическое значение в многомерном пространстве.
  ​
* Объяснимость – Объяснимость в ИИ — это способность системы искусственного интеллекта предоставлять понятные для человека причины своих решений и прогнозов, обеспечивая понимание её внутреннего функционирования.
  ​
* Объяснимая ИИ (XAI): системы искусственного интеллекта, разработанные для предоставления человекопонятных объяснений своих решений и поведения с помощью различных методов и архитектур.
  ​
* Федеративное обучение: подход к машинному обучению, при котором модели обучаются на нескольких децентрализованных устройствах, содержащих локальные данные, без обмена самими данными.
  ​
* Стратегии ограничения: Ограничения, внедренные для предотвращения генерации системами ИИ вредоносных, предвзятых или иным образом нежелательных результатов.
  ​
* Галлюцинация — галлюцинация ИИ относится к явлению, при котором модель ИИ генерирует неверную или вводящую в заблуждение информацию, которая не основана на обучающих данных или фактической реальности.
  ​
* Человек в цикле (Human-in-the-Loop, HITL): системы, предназначенные для обеспечения контроля, проверки или вмешательства человека в ключевых точках принятия решений.
  ​
* Инфраструктура как код (IaC): управление и предоставление инфраструктуры с помощью кода вместо ручных процессов, что позволяет проводить сканирование безопасности и обеспечивать последовательные развертывания.
  ​
* Джейлбрейк: методы, используемые для обхода защитных ограничений в системах ИИ, особенно в больших языковых моделях, с целью создания запрещенного контента.
  ​
* Минимальные привилегии: принцип безопасности, предусматривающий предоставление пользователям и процессам только минимально необходимых прав доступа.
  ​
* LIME (Локальные интерпретируемые объяснения модели-независимые): метод объяснения предсказаний любого классификатора машинного обучения путем локального приближения его интерпретируемой моделью.
  ​
* Атака на определение членства: атака, направленная на установление того, использовалась ли конкретная точка данных для обучения модели машинного обучения.
  ​
* MITRE ATLAS: Ландшафт враждебных угроз для систем искусственного интеллекта; база знаний враждебных тактик и техник против систем ИИ.
  ​
* Карточка модели — это документ, который предоставляет стандартизированную информацию о производительности модели искусственного интеллекта, её ограничениях, предполагаемых областях применения и этических аспектах с целью продвижения прозрачности и ответственной разработки ИИ.
  ​
* Извлечение модели: атака, при которой злоумышленник неоднократно запрашивает целевую модель для создания функционально аналогичной копии без разрешения.
  ​
* Инверсия модели: атака, направленная на восстановление обучающих данных путем анализа выходных данных модели.
  ​
* Управление жизненным циклом модели – управление жизненным циклом ИИ-модели представляет собой процесс контроля всех этапов существования модели ИИ, включая её проектирование, разработку, развертывание, мониторинг, техническое обслуживание и окончательный вывод из эксплуатации, чтобы обеспечить её эффективность и соответствие поставленным целям.
  ​
* Отравление модели: Введение уязвимостей или задних дверей непосредственно в модель во время процесса обучения.
  ​
* Кража модели: получение копии или приближенного варианта проприетарной модели путем многократных запросов.
  ​
* Мультиагентная система: система, состоящая из нескольких взаимодействующих ИИ-агентов, каждый из которых может обладать различными возможностями и целями.
  ​
* OPA (Open Policy Agent): Открытый движок политик, который обеспечивает единое применение политик во всем стеке.
  ​
* Машинное обучение с сохранением конфиденциальности (PPML): методы и техники для обучения и развертывания моделей машинного обучения с защитой конфиденциальности обучающих данных.
  ​
* Внедрение в запрос: атака, при которой вредоносные инструкции внедряются в вводимые данные для переопределения заданного поведения модели.
  ​
* RAG (Генерация с поддержкой извлечения): метод, который улучшает большие языковые модели за счет извлечения релевантной информации из внешних источников знаний перед генерацией ответа.
  ​
* Ред-тиминг: практика активного тестирования ИИ-систем путем имитации враждебных атак с целью выявления уязвимостей.
  ​
* SBOM (Перечень компонентов программного обеспечения): Формальная запись, содержащая детали и отношения цепочки поставок различных компонентов, используемых при создании программного обеспечения или моделей ИИ.
  ​
* SHAP (SHapley Additive exPlanations): Теоретический подход из теории игр для объяснения вывода любой модели машинного обучения путем вычисления вклада каждого признака в предсказание.
  ​
* Атака на цепочку поставок: компрометация системы за счет нацеливания на менее защищенные элементы в ее цепочке поставок, такие как сторонние библиотеки, наборы данных или предобученные модели.
  ​
* Трансферное обучение: метод, при котором модель, разработанная для одной задачи, используется в качестве отправной точки для модели по другой задаче.
  ​
* Векторная база данных: специализированная база данных, предназначенная для хранения высокоразмерных векторов (встраиваний) и выполнения эффективного поиска по сходству.
  ​
* Сканирование уязвимостей: Автоматизированные инструменты, которые выявляют известные уязвимости безопасности в программных компонентах, включая фреймворки ИИ и зависимости.
  ​
* Водяные знаки: методы внедрения незаметных маркеров в контент, созданный искусственным интеллектом, для отслеживания его происхождения или выявления создания с помощью ИИ.
  ​
* Уязвимость нулевого дня: ранее неизвестная уязвимость, которую злоумышленники могут использовать до того, как разработчики создадут и выпустят исправление.

