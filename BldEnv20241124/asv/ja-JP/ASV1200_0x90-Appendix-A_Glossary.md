# 付録A：用語集

> この包括的な用語集は、AISVS全体で使用される主要なAI、機械学習（ML）、およびセキュリティ用語の定義を提供し、明確性と共通理解を確保します。
> ​
* 敵対的サンプル：人間には知覚できない微細な摂動を加えることで、AIモデルに誤りを起こさせるよう意図的に作成された入力。
  ​
* 敵対的堅牢性 – AIにおける敵対的堅牢性とは、モデルがその性能を維持し、意図的に作成された悪意ある入力によって誤動作や誤りを引き起こされるのを防ぐ能力を指します。
  ​
* エージェント – AIエージェントは、ユーザーの代わりに目標を追求しタスクを完了するためにAIを利用するソフトウェアシステムです。これらは推論、計画、記憶を示し、意思決定、学習、適応を行う一定の自律性を持っています。
  ​
* エージェンティックAI：目標達成のためにある程度自律的に動作できるAIシステムであり、多くの場合、直接的な人間の介入なしに意思決定や行動を行う。
  ​
* 属性ベースアクセス制御（ABAC）：ユーザー、リソース、アクション、および環境の属性に基づいて認可判断が行われ、クエリ時に評価されるアクセス制御のパラダイム。
  ​
* バックドア攻撃：モデルが特定のトリガーに対して特定の反応をするように訓練され、それ以外の場合は通常通りに動作するデータポイズニング攻撃の一種。
  ​
* バイアス：特定のグループや特定の文脈において不公平または差別的な結果をもたらす可能性のある、AIモデルの出力における系統的な誤り。
  ​
* バイアス悪用：AIモデルに存在する既知のバイアスを利用して、出力や結果を操作する攻撃手法。
  ​
* Cedar：AIシステムのABAC実装に使用される、細粒度の権限のためのAmazonのポリシー言語およびエンジン。
  ​
* 思考の連鎖（Chain of Thought）：最終的な回答を出す前に中間的な推論ステップを生成することで、言語モデルの推論能力を向上させる手法。
  ​
* サーキットブレーカー：特定のリスクしきい値を超えた場合に、AIシステムの動作を自動的に停止するメカニズム。
  ​
* データ漏洩：AIモデルの出力や挙動を通じて機密情報が意図せずに公開されること。
  ​
* データポイズニング：モデルの完全性を損なうために訓練データを意図的に改ざんすることで、バックドアを仕込んだり、性能を低下させたりする行為。
  ​
* 差分プライバシー – 差分プライバシーは、個々のデータ主体のプライバシーを保護しながら、データセットに関する統計情報を公開するための数学的に厳密な枠組みです。これにより、データ保持者は特定の個人に関する情報漏洩を制限しつつ、グループの集約的なパターンを共有することが可能になります。
  ​
* 埋め込み（Embeddings）：データ（テキスト、画像など）の意味を高次元空間で捉える密ベクトル表現。
  ​
* 説明可能性 — AIにおける説明可能性とは、AIシステムがその決定や予測に対して人間が理解できる理由を提供し、内部の動作に関する洞察を示す能力のことです。
  ​
* 説明可能なAI（XAI）：その決定や行動について、人間が理解できる説明を様々な技術やフレームワークを通じて提供するように設計されたAIシステム。
  ​
* フェデレーテッドラーニング：モデルがデータ自体を交換することなく、複数の分散したデバイス上でローカルデータサンプルを保持したまま学習される機械学習の手法。
  ​
* ガードレール：AIシステムが有害、偏った、またはその他望ましくない出力を生成するのを防ぐために実装される制約。
  ​
* 幻覚（ハルシネーション）とは、AIモデルがトレーニングデータや事実に基づかない誤った情報や誤解を招く情報を生成する現象を指します。
  ​
* ヒューマン・イン・ザ・ループ（HITL）：重要な意思決定のポイントで人間による監視、検証、または介入を必要とするように設計されたシステム。
  ​
* インフラストラクチャ as Code (IaC): 手動プロセスの代わりにコードを通じてインフラストラクチャを管理およびプロビジョニングし、セキュリティスキャンと一貫したデプロイメントを可能にすること。
  ​
* 脱獄（ジェイルブレイク）：AIシステム、特に大規模言語モデルにおいて、安全ガードレールを回避し、禁止されたコンテンツを生成するために使用される技術。
  ​
* 最小権限: ユーザーおよびプロセスに対して必要最小限のアクセス権のみを付与するセキュリティ原則。
  ​
* LIME（局所的解釈可能なモデル非依存の説明）：任意の機械学習分類器の予測を解釈可能なモデルで局所的に近似することで説明する手法。
  ​
* メンバーシップ推論攻撃：特定のデータポイントが機械学習モデルの訓練に使用されたかどうかを判別することを目的とした攻撃。
  ​
* MITRE ATLAS：人工知能システムに対する敵対的脅威の全景。AIシステムに対する敵対的戦術と技術の知識ベース。
  ​
* モデルカード – モデルカードとは、AIモデルの性能、制限、意図された使用用途、および倫理的考慮事項に関する標準化された情報を提供し、透明性と責任あるAI開発を促進するための文書です。
  ​
* モデル抽出：攻撃者が対象モデルに繰り返し問い合わせを行い、許可なく機能的に類似したコピーを作成する攻撃。
  ​
* モデル反転攻撃：モデルの出力を分析することでトレーニングデータの再構築を試みる攻撃。
  ​
* モデルライフサイクル管理 – AIモデルライフサイクル管理は、AIモデルの設計、開発、展開、監視、保守、そして最終的な廃止に至るまでのすべての段階を監督し、モデルが効果的で目的に合致していることを確保するプロセスです。
  ​
* モデルポイズニング：トレーニングプロセス中にモデルに脆弱性やバックドアを直接導入すること。
  ​
* モデル窃盗/盗用：繰り返しのクエリを通じて、独自モデルのコピーまたは近似を抽出すること。
  ​
* マルチエージェントシステム：異なる能力や目標を持つ複数の相互作用するAIエージェントで構成されるシステム。
  ​
* OPA（Open Policy Agent）：スタック全体で統一されたポリシーの適用を可能にするオープンソースのポリシーエンジン。
  ​
* プライバシー保護機械学習（PPML）：トレーニングデータのプライバシーを保護しながら、機械学習モデルをトレーニングおよび展開するための技術と方法。
  ​
* プロンプトインジェクション：モデルの本来の動作を上書きするために悪意のある命令が入力に埋め込まれる攻撃。
  ​
* RAG（検索補強生成）：生成する前に外部の知識源から関連情報を検索して大型言語モデルの応答を強化する技術。
  ​
* レッドチーミング：脆弱性を特定するために、敵対的攻撃を模擬してAIシステムを積極的にテストする手法。
  ​
* SBOM（ソフトウェア部品表）：ソフトウェアやAIモデルの構築に使用されるさまざまなコンポーネントの詳細とサプライチェーン関係を含む正式な記録。
  ​
* SHAP（SHapley Additive exPlanations）：各特徴量が予測にどの程度寄与しているかを計算することで、あらゆる機械学習モデルの出力を説明するゲーム理論に基づく手法。
  ​
* サプライチェーン攻撃：サードパーティのライブラリ、データセット、または事前学習済みモデルなど、供給チェーン内のセキュリティが低い要素を標的にしてシステムを侵害すること。
  ​
* 転移学習：あるタスクのために開発されたモデルを、別のタスクのモデルの出発点として再利用する技術。
  ​
* ベクターデータベース：高次元ベクトル（埋め込み）を格納し、効率的な類似検索を実行するために設計された専門データベース。
  ​
* 脆弱性スキャン：AIフレームワークや依存関係を含むソフトウェアコンポーネントの既知のセキュリティ脆弱性を特定する自動化ツール。
  ​
* ウォーターマーキング：AI生成コンテンツの起源を追跡したりAI生成を検出したりするために、知覚できないマーカーを埋め込む技術。
  ​
* ゼロデイ脆弱性：開発者がパッチを作成・展開する前に攻撃者が悪用可能な、これまでに知られていなかった脆弱性。

