# Appendice A: Glossario

> Questo glossario completo fornisce definizioni dei termini chiave di AI, ML e sicurezza utilizzati in tutto l'AISVS per garantire chiarezza e comprensione comune.
> ​
* Esempio avversario: un input appositamente creato per indurre un modello di intelligenza artificiale a commettere un errore, spesso aggiungendo perturbazioni sottili impercettibili per gli esseri umani.
  ​
* Robustezza avversaria – La robustezza avversaria nell'IA si riferisce alla capacità di un modello di mantenere le proprie prestazioni e resistere a essere ingannato o manipolato da input maligni appositamente creati per causare errori.
  ​
* Agente – Gli agenti AI sono sistemi software che utilizzano l'intelligenza artificiale per perseguire obiettivi e completare compiti per conto degli utenti. Dimostrano capacità di ragionamento, pianificazione e memoria, e possiedono un livello di autonomia per prendere decisioni, apprendere e adattarsi.
  ​
* AI agentico: Sistemi di intelligenza artificiale che possono operare con un certo grado di autonomia per raggiungere obiettivi, spesso prendendo decisioni e agendo senza intervento umano diretto.
  ​
* Controllo degli Accessi Basato su Attributi (ABAC): un paradigma di controllo degli accessi in cui le decisioni di autorizzazione si basano sugli attributi dell'utente, della risorsa, dell'azione e dell'ambiente, valutati al momento della richiesta.
  ​
* Attacco Backdoor: Un tipo di attacco di avvelenamento dei dati in cui il modello viene addestrato a rispondere in un modo specifico a determinati trigger mentre si comporta normalmente altrimenti.
  ​
* Bias: Errori sistematici nei risultati dei modelli di IA che possono portare a risultati ingiusti o discriminatori per determinati gruppi o in contesti specifici.
  ​
* Sfruttamento dei bias: una tecnica di attacco che sfrutta i bias noti nei modelli di IA per manipolare risultati o esiti.
  ​
* Cedar: il linguaggio e motore di policy di Amazon per permessi dettagliati utilizzati nell'implementazione di ABAC per sistemi di intelligenza artificiale.
  ​
* Catena di Pensiero: una tecnica per migliorare il ragionamento nei modelli linguistici generando passaggi intermedi di ragionamento prima di produrre una risposta finale.
  ​
* Interruttori automatici: meccanismi che interrompono automaticamente le operazioni del sistema AI quando vengono superate soglie di rischio specifiche.
  ​
* Perdita di dati: esposizione non intenzionale di informazioni sensibili attraverso output o comportamenti del modello di IA.
  ​
* Avvelenamento dei dati: La corruzione deliberata dei dati di addestramento per compromettere l'integrità del modello, spesso per installare backdoor o degradare le prestazioni.
  ​
* Privacy Differenziale – La privacy differenziale è un quadro matematicamente rigoroso per rilasciare informazioni statistiche su set di dati proteggendo allo stesso tempo la privacy dei singoli individui. Consente al detentore dei dati di condividere modelli aggregati del gruppo limitando le informazioni trapelate riguardo a individui specifici.
  ​
* Embedding: rappresentazioni vettoriali dense di dati (testo, immagini, ecc.) che catturano il significato semantico in uno spazio ad alta dimensione.
  ​
* Spiegabilità – La spiegabilità nell’IA è la capacità di un sistema di intelligenza artificiale di fornire motivazioni comprensibili all’essere umano per le sue decisioni e predizioni, offrendo approfondimenti sul suo funzionamento interno.
  ​
* Intelligenza Artificiale Spiegabile (XAI): Sistemi di intelligenza artificiale progettati per fornire spiegazioni comprensibili agli esseri umani riguardo alle loro decisioni e comportamenti attraverso varie tecniche e framework.
  ​
* Apprendimento Federato: un approccio di machine learning in cui i modelli vengono addestrati su più dispositivi decentralizzati che possiedono dati locali, senza scambiare i dati stessi.
  ​
* Barriere di sicurezza: vincoli implementati per impedire ai sistemi di intelligenza artificiale di produrre output dannosi, faziosi o altrimenti indesiderati.
  ​
* Allucinazione – Un'allucinazione dell'IA si riferisce a un fenomeno in cui un modello di intelligenza artificiale genera informazioni errate o fuorvianti che non si basano sui dati di addestramento o sulla realtà fattuale.
  ​
* Human-in-the-Loop (HITL): Sistemi progettati per richiedere supervisione, verifica o intervento umano nei punti decisionali cruciali.
  ​
* Infrastructure as Code (IaC): Gestione e provisioning dell'infrastruttura tramite codice anziché processi manuali, consentendo scansioni di sicurezza e distribuzioni coerenti.
  ​
* Jailbreak: Tecniche utilizzate per aggirare le protezioni di sicurezza nei sistemi di intelligenza artificiale, in particolare nei grandi modelli di linguaggio, per produrre contenuti proibiti.
  ​
* Minimo privilegio: il principio di sicurezza che consiste nel concedere solo i diritti di accesso minimi necessari agli utenti e ai processi.
  ​
* LIME (Spiegazioni Locali Interpretabili Agnostiche al Modello): una tecnica per spiegare le predizioni di qualsiasi classificatore di machine learning approssimandolo localmente con un modello interpretabile.
  ​
* Attacco di Inferenza sulla Membri: Un attacco che mira a determinare se un punto dati specifico è stato utilizzato per addestrare un modello di apprendimento automatico.
  ​
* MITRE ATLAS: Paesaggio delle minacce avversarie per i sistemi di intelligenza artificiale; una base di conoscenza di tattiche e tecniche avversarie contro i sistemi di IA.
  ​
* Scheda del Modello – Una scheda del modello è un documento che fornisce informazioni standardizzate sulle prestazioni di un modello di intelligenza artificiale, le limitazioni, gli usi previsti e le considerazioni etiche per promuovere la trasparenza e uno sviluppo responsabile dell'IA.
  ​
* Estrazione del Modello: Un attacco in cui un avversario interroga ripetutamente un modello target per creare una copia funzionalmente simile senza autorizzazione.
  ​
* Inversione del modello: un attacco che tenta di ricostruire i dati di addestramento analizzando gli output del modello.
  ​
* Gestione del Ciclo di Vita del Modello – La Gestione del Ciclo di Vita del Modello AI è il processo di supervisione di tutte le fasi dell'esistenza di un modello AI, inclusi progettazione, sviluppo, implementazione, monitoraggio, manutenzione e eventuale ritiro, per garantire che rimanga efficace e allineato agli obiettivi.
  ​
* Avvelenamento del modello: introduzione di vulnerabilità o backdoor direttamente in un modello durante il processo di addestramento.
  ​
* Furto/Estrazione del Modello: Ottenere una copia o un'approssimazione di un modello proprietario tramite interrogazioni ripetute.
  ​
* Sistema multi-agente: un sistema composto da molteplici agenti di intelligenza artificiale interagenti, ciascuno con potenzialmente diverse capacità e obiettivi.
  ​
* OPA (Open Policy Agent): Un motore di policy open-source che consente l'applicazione unificata delle policy attraverso l'intero stack.
  ​
* Apprendimento Automatico Privacy-Preserving (PPML): Tecniche e metodi per addestrare e implementare modelli di ML proteggendo la privacy dei dati di addestramento.
  ​
* Iniezione di Prompt: Un attacco in cui istruzioni malevoli sono inserite negli input per sovrascrivere il comportamento previsto di un modello.
  ​
* RAG (Generazione Incrementata dal Recupero): Una tecnica che migliora i modelli di linguaggio di grandi dimensioni recuperando informazioni rilevanti da fonti di conoscenza esterne prima di generare una risposta.
  ​
* Red-Teaming: La pratica di testare attivamente i sistemi di intelligenza artificiale simulando attacchi avversari per identificare le vulnerabilità.
  ​
* SBOM (Elenco dei Componenti del Software): Un registro formale che contiene i dettagli e le relazioni della catena di fornitura di vari componenti utilizzati nella costruzione di software o modelli di intelligenza artificiale.
  ​
* SHAP (SHapley Additive exPlanations): Un approccio basato sulla teoria dei giochi per spiegare l'output di qualsiasi modello di machine learning calcolando il contributo di ogni caratteristica alla previsione.
  ​
* Attacco alla catena di approvvigionamento: Compromettere un sistema prendendo di mira gli elementi meno sicuri nella sua catena di approvvigionamento, come librerie di terze parti, dataset o modelli pre-addestrati.
  ​
* Apprendimento per trasferimento: una tecnica in cui un modello sviluppato per un compito viene riutilizzato come punto di partenza per un modello destinato a un secondo compito.
  ​
* Database vettoriale: un database specializzato progettato per memorizzare vettori ad alta dimensionalità (embedding) e eseguire ricerche di similarità efficienti.
  ​
* Scansione delle vulnerabilità: strumenti automatizzati che identificano vulnerabilità di sicurezza note nei componenti software, inclusi framework di intelligenza artificiale e dipendenze.
  ​
* Filigranatura: Tecniche per incorporare marcatori impercettibili nei contenuti generati dall'IA per tracciare la loro origine o rilevare la generazione tramite IA.
  ​
* Vulnerabilità Zero-Day: Una vulnerabilità precedentemente sconosciuta che gli aggressori possono sfruttare prima che gli sviluppatori creino e distribuiscano una patch.

