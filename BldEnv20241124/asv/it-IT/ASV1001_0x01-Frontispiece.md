# Frontespizio

## Informazioni sullo Standard

Lo Standard di Verifica della Sicurezza dell'Intelligenza Artificiale (AISVS) è un catalogo di requisiti di sicurezza guidato dalla comunità che data scientist, ingegneri MLOps, architetti software, sviluppatori, tester, professionisti della sicurezza, fornitori di strumenti, regolatori e consumatori possono utilizzare per progettare, costruire, testare e verificare sistemi e applicazioni abilitati all'IA affidabili. Fornisce un linguaggio comune per specificare i controlli di sicurezza lungo l'intero ciclo di vita dell'IA — dalla raccolta dei dati e sviluppo del modello fino al deployment e al monitoraggio continuo — in modo che le organizzazioni possano misurare e migliorare la resilienza, la privacy e la sicurezza delle loro soluzioni di IA.

## Copyright e Licenza

Versione 0.1 (Prima bozza pubblica - Lavori in corso), 2025  

![license](../images/license.png)

Copyright © 2025 Il Progetto AISVS.  

Rilasciato sotto la[Creative Commons Attribution‑ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).  
Per qualsiasi riutilizzo o distribuzione, devi comunicare chiaramente i termini della licenza di questo lavoro agli altri.

## Responsabili di Progetto

|            |                         |
| ---------- | ----------------------- |
| Jim Manico | Aras “Russ” Memisyazici |

## Contributori e Revisori

|                                    |                             |
| ---------------------------------- | --------------------------- |
| https://github.com/ottosulin       | https://github.com/mbhatt1  |
| https://github.com/vineethsai      | https://github.com/cciprofm |
| https://github.com/deepakrpandey12 |                             |

---

AISVS è un nuovo standard creato appositamente per affrontare le sfide uniche di sicurezza dei sistemi di intelligenza artificiale. Pur traendo ispirazione dalle migliori pratiche di sicurezza più ampie, ogni requisito di AISVS è stato sviluppato da zero per riflettere il panorama delle minacce dell'IA e per aiutare le organizzazioni a costruire soluzioni di IA più sicure e resilienti.

