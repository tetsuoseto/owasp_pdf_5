# Vedlegg A: Ordliste

> Dette omfattende ordlisten gir definisjoner av viktige AI-, ML- og sikkerhetsbegreper brukt gjennom AISVS for å sikre klarhet og felles forståelse.
> ​
* Adversarialt eksempel: En inngang bevisst utformet for å få en AI-modell til å gjøre en feil, ofte ved å legge til subtile forstyrrelser som er umerkelige for mennesker.
  ​
* Motstandsdyktighet mot angrep – Motstandsdyktighet mot angrep i KI refererer til en modells evne til å opprettholde ytelsen og motstå å bli lurt eller manipulert av bevisst utformede, ondsinnede inndata laget for å forårsake feil.
  ​
* Agent – AI-agenter er programvaresystemer som bruker kunstig intelligens for å forfølge mål og fullføre oppgaver på vegne av brukere. De viser resonnering, planlegging og hukommelse, og har en grad av autonomi til å ta beslutninger, lære og tilpasse seg.
  ​
* Agentisk AI: AI-systemer som kan operere med en viss grad av autonomi for å oppnå mål, ofte ved å ta beslutninger og utføre handlinger uten direkte menneskelig inngripen.
  ​
* Attributtbasert tilgangskontroll (ABAC): Et tilgangskontrollparadigme hvor autorisasjonsbeslutninger baseres på attributter til brukeren, ressursen, handlingen og miljøet, vurdert på tidspunktet for forespørselen.
  ​
* Bakdørsangrep: En type dataforgiftingsangrep hvor modellen trenes til å svare på en bestemt måte på visse triggere, samtidig som den oppfører seg normalt ellers.
  ​
* Skjevhet: Systematiske feil i AI-modellens resultater som kan føre til urettferdige eller diskriminerende utfall for visse grupper eller i spesifikke kontekster.
  ​
* Utnyttelse av skjevheter: En angrepsteknikk som utnytter kjente skjevheter i AI-modeller for å manipulere resultater eller utfall.
  ​
* Cedar: Amazons policy-språk og motor for finmaskede tillatelser brukt i implementeringen av ABAC for AI-systemer.
  ​
* Tankerekke: En teknikk for å forbedre resonnering i språkmodeller ved å generere mellomliggende resonnementstrinn før man produserer et endelig svar.
  ​
* Strømbrytere: Mekanismer som automatisk stopper AI-systemets operasjoner når spesifikke risikogrenseverdier overskrides.
  ​
* Datalekkasjer: Utilsiktet eksponering av sensitiv informasjon gjennom AI-modellers resultater eller oppførsel.
  ​
* Dataforgiftning: Den bevisste korrupsjonen av treningsdata for å undergrave modellens integritet, ofte for å installere bakdører eller redusere ytelsen.
  ​
* Differensial personvern – Differensial personvern er et matematisk strengt rammeverk for å offentliggjøre statistisk informasjon om datasett samtidig som personvernet til enkeltpersoner beskyttes. Det gjør det mulig for en dataeier å dele aggregerte mønstre for gruppen, samtidig som informasjon som lekkes om spesifikke individer begrenses.
  ​
* Inbeddinger: Tette vektorrepresentasjoner av data (tekst, bilder osv.) som fanger semantisk mening i et høy-dimensjonalt rom.
  ​
* Forklarbarhet – Forklarbarhet i kunstig intelligens er evnen til et AI-system til å gi mennesker forståelige årsaker for sine avgjørelser og prediksjoner, og dermed tilby innsikt i dets interne funksjoner.
  ​
* Forklarbar KI (XAI): KI-systemer utviklet for å gi menneskelig forståelige forklaringer på sine beslutninger og handlinger gjennom ulike teknikker og rammeverk.
  ​
* Federert læring: En maskinlæringsmetode der modeller trenes på tvers av flere desentraliserte enheter som inneholder lokale datasett, uten å utveksle selve dataene.
  ​
* Vernetiltak: Begrensninger implementert for å hindre at AI-systemer produserer skadelige, partiske eller på annen måte uønskede resultater.
  ​
* Hallusinasjon – En AI-hallusinasjon refererer til et fenomen der en AI-modell genererer feilaktig eller villedende informasjon som ikke er basert på dens treningsdata eller faktiske virkelighet.
  ​
* Menneske-i-løyfen (HITL): Systemer utformet for å kreve menneskelig overvåking, verifisering eller inngrep på avgjørende beslutningspunkter.
  ​
* Infrastruktur som kode (IaC): Håndtering og provisjonering av infrastruktur gjennom kode i stedet for manuelle prosesser, noe som muliggjør sikkerhetsskanning og konsistente distribusjoner.
  ​
* Jailbreak: Teknikker som brukes for å omgå sikkerhetsbarrierer i AI-systemer, spesielt i store språkmodeller, for å produsere forbudt innhold.
  ​
* Minste privilegium: Sikkerhetsprinsippet om å gi kun de minimum nødvendige tilgangsrettighetene for brukere og prosesser.
  ​
* LIME (Lokale Tolkningsbare Modell-agnostiske Forklaringer): En teknikk for å forklare prediksjonene til en hvilken som helst maskinlæringsklassifikator ved å tilnærme den lokalt med en tolkningsbar modell.
  ​
* Medlemskapsinferensangrep: Et angrep som har som mål å avgjøre om et spesifikt datapunkt ble brukt til å trene en maskinlæringsmodell.
  ​
* MITRE ATLAS: Trussellandskap for motstandere i kunstig intelligens-systemer; en kunnskapsbase over motstandertaktikker og -teknikker mot AI-systemer.
  ​
* Modellkort – Et modellkort er et dokument som gir standardisert informasjon om ytelsen til en AI-modell, begrensninger, tiltenkte bruksområder og etiske hensyn for å fremme åpenhet og ansvarlig AI-utvikling.
  ​
* Modellutvinning: Et angrep der en motstander gjentatte ganger spør en målsmodell for å lage en funksjonelt lik kopi uten tillatelse.
  ​
* Modellinversjon: Et angrep som forsøker å rekonstruere treningsdata ved å analysere modellens utdata.
  ​
* Modell livssyklusadministrasjon – AI-modell livssyklusadministrasjon er prosessen med å overvåke alle stadier i en AI-modells eksistens, inkludert design, utvikling, implementering, overvåking, vedlikehold og til slutt avvikling, for å sikre at den forblir effektiv og i samsvar med målene.
  ​
* Modellforgiftning: Innføring av sårbarheter eller bakdører direkte i en modell under treningsprosessen.
  ​
* Modelltyveri: Å hente ut en kopi eller en tilnærming av en proprietær modell gjennom gjentatte spørringer.
  ​
* Multi-agent system: Et system sammensatt av flere interagerende AI-agenter, hver med potensielt ulike evner og mål.
  ​
* OPA (Open Policy Agent): En åpen kildekode-policy-motor som muliggjør enhetlig håndheving av policyer på tvers av hele stakken.
  ​
* Personvernbevarende maskinlæring (PPML): Teknikk og metoder for å trene og implementere ML-modeller samtidig som personvernet til treningsdataene beskyttes.
  ​
* Promptinjeksjon: Et angrep der ondsinnede instruksjoner er innebygd i inndata for å overstyre en modells tiltenkte oppførsel.
  ​
* RAG (Retrieval-Augmented Generation): En teknikk som forbedrer store språkmodeller ved å hente relevant informasjon fra eksterne kunnskapskilder før et svar genereres.
  ​
* Red-Teaming: Praksisen med aktivt å teste AI-systemer ved å simulere fiendtlige angrep for å identifisere sårbarheter.
  ​
* SBOM (Programvarematerialeliste): En formell oversikt som inneholder detaljer og forsyningskjederelasjoner for forskjellige komponenter brukt i utviklingen av programvare eller AI-modeller.
  ​
* SHAP (SHapley Additive exPlanations): En spillteoretisk tilnærming for å forklare resultatet av hvilken som helst maskinlæringsmodell ved å beregne bidraget til hver funksjon for prediksjonen.
  ​
* Forsyningskjedeangrep: Kompromittering av et system ved å angripe mindre sikre elementer i forsyningskjeden, slik som tredjepartsbiblioteker, datasett eller forhåndstrente modeller.
  ​
* Transferlæring: En teknikk der en modell utviklet for én oppgave gjenbrukes som utgangspunkt for en modell for en annen oppgave.
  ​
* Vektordatabase: En spesialisert database designet for å lagre høy-dimensjonale vektorer (innbedded data) og utføre effektive likhetssøk.
  ​
* Sårbarhetsskanning: Automatiserte verktøy som identifiserer kjente sikkerhetssårbarheter i programvarekomponenter, inkludert AI-rammeverk og avhengigheter.
  ​
* Vannmerking: Teknikker for å legge inn umerkelige markører i AI-generert innhold for å spore opprinnelsen eller oppdage AI-generering.
  ​
* Zero-dagssårbarhet: En tidligere ukjent sårbarhet som angripere kan utnytte før utviklere lager og distribuerer en sikkerhetsoppdatering.

