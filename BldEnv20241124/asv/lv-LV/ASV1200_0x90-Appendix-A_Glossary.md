# Pielikums A: Vārdnīca

> Šis visaptverošais glosārijs sniedz būtisku mākslīgā intelekta (MI), mašīnmācīšanās (MM) un drošības terminu definīcijas, kas tiek lietotas visā AISVS, lai nodrošinātu skaidrību un kopēju izpratni.
> ​
* Pretenciozs paraugs: Ieejas dati, kas apzināti izveidoti, lai liktu mākslīgā intelekta modelim pieļaut kļūdu, bieži pievienojot smalkas perturbācijas, kuras cilvēkiem ir grūti pamanīt.
  ​
* Pretendentprasme – Pretendentprasme mākslīgajā intelektā attiecas uz modeļa spēju saglabāt savu veiktspēju un pretoties apzināti veidotu, ļaunprātīgu ievades datu manipulācijām, kas paredzētas kļūdu izraisīšanai.
  ​
* Aģents – AI aģenti ir programmatūras sistēmas, kas izmanto mākslīgo intelektu, lai sasniegtu mērķus un veiktu uzdevumus lietotāju vārdā. Tie demonstrē spriešanas, plānošanas un atmiņas spējas, kā arī ir noteikts autonomijas līmenis lēmumu pieņemšanai, mācībām un pielāgošanai.
  ​
* Agentic AI: Mākslīgā intelekta sistēmas, kas spēj darboties ar zināmu autonomijas pakāpi, lai sasniegtu mērķus, bieži pieņemot lēmumus un veicot darbības bez tiešas cilvēka iejaukšanās.
  ​
* Piekļuves kontrole, balstoties uz atribūtiem (ABAC): Piekļuves kontroles paradigma, kurā autorizācijas lēmumi tiek pieņemti, balstoties uz lietotāja, resursa, darbības un vides atribūtiem, kas tiek izvērtēti vaicājuma laikā.
  ​
* Aizmugures durvju uzbrukums: datu indēšanas uzbrukuma veids, kurā modelis tiek apmācīts reaģēt noteiktā veidā uz konkrētiem aktivizētājiem, savukārt citādi tas uzvedas parasti.
  ​
* Nostāja: Sistēmiski kļūdu AI modeļa rezultātos, kas var radīt negodīgas vai diskriminējošas sekas noteiktām grupām vai specifiskos kontekstos.
  ​
* Priekšrocību izmantošana: uzbrukuma tehnika, kas izmanto zināmās aizspriedumu vājās vietas mākslīgā intelekta modeļos, lai manipulētu ar rezultātiem vai iznākumiem.
  ​
* Cedar: Amazon politikas valoda un dzinējs smalki niansētām atļaujām, ko izmanto ABAC īstenošanai AI sistēmām.
  ​
* Domāšanas ķēde: tehnika, kas uzlabo valodas modeļu spriešanas spējas, ģenerējot starpposma sprieduma soļus pirms galīgā atbildes izveides.
  ​
* Izslēgmehānismi: mehānismi, kas automātiski aptur mākslīgā intelekta sistēmas darbību, ja tiek pārsniegti noteikti riska sliekšņa līmeņi.
  ​
* Datu noplūde: nejauša sensitīvas informācijas atklāšana caur mākslīgā intelekta modeļa rezultātiem vai uzvedību.
  ​
* Datu indesēšana: apzināta mācību datu bojāšana, lai kompromitētu modeļa integritāti, bieži vien, lai ievietotu aizmugures durvis vai pasliktinātu veiktspēju.
  ​
* Differenciālā privātums – Differenciālā privātums ir matemātiski stingra sistēma statistiskas informācijas publiskošanai par datu kopām, vienlaikus aizsargājot atsevišķu datu subjektu privātumu. Tā ļauj datu turētājam koplietot grupas kopējos modeļus, ierobežojot informācijas noplūdi par konkrētiem indivīdiem.
  ​
* Iegulti: blīvas vektoru reprezentācijas datiem (tekstam, attēliem utt.), kas uztver semantisko nozīmi augstdimensionālā telpā.
  ​
* Izskaidrojams – AI izskaidrojamība ir AI sistēmas spēja sniegt cilvēkam saprotamus iemeslus savām lēmumiem un prognozēm, piedāvājot ieskatu tās iekšējā darbībā.
  ​
* Paskaidrojama mākslīgā intelekta (XAI): mākslīgā intelekta sistēmas, kas izstrādātas, lai sniegtu cilvēkam saprotamus skaidrojumus par saviem lēmumiem un uzvedību, izmantojot dažādas metodes un ietvarus.
  ​
* Federētā mācīšanās: Mašīnmācīšanās pieeja, kurā modeļi tiek apmācīti vairākās decentralizētās ierīcēs, kurās glabājas vietējie datu paraugi, nekādi nemainot pašu datus.
  ​
* Drošības ierobežojumi: Ierobežojumi, kas īstenoti, lai novērstu mākslīgā intelekta sistēmu radīt kaitīgus, aizspriedumainus vai citādi nevēlamus rezultātus.
  ​
* Halucinācija – AI halucinācija attiecas uz fenomenu, kad AI modelis ģenerē nepareizu vai maldinošu informāciju, kas balstīta nevis uz tā apmācības datiem vai faktisko realitāti.
  ​
* Cilvēks cilpā (HITL): Sistēmas, kas izstrādātas, lai prasa cilvēka uzraudzību, pārbaudi vai iejaukšanos būtiskos lēmumu pieņemšanas punktos.
  ​
* Infrastruktūra kā kods (IaC): infrastruktūras pārvaldība un izvietošana, izmantojot kodu, nevis manuālas procedūras, ļaujot veikt drošības skenēšanu un nodrošinot konsekventas izvietošanas.
  ​
* Jailbreak: Metodes, ko izmanto, lai apietu drošības aizsardzību AI sistēmās, īpaši lielos valodas modeļos, lai radītu aizliegtu saturu.
  ​
* Minimālo tiesību princips: drošības princips, kas nodrošina tikai minimālo nepieciešamo piekļuves tiesību piešķiršanu lietotājiem un procesiem.
  ​
* LIME (Vietējā interpretējamā modeļa-neatkarīgās skaidrošanas metode): Tehnika, kas ļauj izskaidrot jebkura mašīnmācīšanās klasifikatora prognozes, tuvumā aproksimējot tos ar interpretējamu modeli.
  ​
* Dalības informācijas uzbrukums: Uzbrukums, kura mērķis ir noteikt, vai konkrēts datu punkts tika izmantots mašīnmācīšanās modeļa apmācībai.
  ​
* MITRE ATLAS: Mākslīgā intelekta sistēmu pretinieku draudu ainava; pretinieku taktiku un paņēmienu zināšanu bāze, kas vērsta pret AI sistēmām.
  ​
* Modeļa karte – Modeļa karte ir dokuments, kas sniedz standartizētu informāciju par mākslīgā intelekta modeļa veiktspēju, ierobežojumiem, paredzēto lietojumu un ētiskajiem apsvērumiem, lai veicinātu caurspīdīgumu un atbildīgu mākslīgā intelekta izstrādi.
  ​
* Modeļa ekstrakcija: uzbrukums, kurā pretinieks atkārtoti veic vaicājumus mērķa modelim, lai izveidotu funkcionāli līdzīgu kopiju bez atļaujas.
  ​
* Modeļa apgriešana: Uzbrukums, kas mēģina rekonstruēt mācību datus, analizējot modeļa izvades rezultātus.
  ​
* Modeļa dzīves cikla pārvaldība – AI modeļa dzīves cikla pārvaldība ir process, kurā tiek uzraudzītas visas AI modeļa dzīves fāzes, tai skaitā tā dizains, izstrāde, ieviešana, uzraudzība, uzturēšana un beigu izmantošanas pārtraukšana, lai nodrošinātu tā efektivitāti un atbilstību mērķiem.
  ​
* Modeļa indes ieviešana: ievadīt ievainojamības vai slepenus piekļuves ceļus tieši modelī mācību procesa laikā.
  ​
* Modeļa zādzība/Nozagšana: Proprietārā modeļa kopijas vai aptuvenas versijas izgūšana, izmantojot atkārtotus pieprasījumus.
  ​
* Multi-agentu sistēma: sistēma, kas sastāv no vairākiem savstarpēji mijiedarbojošiem mākslīgā intelekta aģentiem, katram ar potenciāli atšķirīgām spējām un mērķiem.
  ​
* OPA (Open Policy Agent): Atvērta koda politikas dzinējs, kas nodrošina vienotu politikas īstenošanu visā sistēmā.
  ​
* Privātumu saglabājoša mašīnmācīšanās (PPML): tehnikas un metodes, lai apmācītu un izvietotu mašīnmācīšanās modeļus, vienlaikus aizsargājot apmācības datu privātumu.
  ​
* Uzvednes injekcija: uzbrukums, kurā ļaunprātīgas instrukcijas tiek iegultas ievadēs, lai pārrakstītu modeļa paredzēto uzvedību.
  ​
* RAG (Atsaukumu pastiprināta ģenerēšana): tehnika, kas uzlabo lielos valodas modeļus, izgūstot atbilstošu informāciju no ārējiem zināšanu avotiem pirms atbildes ģenerēšanas.
  ​
* Red-Teaming: Praktika aktīvi testēt mākslīgā intelekta sistēmas, simulējot pretinieciski vērstas uzbrukumus, lai identificētu ievainojamības.
  ​
* SBOM (Programmatūras materiālu saraksts): Formāls ieraksts, kas satur dažādu komponentu detalizētu informāciju un piegādes ķēdes attiecības, kas izmantotas programmatūras vai mākslīgā intelekta modeļu izstrādē.
  ​
* SHAP (SHapley pievienojamās skaidrošanas): Spēļu teorijas pieeja, lai izskaidrotu jebkura mašīnmācīšanās modeļa rezultātu, aprēķinot katras iezīmes ieguldījumu prognozē.
  ​
* Piegādes ķēdes uzbrukums: sistēmas kompromitēšana, mērķējot uz mazāk drošiem tās piegādes ķēdes elementiem, piemēram, trešo pušu bibliotēkām, datu kopām vai iepriekš apmācītiem modeļiem.
  ​
* Pārneses mācīšanās: tehnika, kurā modelis, kas izstrādāts vienam uzdevumam, tiek atkārtoti izmantots kā sākumpunkts modelim otram uzdevumam.
  ​
* Vektoru datubāze: specializēta datubāze, kas izstrādāta, lai glabātu augstdimensionālus vektorus (iegultnes) un veiktu efektīvas līdzīguma meklēšanas.
  ​
* Trūkumu skenēšana: Automatizēti rīki, kas identificē zināmas drošības ievainojamības programmatūras komponentos, tostarp mākslīgā intelekta ietvaros un atkarībās.
  ​
* Ūdenszīmju ievietošana: tehnikas, lai iebūvētu nemanāmus marķierus mākslīgā intelekta ģenerētā saturā, lai izsekotu tā izcelsmi vai noteiktu, ka to radījis mākslīgais intelekts.
  ​
* Nulles dienas ievainojamība: iepriekš nezināma ievainojamība, ko uzbrucēji var izmantot, pirms izstrādātāji izveido un ievieš labojumu.

