# 부록 A: 용어집

> 이 포괄적인 용어집은 AISVS 전반에 걸쳐 사용되는 주요 AI, ML 및 보안 용어의 정의를 제공하여 명확성과 공통 이해를 보장합니다.
> ​
* 적대적 예제: 인간에게는 인지할 수 없는 미세한 교란을 추가하여 AI 모델이 실수를 하도록 의도적으로 조작된 입력 데이터.
  ​
* 적대적 견고성 – 인공지능에서 적대적 견고성은 고의적으로 설계된 악의적인 입력에 의해 오류를 발생하도록 속임수나 조작을 당하지 않고 모델이 성능을 유지하는 능력을 의미합니다.
  ​
* 에이전트 – AI 에이전트는 사용자를 대신하여 목표를 추구하고 작업을 완료하기 위해 AI를 사용하는 소프트웨어 시스템입니다. 이들은 추론, 계획, 기억 능력을 보이며, 의사결정, 학습 및 적응을 할 수 있는 자율성을 갖추고 있습니다.
  ​
* 에이전틱 AI: 목표 달성을 위해 일정 정도 자율적으로 작동할 수 있는 AI 시스템으로, 종종 직접적인 인간 개입 없이도 의사 결정을 내리고 행동을 수행합니다.
  ​
* 속성 기반 접근 제어(ABAC): 사용자, 자원, 작업 및 환경의 속성들을 기반으로 권한 결정을 내리며, 쿼리 시점에 평가되는 접근 제어 패러다임.
  ​
* 백도어 공격: 모델이 특정 트리거에 대해 특정한 방식으로 반응하도록 훈련되지만, 그 외에는 정상적으로 동작하는 데이터 중독 공격의 한 유형입니다.
  ​
* 편향: 특정 그룹이나 특정 상황에서 불공정하거나 차별적인 결과를 초래할 수 있는 AI 모델 출력의 체계적인 오류.
  ​
* 편향 악용: AI 모델의 알려진 편향을 이용하여 출력 또는 결과를 조작하는 공격 기법.
  ​
* Cedar: AI 시스템용 ABAC 구현에 사용되는 세밀한 권한 관리를 위한 아마존의 정책 언어 및 엔진.
  ​
* 사고의 연쇄: 최종 답변을 도출하기 전에 중간 추론 단계를 생성하여 언어 모델의 추론 능력을 향상시키는 기법.
  ​
* 서킷 브레이커: 특정 위험 임계값을 초과할 경우 AI 시스템 작동을 자동으로 중단시키는 메커니즘.
  ​
* 데이터 유출: AI 모델 출력이나 동작을 통해 민감한 정보가 의도치 않게 노출되는 현상.
  ​
* 데이터 포이즈닝: 모델 무결성을 훼손하기 위해 교육 데이터를 의도적으로 손상시키는 행위로, 주로 백도어를 설치하거나 성능을 저하시킬 목적으로 수행됨.
  ​
* 차분 프라이버시 – 차분 프라이버시는 개별 데이터 주체의 프라이버시를 보호하면서 데이터셋에 대한 통계 정보를 공개하기 위한 수학적으로 엄격한 프레임워크입니다. 이는 데이터 보유자가 특정 개인에 대한 정보 누출을 제한하면서도 집단의 집계 패턴을 공유할 수 있도록 합니다.
  ​
* 임베딩: 의미론적 의미를 고차원 공간에서 포착하는 데이터(텍스트, 이미지 등)의 밀집 벡터 표현.
  ​
* 설명 가능성 – AI에서 설명 가능성이란 AI 시스템이 자신의 결정과 예측에 대해 사람이 이해할 수 있는 이유를 제공하여 내부 작동 방식에 대한 통찰을 제공하는 능력을 의미합니다.
  ​
* 설명 가능한 AI (XAI): 다양한 기법과 프레임워크를 통해 그 결정과 행위에 대해 사람이 이해할 수 있는 설명을 제공하도록 설계된 AI 시스템.
  ​
* 연합 학습: 데이터 자체를 교환하지 않고 로컬 데이터 샘플을 보유한 여러 분산 디바이스에서 모델을 학습하는 머신 러닝 방법.
  ​
* 가드레일: AI 시스템이 유해하거나 편향되었거나 바람직하지 않은 출력을 생성하는 것을 방지하기 위해 구현된 제약 조건.
  ​
* 환각 – AI 환각이란 AI 모델이 훈련 데이터나 사실적 현실에 기반하지 않은 잘못되거나 오도된 정보를 생성하는 현상을 의미합니다.
  ​
* 휴먼 인 더 루프(HITL): 중요한 의사결정 시점에서 인간의 감독, 검증 또는 개입이 필요하도록 설계된 시스템.
  ​
* 코드로서의 인프라(IaC): 수동 프로세스 대신 코드를 통해 인프라를 관리하고 프로비저닝하여 보안 스캔과 일관된 배포를 가능하게 함.
  ​
* 탈옥(Jailbreak): 대규모 언어 모델을 포함한 AI 시스템에서 안전 장치를 우회하여 금지된 콘텐츠를 생성하는 데 사용되는 기술.
  ​
* 최소 권한 원칙: 사용자와 프로세스에 필요한 최소한의 접근 권한만을 부여하는 보안 원칙.
  ​
* LIME(국소 해석 가능 모델 불가지론적 설명): 해석 가능한 모델로 국소적으로 근사하여 어떤 머신러닝 분류기의 예측도 설명하는 기법입니다.
  ​
* 멤버십 추론 공격: 특정 데이터 포인트가 기계 학습 모델을 훈련하는 데 사용되었는지를 판별하는 것을 목표로 하는 공격.
  ​
* MITRE ATLAS: 인공지능 시스템에 대한 적대적 위협 지형도; AI 시스템에 대한 적대적 전술 및 기법의 지식 기반.
  ​
* 모델 카드 – 모델 카드는 AI 모델의 성능, 한계, 의도된 사용법 및 윤리적 고려사항에 대한 표준화된 정보를 제공하는 문서로, 투명성과 책임 있는 AI 개발을 촉진합니다.
  ​
* 모델 추출: 공격자가 대상 모델에 반복적으로 쿼리를 보내 허가 없이 기능적으로 유사한 복제본을 생성하는 공격.
  ​
* 모델 반전: 모델 출력을 분석하여 학습 데이터를 재구성하려는 공격.
  ​
* 모델 수명 주기 관리 – AI 모델 수명 주기 관리는 AI 모델의 설계, 개발, 배포, 모니터링, 유지보수 및 최종 폐기를 포함한 모든 단계를 감독하여 모델이 효과적이고 목표에 부합하도록 하는 프로세스입니다.
  ​
* 모델 중독: 학습 과정 중에 모델에 직접 취약점이나 백도어를 주입하는 행위.
  ​
* 모델 도용/탈취: 반복적인 쿼리를 통해 독점 모델의 사본 또는 근사치를 추출하는 행위.
  ​
* 다중 에이전트 시스템: 서로 상호작용하는 여러 AI 에이전트로 구성된 시스템으로, 각각은 잠재적으로 다른 능력과 목표를 가질 수 있다.
  ​
* OPA (Open Policy Agent): 스택 전반에 걸쳐 통합된 정책 집행을 가능하게 하는 오픈 소스 정책 엔진입니다.
  ​
* 프라이버시 보호 기계 학습(PPML): 학습 데이터의 프라이버시를 보호하면서 ML 모델을 학습하고 배포하는 기술과 방법.
  ​
* 프롬프트 인젝션: 악성 명령어가 입력에 삽입되어 모델의 의도된 동작을 무력화하는 공격.
  ​
* RAG (검색 증강 생성): 응답을 생성하기 전에 외부 지식 소스에서 관련 정보를 검색하여 대형 언어 모델을 향상시키는 기법입니다.
  ​
* 레드 팀 활동: AI 시스템의 취약점을 식별하기 위해 적대적 공격을 시뮬레이션하여 적극적으로 테스트하는 행위.
  ​
* SBOM(소프트웨어 자재 명세서): 소프트웨어나 AI 모델을 구축하는 데 사용된 다양한 구성 요소의 세부 정보와 공급망 관계를 포함하는 공식 기록.
  ​
* SHAP (셰이플리 가법 설명): 모든 머신러닝 모델의 출력 결과를 설명하기 위해 각 특징이 예측에 기여한 정도를 계산하는 게임 이론적 접근법입니다.
  ​
* 공급망 공격: 제3자 라이브러리, 데이터 세트 또는 사전 학습된 모델과 같이 공급망 내 보안이 취약한 요소를 대상으로 시스템을 침해하는 행위.
  ​
* 전이 학습: 한 작업을 위해 개발된 모델을 두 번째 작업의 모델 시작점으로 재사용하는 기법.
  ​
* 벡터 데이터베이스: 고차원 벡터(임베딩)를 저장하고 효율적인 유사도 검색을 수행하도록 설계된 특수 데이터베이스.
  ​
* 취약점 스캐닝: AI 프레임워크 및 종속성을 포함한 소프트웨어 구성 요소에서 알려진 보안 취약점을 식별하는 자동화된 도구입니다.
  ​
* 워터마킹: AI 생성 콘텐츠의 출처를 추적하거나 AI 생성 여부를 감지하기 위해 눈에 띄지 않는 표시를 삽입하는 기법.
  ​
* 제로데이 취약점: 개발자가 패치를 만들고 배포하기 전에 공격자가 악용할 수 있는 이전에 알려지지 않은 취약점.

