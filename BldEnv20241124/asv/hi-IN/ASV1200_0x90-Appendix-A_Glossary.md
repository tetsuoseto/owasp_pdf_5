# परिशिष्ट A: शब्दावली

> यह व्यापक शब्दावली AISVS में पूरे समय उपयोग किए जाने वाले प्रमुख AI, ML, और सुरक्षा शब्दों की परिभाषाएँ प्रदान करती है ताकि स्पष्टता और सामान्य समझ सुनिश्चित हो सके।
> ​
* विरोधी उदाहरण: एक ऐसा इनपुट जो जानबूझकर एक एआई मॉडल को गलती करने के लिए तैयार किया जाता है, अक्सर मानवीय दृष्टि से अदृश्य सूक्ष्म भ्रांतियों को जोड़कर।
  ​
* एडवरसियल रॉबस्टनेस – AI में एडवरसियल रॉबस्टनेस का अर्थ है एक मॉडल की ऐसी क्षमता जो उसके प्रदर्शन को बनाए रख सके और जानबूझकर तैयार किए गए, दुर्भावनापूर्ण इनपुट्स द्वारा त्रुटियां उत्पन्न करने से बचा सके।
  ​
* एजेंट – एआई एजेंट ऐसे सॉफ़्टवेयर सिस्टम होते हैं जो उपयोगकर्ताओं की ओर से लक्ष्य प्राप्त करने और कार्य पूर्ण करने के लिए एआई का उपयोग करते हैं। वे तर्कस्वरूप, योजना बनाने और स्मृति दिखाते हैं और निर्णय लेने, सीखने और अनुकूलित होने की एक स्तर की स्वायत्तता रखते हैं।
  ​
* एजेंटिक एआई: एआई सिस्टम जो कुछ हद तक स्वायत्तता के साथ कार्य कर सकते हैं ताकि लक्ष्यों को हासिल किया जा सके, अक्सर सीधे मानव हस्तक्षेप के बिना निर्णय ले सकते हैं और क्रियाएं कर सकते हैं।
  ​
* एट्रिब्यूट-आधारित एक्सेस कंट्रोल (ABAC): एक एक्सेस कंट्रोल मॉडल जिसमें प्राधिकरण निर्णय उपयोगकर्ता, संसाधन, क्रिया, और पर्यावरण के एट्रिब्यूट्स के आधार पर लिया जाता है, जो क्वेरी के समय मूल्यांकन किया जाता है।
  ​
* बैकडोर हमला: एक प्रकार का डेटा पॉइजनिंग हमला जिसमें मॉडल को कुछ विशिष्ट ट्रिगर्स के प्रति एक खास तरीके से प्रतिक्रिया देने के लिए प्रशिक्षित किया जाता है, जबकि अन्यथा यह सामान्य रूप से कार्य करता है।
  ​
* पक्षपात: AI मॉडल आउटपुट में व्यवस्थित त्रुटियाँ जो कुछ समूहों या विशिष्ट संदर्भों में अनुचित या भेदभावपूर्ण परिणामों का कारण बन सकती हैं।
  ​
* बायस का दुरुपयोग: एक हमला तकनीक जो AI मॉडलों में ज्ञात पूर्वाग्रहों का फायदा उठाकर आउटपुट या परिणामों को नियंत्रित करती है।
  ​
* Cedar: अमेज़ॅन की नीति भाषा और इंजन, जो AI सिस्टम के लिए ABAC लागू करने में सूक्ष्म अनुमति प्रदान करता है।
  ​
* चेन ऑफ थॉट: भाषा मॉडल में तर्क को सुधारने की एक तकनीक है जिसमें अंतिम उत्तर देने से पहले मध्यवर्ती तर्क चरण उत्पन्न किए जाते हैं।
  ​
* सर्किट ब्रेकर्स: ऐसे तंत्र जो स्वतः ही AI सिस्टम के संचालन को रोक देते हैं जब विशिष्ट जोखिम सीमा से अधिक हो जाती है।
  ​
* डेटा लीक: एआई मॉडल के आउटपुट या व्यवहार के माध्यम से संवेदनशील जानकारी का अनजाने में खुलासा।
  ​
* डेटा पॉइजनिंग: मॉडल की अखंडता को प्रभावित करने के लिए प्रशिक्षण डेटा का जानबूझकर भ्रष्टकरण, अक्सर बैकडोर स्थापित करने या प्रदर्शन को कमजोर करने के लिए।
  ​
* डिफरेंशियल प्राइवेसी – डिफरेंशियल प्राइवेसी एक गणितीय रूप से सुदृढ़ ढांचा है जो डेटासेट्स के बारे में सांख्यिकीय जानकारी जारी करते समय व्यक्तिगत डेटा विषयों की गोपनीयता की सुरक्षा करता है। यह डेटा धारक को समूह के सामूहिक पैटर्न साझा करने में सक्षम बनाता है, जबकि विशिष्ट व्यक्तियों के बारे में लीक होने वाली जानकारी को सीमित करता है।
  ​
* एम्बेडिंग्स: डेटा (टेक्स्ट, छवियां, आदि) के सघन वेक्टर प्रतिनिधित्व जो उच्च-आयामी स्थान में अर्थगर्भित अर्थ को पकड़ते हैं।
  ​
* व्याख्यात्मकता – AI में व्याख्यात्मकता किसी AI सिस्टम की वह क्षमता होती है जो उसके निर्णयों और पूर्वानुमानों के लिए मानव-सुलभ कारण प्रदान करती है, जिससे इसके आंतरिक कार्यप्रणाली को समझने में सहायता मिलती है।
  ​
* व्याख्यात्मक एआई (XAI): ऐसे एआई सिस्टम जो अपने निर्णयों और व्यवहारों के लिए मानव-सुलभ स्पष्टीकरण प्रदान करने के लिए विभिन्न तकनीकों और ढांचों के माध्यम से डिज़ाइन किए गए हैं।
  ​
* संघीय शिक्षण: एक मशीन लर्निंग दृष्टिकोण जिसमें मॉडल स्थानीय डेटा नमूनों को रखने वाले कई विकेंद्रीकृत उपकरणों में प्रशिक्षित होते हैं, बिना डेटा को स्वयं अदला-बदली किए।
  ​
* गार्डरेल्स: सीमाएँ जो AI सिस्टमों को हानिकारक, पक्षपाती, या अन्यथा असामान्य आउटपुट उत्पन्न करने से रोकने के लिए लागू की जाती हैं।
  ​
* हेलुसिनेशन – एक AI हेलुसिनेशन उस घटना को कहते हैं जहां AI मॉडल ऐसा गलत या भ्रामक जानकारी उत्पन्न करता है जो उसके प्रशिक्षण डेटा या वास्तविक तथ्यों पर आधारित नहीं होती।
  ​
* ह्यूमन-इन-द-लूप (HITL): ऐसे सिस्टम जिन्हें महत्वपूर्ण निर्णय बिंदुओं पर मानव निगरानी, सत्यापन, या हस्तक्षेप की आवश्यकता होती है।
  ​
* इन्फ्रास्ट्रक्चर एज कोड (IaC): मैनुअल प्रक्रियाओं के बजाय कोड के माध्यम से इंफ्रास्ट्रक्चर का प्रबंधन और प्रावधान करना, जो सुरक्षा स्कैनिंग और सुसंगत तैनाती को सक्षम बनाता है।
  ​
* जेलब्रेक: एआई सिस्टमों में सुरक्षा गार्डरेल्स को दरकिनार करने के लिए उपयोग की जाने वाली तकनीकें, विशेष रूप से बड़े भाषा मॉडल में, जिससे प्रतिबंधित सामग्री उत्पन्न हो सके।
  ​
* न्यूनतम विशेषाधिकार: उपयोगकर्ताओं और प्रक्रियाओं के लिए केवल आवश्यक न्यूनतम पहुँच अधिकार प्रदान करने का सुरक्षा सिद्धांत।
  ​
* LIME (लोकल इंटरप्रेटेबल मॉडल-एग्नोस्टिक एक्सप्लानेशंस): किसी भी मशीन लर्निंग क्लासिफायर की भविष्यवाणियों को स्थानीय स्तर पर एक इंटरप्रेटेबल मॉडल से अंदाजित करके समझाने की तकनीक।
  ​
* सदस्यता अनुमान हमला: एक ऐसा हमला जिसका उद्देश्य यह निर्धारित करना होता है कि क्या किसी विशिष्ट डेटा पॉइंट का उपयोग किसी मशीन लर्निंग मॉडल को प्रशिक्षित करने के लिए किया गया था।
  ​
* MITRE ATLAS: कृत्रिम बुद्धिमत्ता प्रणालियों के लिए प्रत्यक्ष खतरे का परिदृश्य; AI प्रणालियों के खिलाफ प्रत्यक्ष रणनीतियों और तकनीकों का ज्ञान आधार।
  ​
* मॉडल कार्ड – एक मॉडल कार्ड एक दस्तावेज़ होता है जो एक AI मॉडल के प्रदर्शन, सीमाओं, इच्छित उपयोगों, और नैतिक विचारों के बारे में मानकीकृत जानकारी प्रदान करता है ताकि पारदर्शिता और जिम्मेदार AI विकास को बढ़ावा दिया जा सके।
  ​
* मॉडल निष्कर्षण: एक हमला जिसमें एक प्रतिद्वंद्वी बार-बार लक्षित मॉडल से प्रश्न करता है ताकि बिना अनुमति के एक कार्यात्मक रूप से समान प्रति बनाई जा सके।
  ​
* मॉडल इनवर्शन: एक हमला जो मॉडल आउटपुट का विश्लेषण करके प्रशिक्षण डेटा को पुनर्निर्मित करने का प्रयास करता है।
  ​
* मॉडल जीवनचक्र प्रबंधन – एआई मॉडल जीवनचक्र प्रबंधन एक एआई मॉडल के सभी चरणों की निगरानी करने की प्रक्रिया है, जिसमें इसके डिजाइन, विकास, परिनियोजन, निगरानी, रखरखाव, और अंततः अवकाश शामिल है, ताकि यह सुनिश्चित किया जा सके कि मॉडल प्रभावी बना रहे और उद्देश्यों के अनुरूप हो।
  ​
* मॉडल विषाक्तता: प्रशिक्षण प्रक्रिया के दौरान सीधे मॉडल में कमजोरियां या बैकडोर डालना।
  ​
* मॉडल चोरी/चोरी: मालिकाना मॉडल की एक प्रति या अनुमान को बार-बार पूछताछ के माध्यम से निकालना।
  ​
* मल्टी-एजेंट सिस्टम: एक ऐसा सिस्टम जो कई इंटरैक्टिंग AI एजेंट्स से बना होता है, जिनमें से प्रत्येक की संभावित रूप से अलग क्षमताएं और लक्ष्य हो सकते हैं।
  ​
* OPA (ओपन पॉलिसी एजेंट): एक खुला स्रोत नीति इंजन जो स्टैक भर में एकीकृत नीति प्रवर्तन सक्षम बनाता है।
  ​
* गोपनीयता-संरक्षित मशीन लर्निंग (PPML): प्रशिक्षक डेटा की गोपनीयता की रक्षा करते हुए एमएल मॉडल को प्रशिक्षित और तैनात करने की तकनीकें और विधियाँ।
  ​
* प्रॉम्प्ट इंजेक्शन: एक ऐसा हमला जिसमें मॉडल के निर्धारित व्यवहार को ओवरराइड करने के लिए इनपुट्स में दुर्भावनापूर्ण निर्देश एम्बेड किए जाते हैं।
  ​
* RAG (रीट्राइवल-अग्मेंटेड जनरेशन): एक तकनीक जो बड़ी भाषा मॉडलों को बाहरी ज्ञान स्रोतों से संबंधित जानकारी प्राप्त करके प्रतिक्रिया उत्पन्न करने से पहले बेहतर बनाती है।
  ​
* रेड-टीमिंग: एआई सिस्टमों का सक्रिय रूप से परीक्षण करने का अभ्यास, जिसमें विरोधी हमलों का अनुकरण करके कमजोरियों की पहचान की जाती है।
  ​
* SBOM (सॉफ्टवेयर बिल ऑफ मैटेरियल्स): एक औपचारिक रिकॉर्ड जिसमें सॉफ़्टवेयर या एआई मॉडल बनाने में उपयोग किए गए विभिन्न घटकों के विवरण और आपूर्ति श्रृंखला संबंध शामिल होते हैं।
  ​
* SHAP (शैप्ले एडिटिव एक्स्प्लनेशंस): किसी भी मशीन लर्निंग मॉडल के आउटपुट को समझाने के लिए एक गेम थ्योरिटिक दृष्टिकोण, जो भविष्यवाणी में प्रत्येक फीचर के योगदान की गणना करता है।
  ​
* सप्लाई चेन हमला: किसी सिस्टम को उसकी सप्लाई चेन में कम सुरक्षित तत्वों जैसे तृतीय-पक्ष लाइब्रेरीज, डेटासेट्स, या प्री-ट्रेंड मॉडल्स को लक्षित करके समझौता करना।
  ​
* ट्रांसफर लर्निंग: एक तकनीक जिसमें एक टास्क के लिए विकसित मॉडल को दूसरे टास्क पर मॉडल के प्रारंभिक बिंदु के रूप में पुनः उपयोग किया जाता है।
  ​
* वेक्तर डेटाबेस: एक विशेषीकृत डेटाबेस जो उच्च-आयामी वेक्तर (एम्बेडिंग्स) को संग्रहीत करने और कुशल समानता खोजें करने के लिए डिज़ाइन किया गया है।
  ​
* सुरक्षा कमजोरी स्कैनिंग: स्वचालित उपकरण जो सॉफ़्टवेयर घटकों में ज्ञात सुरक्षा कमजोरियों की पहचान करते हैं, जिसमें AI फ्रेमवर्क और निर्भरताएं शामिल हैं।
  ​
* वॉटरमार्किंग: AI-जनित सामग्री में उसकी उत्पत्ति को ट्रैक करने या AI जनरेशन का पता लगाने के लिए अदृश्य मार्कर एम्बेड करने की तकनीकें।
  ​
* ज़ीरो-डे भेद्यता: एक पूर्व में अज्ञात भेद्यता जिसे हमलावर तब तक फायदा उठा सकते हैं जब तक विकासकर्ता इसका समाधान तैयार कर उसे लागू न करें।

